{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d755ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORER_ANALYSIS_PROMPT= \"\"\" A scorer named {scorer_name} is run on different runs of the same agent/llm/tool,\n",
    "the scorer gives a score and a reasoning, you will be given 25 such samples. Now the scorer may be giving different\n",
    "reasonings and all of them are in natural language. I want you to highlight the key reasons for your response.\n",
    "These key reasons will further be used for analysis, and improvement of the agent. Do not suggest any fixes.\n",
    "Just focus on not missing out on any of the information regarding why the agent is failing.\n",
    "You have to focus on the low scores only, as we have to improve them.\n",
    "Some rows might show - \"Missing required fields\" it is a code issue, on the developer's side, so do not include it in the reasoning.\n",
    "\n",
    "Just to clarify, your job is not to analyze the scorers, but to analyze the agent. You are basically the representative of the scorers.\n",
    "\n",
    "Give the reasoning, and start with the scorer name.\n",
    "\n",
    "In the format - \n",
    "Scorer Name: Task Progression\n",
    "Reasoning: \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ed8e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENTWISE_SUMMARY_PROMPT= \"\"\"\n",
    "Different scorers are run on different runs of the same agent/llm/tool, you will be given the reasoning\n",
    "for each scorer, as to why it gave poor scores. You job is to summarize the information from different\n",
    "scorers into a single analysis. All the scores are of one specific part of the entire agentic workflow,\n",
    "so please remove the redundancies that you get. Do not try to suggest fixes, only focus on removing\n",
    "the redundant information, and keeping the important information.\n",
    "\n",
    "Just to clarify, your job is not to analyze the summaries, but to analyze the agent. You are basically the representative of the scorers.\n",
    "\n",
    "In the format -\n",
    "Agent Name: query_generation\n",
    "Reasoning: \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35b891a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_ANALYSIS_PROMPT= \"\"\"\n",
    "An agent is run, and then different scorers are run on specific parts of the agentic workflow.\n",
    "So if an agent has 5 different parts (llm/tool/agent), and there are 3 scorers, then there will be a total of 15 scores, and respective reasonings. I have condensed these reasonings, in a part wise manner.\n",
    "You will be given the part wise analysis, and you will also be given the entire agentic workflow, explaining how the agent is set up.\n",
    "\n",
    "You have to figure out why the agent is failing, you are given a bird's eye view, as in agents, a failure at step 1, may surface at step 3 in the analysis, so you will have to be aware of that.\n",
    "\n",
    "You have to suggest fixes to the developer in bullet points, in the format ->\n",
    "\n",
    "Suggested Fixes:\n",
    " - fix_1: \n",
    " - fix_2: \n",
    " \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "585857a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Gemini API\n",
    "genai.configure(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "model = genai.GenerativeModel('gemini-2.5-pro')\n",
    "\n",
    "print(\"Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45319774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit agent document loaded: 8492 characters\n",
      "Log file will be saved to: log/analysis_log_20250925_004622.txt\n"
     ]
    }
   ],
   "source": [
    "# Read the reddit_agent.md file\n",
    "with open('reddit_agent.md', 'r', encoding='utf-8') as f:\n",
    "    reddit_agent_doc = f.read()\n",
    "\n",
    "print(f\"Reddit agent document loaded: {len(reddit_agent_doc)} characters\")\n",
    "\n",
    "# Create log directory\n",
    "log_dir = Path('log')\n",
    "log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Generate timestamp for log file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = log_dir / f\"analysis_log_{timestamp}.txt\"\n",
    "\n",
    "print(f\"Log file will be saved to: {log_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd247a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Function to log responses\n",
    "def log_response(response, description):\n",
    "    with open(log_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(f\"\\n{'='*50}\\n\")\n",
    "        f.write(f\"TIMESTAMP: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"DESCRIPTION: {description}\\n\")\n",
    "        f.write(f\"{'='*50}\\n\")\n",
    "        f.write(f\"{response}\\n\")\n",
    "        f.write(f\"{'='*50}\\n\\n\")\n",
    "\n",
    "def process_csv_file(csv_path):\n",
    "    \"\"\"\n",
    "    Process CSV file and extract scorer data for first 25 rows\n",
    "    Returns dict with scorer data strings\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Get first 25 rows (or all if less than 25)\n",
    "    rows_to_process = min(25, len(df))\n",
    "    df_subset = df.head(rows_to_process)\n",
    "    \n",
    "    # Get column names\n",
    "    columns = df.columns.tolist()\n",
    "    \n",
    "    # Skip first 4 columns (IDs)\n",
    "    # The remaining columns are split into score columns and reasoning columns\n",
    "    remaining_columns = columns[4:]\n",
    "    n_scorers = len(remaining_columns) // 2  # Each scorer has score + reasoning column\n",
    "    \n",
    "    score_columns = remaining_columns[:n_scorers]  # First half are score columns\n",
    "    reasoning_columns = remaining_columns[n_scorers:]  # Second half are reasoning columns\n",
    "    \n",
    "    scorer_data = {}\n",
    "    \n",
    "    # For each scorer\n",
    "    for i, scorer_name in enumerate(score_columns):\n",
    "        scorer_strings = []\n",
    "        reasoning_col = reasoning_columns[i]\n",
    "        \n",
    "        for _, row in df_subset.iterrows():\n",
    "            score = row[scorer_name]\n",
    "            reasoning = row[reasoning_col]\n",
    "            scorer_string = f\"{scorer_name} score = {score} reasoning = {reasoning}\"\n",
    "            scorer_strings.append(scorer_string)\n",
    "        \n",
    "        scorer_data[scorer_name] = \"\\n\".join(scorer_strings)\n",
    "    \n",
    "    return scorer_data\n",
    "print(\"Helper functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e2ee058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 dataset directories to process:\n",
      "  - email_gen_send_dataset\n",
      "  - agent_comment_gen_dataset\n",
      "  - post_validation_dataset\n",
      "  - agent_query_gen_dataset\n",
      "  - tavily_search_results_dataset\n",
      "\n",
      "Processing email_gen_send_dataset...\n",
      "  Processing CSV: agent_evaluation_results.csv\n",
      "    Making Gemini call for scorer: task_progression\n",
      "    Making Gemini call for scorer: context_relevancy\n",
      "    Making Gemini call for scorer: role_adherence\n",
      "    Making Gemini call for scorer: tool_relevancy\n",
      "    Making Gemini call for scorer: parameter_correctness\n",
      "    Making summary call for email_gen_send_dataset\n",
      "\n",
      "Processing agent_comment_gen_dataset...\n",
      "  Processing CSV: agent_evaluation_results.csv\n",
      "    Making Gemini call for scorer: task_progression\n",
      "    Making Gemini call for scorer: context_relevancy\n",
      "    Making Gemini call for scorer: role_adherence\n",
      "    Making Gemini call for scorer: tool_relevancy\n",
      "    Making Gemini call for scorer: parameter_correctness\n",
      "    Making summary call for agent_comment_gen_dataset\n",
      "\n",
      "Processing post_validation_dataset...\n",
      "  Processing CSV: agent_evaluation_results.csv\n",
      "    Making Gemini call for scorer: task_progression\n",
      "    Making Gemini call for scorer: context_relevancy\n",
      "    Making Gemini call for scorer: role_adherence\n",
      "    Making Gemini call for scorer: tool_relevancy\n",
      "    Making Gemini call for scorer: parameter_correctness\n",
      "    Making summary call for post_validation_dataset\n",
      "\n",
      "Processing agent_query_gen_dataset...\n",
      "  Processing CSV: agent_evaluation_results.csv\n",
      "    Making Gemini call for scorer: task_progression\n",
      "    Making Gemini call for scorer: context_relevancy\n",
      "    Making Gemini call for scorer: role_adherence\n",
      "    Making Gemini call for scorer: tool_relevancy\n",
      "    Making Gemini call for scorer: parameter_correctness\n",
      "    Making summary call for agent_query_gen_dataset\n",
      "\n",
      "Processing tavily_search_results_dataset...\n",
      "  Processing CSV: agent_evaluation_results.csv\n",
      "    Making Gemini call for scorer: task_progression\n",
      "    Making Gemini call for scorer: context_relevancy\n",
      "    Making Gemini call for scorer: role_adherence\n",
      "    Making Gemini call for scorer: tool_relevancy\n",
      "    Making Gemini call for scorer: parameter_correctness\n",
      "    Making summary call for tavily_search_results_dataset\n",
      "\n",
      "Completed processing 5 datasets.\n"
     ]
    }
   ],
   "source": [
    "# Main processing loop\n",
    "demo_results_dir = Path('demo_results')\n",
    "all_summaries = []\n",
    "\n",
    "# Get all dataset directories\n",
    "dataset_dirs = [d for d in demo_results_dir.iterdir() if d.is_dir()]\n",
    "\n",
    "print(f\"Found {len(dataset_dirs)} dataset directories to process:\")\n",
    "for d in dataset_dirs:\n",
    "    print(f\"  - {d.name}\")\n",
    "\n",
    "# Process each dataset directory\n",
    "for dataset_dir in dataset_dirs:\n",
    "    print(f\"\\nProcessing {dataset_dir.name}...\")\n",
    "    \n",
    "    # Find CSV file in the directory\n",
    "    csv_files = list(dataset_dir.glob('*.csv'))\n",
    "    if not csv_files:\n",
    "        print(f\"  No CSV files found in {dataset_dir.name}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    csv_file = csv_files[0]  # Take the first CSV file\n",
    "    print(f\"  Processing CSV: {csv_file.name}\")\n",
    "    \n",
    "    # Process the CSV file\n",
    "    scorer_data = process_csv_file(csv_file)\n",
    "    \n",
    "    # Store responses for this dataset\n",
    "    dataset_responses = []\n",
    "    \n",
    "    # For each scorer, make a Gemini API call\n",
    "    for scorer_name, scorer_string in scorer_data.items():\n",
    "        print(f\"    Making Gemini call for scorer: {scorer_name}\")\n",
    "        \n",
    "        # TODO: Replace with actual prompt template\n",
    "        prompt = f\"\"\"{SCORER_ANALYSIS_PROMPT}\n",
    "        \n",
    "        Scorer Data: \n",
    "        Scorer Name: {scorer_name}\n",
    "        {scorer_string}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            response_text = response.text\n",
    "            response_text = \"Analysis for scorer: \" + scorer_name + \"\\n\" + response_text\n",
    "            # Log the response\n",
    "            log_response(response_text, f\"{dataset_dir.name} - {scorer_name} Analysis\")\n",
    "            dataset_responses.append(f\"Scorer: {scorer_name}\\n{response_text}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error processing {scorer_name}: {str(e)}\"\n",
    "            print(f\"    {error_msg}\")\n",
    "            log_response(error_msg, f\"{dataset_dir.name} - {scorer_name} Error\")\n",
    "    \n",
    "    # Make summary call for this dataset\n",
    "    print(f\"    Making summary call for {dataset_dir.name}\")\n",
    "    \n",
    "    combined_responses = \"\\n\\n\".join(dataset_responses)\n",
    "    \n",
    "    # TODO: Replace with actual summary prompt template\n",
    "    summary_prompt = f\"\"\"{AGENTWISE_SUMMARY_PROMPT}\n",
    "    \n",
    "    Agent Name: {dataset_dir.name}\n",
    "    Scorer Analyses:\n",
    "    {combined_responses}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        summary_response = model.generate_content(summary_prompt)\n",
    "        summary_text = summary_response.text\n",
    "        \n",
    "        # Log the summary\n",
    "        log_response(summary_text, f\"{dataset_dir.name} - Dataset Summary\")\n",
    "        all_summaries.append(f\"Dataset: {dataset_dir.name}\\n{summary_text}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error creating summary for {dataset_dir.name}: {str(e)}\"\n",
    "        print(f\"    {error_msg}\")\n",
    "        log_response(error_msg, f\"{dataset_dir.name} - Summary Error\")\n",
    "\n",
    "print(f\"\\nCompleted processing {len(dataset_dirs)} datasets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfd48b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Making final comprehensive analysis call...\n",
      "Final analysis completed and logged!\n",
      "All responses have been logged to: log/analysis_log_20250925_004622.txt\n",
      "\n",
      "==================================================\n",
      "ANALYSIS COMPLETE!\n",
      "==================================================\n",
      "Log file location: log/analysis_log_20250925_004622.txt\n",
      "Total datasets processed: 5\n",
      "Total summaries generated: 5\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Final comprehensive analysis\n",
    "print(\"\\nMaking final comprehensive analysis call...\")\n",
    "\n",
    "# Combine all dataset summaries\n",
    "combined_summaries = \"\\n\\n\".join(all_summaries)\n",
    "\n",
    "# TODO: Replace with actual final analysis prompt template\n",
    "final_prompt = f\"\"\"{FINAL_ANALYSIS_PROMPT}\n",
    "\n",
    "Reddit Agent Documentation:\n",
    "{reddit_agent_doc}\n",
    "\n",
    "Dataset Summaries:\n",
    "{combined_summaries}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    final_response = model.generate_content(final_prompt)\n",
    "    final_text = final_response.text\n",
    "    \n",
    "    # Log the final analysis\n",
    "    log_response(final_text, \"Final Comprehensive Analysis\")\n",
    "    \n",
    "    print(\"Final analysis completed and logged!\")\n",
    "    print(f\"All responses have been logged to: {log_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    error_msg = f\"Error creating final analysis: {str(e)}\"\n",
    "    print(error_msg)\n",
    "    log_response(error_msg, \"Final Analysis Error\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Log file location: {log_file}\")\n",
    "print(f\"Total datasets processed: {len(dataset_dirs)}\")\n",
    "print(f\"Total summaries generated: {len(all_summaries)}\")\n",
    "print(\"=\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
