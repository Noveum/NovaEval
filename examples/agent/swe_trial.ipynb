{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# SWE Agent Trajectories Testing\n",
    "\n",
    "#This notebook tests the methods from `swe_agent_trajectories.py` with the specified parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de0a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Add the src directory to the path to import novaeval modules\n",
    "sys.path.append(\"../../../\")\n",
    "\n",
    "from novaeval.datasets.swe_agent_trajectories_dataset import (\n",
    "    create_dataset,\n",
    "    swe_agent_trajectories_preprocessing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c1b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parquet file path\n",
    "parquet_file_path = os.environ.get(\"SWE_PARQUET_PATH\", \"/mnt/drive2/train-00000-of-00012.parquet\")\n",
    "\n",
    "# Check if the file exists\n",
    "print(f\"Checking if file exists: {parquet_file_path}\")\n",
    "print(f\"File exists: {os.path.exists(parquet_file_path)}\")\n",
    "\n",
    "# If file doesn't exist, let's check what's in the directory\n",
    "if not os.path.exists(parquet_file_path):\n",
    "    print(\"\\nFile not found. Checking directory contents:\")\n",
    "    try:\n",
    "        dir_path = \"/mnt/drive2/\"\n",
    "        if os.path.exists(dir_path):\n",
    "            files = os.listdir(dir_path)\n",
    "            parquet_files = [f for f in files if f.endswith(\".parquet\")]\n",
    "            print(f\"Parquet files in {dir_path}: {parquet_files[:10]}\")  # Show first 10\n",
    "        else:\n",
    "            print(f\"Directory {dir_path} does not exist\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking directory: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Try to read the parquet file directly with pandas first\n",
    "try:\n",
    "    print(\"Attempting to read parquet file directly with pandas...\")\n",
    "    df = pd.read_parquet(parquet_file_path)\n",
    "    print(\"Successfully read parquet file!\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Check if required columns exist\n",
    "    required_cols = [\n",
    "        \"instance_id\",\n",
    "        \"model_name\",\n",
    "        \"target\",\n",
    "        \"trajectory\",\n",
    "        \"exit_status\",\n",
    "        \"generated_patch\",\n",
    "        \"eval_logs\",\n",
    "    ]\n",
    "    missing = [col for col in required_cols if col not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"\\nMissing required columns: {missing}\")\n",
    "    else:\n",
    "        print(\"\\nAll required columns present!\")\n",
    "\n",
    "    # Check trajectory column structure\n",
    "    if \"trajectory\" in df.columns:\n",
    "        print(\"\\nTrajectory column sample:\")\n",
    "        print(df[\"trajectory\"].iloc[0])\n",
    "        print(f\"Type: {type(df['trajectory'].iloc[0])}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error reading parquet file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0037e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Test the preprocessing function\n",
    "try:\n",
    "    print(\"Testing swe_agent_trajectories_preprocessing function...\")\n",
    "    output_csv = \"/mnt/drive2/test_output.csv\"\n",
    "\n",
    "    # Call the preprocessing function\n",
    "    swe_agent_trajectories_preprocessing(\n",
    "        parquet_files=[parquet_file_path], output_csv=output_csv\n",
    "    )\n",
    "\n",
    "    print(f\"Preprocessing completed! Output saved to: {output_csv}\")\n",
    "\n",
    "    # Read and display the output\n",
    "    output_df = pd.read_csv(output_csv)\n",
    "    print(f\"\\nOutput shape: {output_df.shape}\")\n",
    "    print(f\"Output columns: {list(output_df.columns)}\")\n",
    "    print(\"\\nFirst few rows of output:\")\n",
    "    print(output_df.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in preprocessing: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a133ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Test the create_dataset function\n",
    "try:\n",
    "    print(\"Testing create_dataset function...\")\n",
    "\n",
    "    # Create dataset from the preprocessed CSV\n",
    "    dataset = create_dataset(output_csv)\n",
    "\n",
    "    print(\"Dataset created successfully!\")\n",
    "    print(f\"Dataset type: {type(dataset)}\")\n",
    "\n",
    "    # Try to access some dataset properties/methods\n",
    "    if hasattr(dataset, \"__len__\"):\n",
    "        print(f\"Dataset length: {len(dataset)}\")\n",
    "\n",
    "    if hasattr(dataset, \"data\"):\n",
    "        print(\n",
    "            f\"Dataset data keys: {list(dataset.data.keys()) if isinstance(dataset.data, dict) else 'Not a dict'}\"\n",
    "        )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in create_dataset: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f1ed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World\")\n",
    "print(type(dataset))\n",
    "print(dir(dataset))\n",
    "gen = dataset.get_datapoint()\n",
    "obj = next(gen)\n",
    "print(next(gen))\n",
    "print(obj.model_dump())\n",
    "print(type(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0880732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add the src directory to the path to import novaeval modules\n",
    "sys.path.append(\"../../../\")\n",
    "\n",
    "from novaeval.datasets.swe_agent_trajectories_dataset import (\n",
    "    swe_agent_trajectories_preprocessing,\n",
    ")\n",
    "\n",
    "total_processed = 0\n",
    "for chunk in dataset.stream_from_csv(\n",
    "    file_path=output_csv,\n",
    "    chunk_size=500,  # Process 500 rows at a time\n",
    "    turn_id=\"instance_id\",\n",
    "    agent_name=\"model_name\",\n",
    "    agent_task=\"target\",\n",
    "    tool_call_results=\"generated_patch\",\n",
    "    metadata=\"eval_logs\",\n",
    "):\n",
    "    # chunk is list[AgentData] with max 500 items\n",
    "    chunk_size = len(chunk)\n",
    "    total_processed += chunk_size\n",
    "\n",
    "    # Example processing: Print first item in chunk\n",
    "    if total_processed <= 500:  # Only print from first chunk\n",
    "        print(\"\\nSample data from first chunk:\")\n",
    "        print(f\"Agent Name: {chunk[0].agent_name}\")\n",
    "        print(f\"Task: {chunk[0].agent_task}\")\n",
    "        print(f\"Turn ID: {chunk[0].turn_id}\")\n",
    "\n",
    "    print(f\"Processed chunk of {chunk_size} items. Total processed: {total_processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0934ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Test with directory approach (if we have multiple files)\n",
    "try:\n",
    "    print(\"Testing with directory approach...\")\n",
    "\n",
    "    # Get the directory of the parquet file\n",
    "    parquet_dir = os.path.dirname(parquet_file_path)\n",
    "    print(f\"Parquet directory: {parquet_dir}\")\n",
    "\n",
    "    # Check if directory exists and contains parquet files\n",
    "    if os.path.exists(parquet_dir):\n",
    "        files = os.listdir(parquet_dir)\n",
    "        parquet_files = [f for f in files if f.endswith(\".parquet\")]\n",
    "        print(f\"Found {len(parquet_files)} parquet files in directory\")\n",
    "\n",
    "        if len(parquet_files) > 1:\n",
    "            print(\"Testing with multiple files using directory approach...\")\n",
    "            output_csv_multi = \"test_output_multi.csv\"\n",
    "\n",
    "            swe_agent_trajectories_preprocessing(\n",
    "                parquet_dir=parquet_dir, output_csv=output_csv_multi\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"Multi-file preprocessing completed! Output saved to: {output_csv_multi}\"\n",
    "            )\n",
    "\n",
    "            # Read and display the output\n",
    "            output_df_multi = pd.read_csv(output_csv_multi)\n",
    "            print(f\"\\nMulti-file output shape: {output_df_multi.shape}\")\n",
    "            print(f\"Multi-file output columns: {list(output_df_multi.columns)}\")\n",
    "        else:\n",
    "            print(\"Only one parquet file found, skipping multi-file test\")\n",
    "    else:\n",
    "        print(f\"Directory {parquet_dir} does not exist\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in directory approach test: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Remove temporary files\n",
    "\n",
    "files_to_cleanup = [\"test_output.csv\", \"test_output_multi.csv\"]\n",
    "for file in files_to_cleanup:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        print(f\"Cleaned up: {file}\")\n",
    "\n",
    "print(\"\\nTesting completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf14c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test preprocessing with a small sample first\n",
    "sample_size = 100  # adjust this number as needed\n",
    "df_sample = df.head(sample_size).copy()\n",
    "\n",
    "# Analyze trajectory lengths BEFORE preprocessing\n",
    "trajectory_lengths = []\n",
    "for _idx, row in df_sample.iterrows():\n",
    "    traj = row[\"trajectory\"]\n",
    "    traj_len = len(traj) if hasattr(traj, \"__len__\") else 0\n",
    "    trajectory_lengths.append(traj_len)\n",
    "\n",
    "print(f\"Total trajectory elements: {sum(trajectory_lengths)}\")\n",
    "\n",
    "# Save sample to parquet\n",
    "sample_parquet = \"sample.parquet\"\n",
    "df_sample.to_parquet(sample_parquet)\n",
    "\n",
    "# Process the sample\n",
    "output_csv_sample = \"sample_output.csv\"\n",
    "swe_agent_trajectories_preprocessing(\n",
    "    parquet_files=[sample_parquet], output_csv=output_csv_sample\n",
    ")\n",
    "\n",
    "# Check sizes AFTER preprocessing\n",
    "sample_output_df = pd.read_csv(output_csv_sample)\n",
    "print(f\"Actual output rows: {len(sample_output_df)}\")\n",
    "\n",
    "print(\"\\nSample processing results:\")\n",
    "print(f\"Input rows: {len(df_sample)}\")\n",
    "print(f\"Output rows: {len(sample_output_df)}\")\n",
    "expansion_factor = len(sample_output_df) / len(df_sample)\n",
    "print(f\"Expansion factor: {expansion_factor:.2f}x\")\n",
    "\n",
    "# Check memory usage of output\n",
    "print(\"\\nOutput DataFrame memory usage per column:\")\n",
    "print(sample_output_df.memory_usage(deep=True) / (1024 * 1024), \"MB\")\n",
    "\n",
    "# Clean up sample files\n",
    "os.remove(sample_parquet)\n",
    "os.remove(output_csv_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16050f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
