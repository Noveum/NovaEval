{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agent Evaluation Demo with NovaEval\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "1. Load agent trace data from dataset.json\n",
        "2. Map trace spans to AgentData format\n",
        "3. Create an AgentDataset \n",
        "4. Evaluate agent performance using AgentEvaluator with Gemini model\n",
        "5. Analyze results with multiple scorers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… All imports successful!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "# NovaEval imports\n",
        "from novaeval.agents.agent_data import AgentData, ToolSchema, ToolCall, ToolResult\n",
        "from novaeval.datasets.agent_dataset import AgentDataset\n",
        "from novaeval.evaluators.agent_evaluator import AgentEvaluator\n",
        "from novaeval.models.gemini import GeminiModel\n",
        "from novaeval.scorers.agent_scorers import (\n",
        "    context_relevancy_scorer,\n",
        "    role_adherence_scorer,\n",
        "    task_progression_scorer,\n",
        "    tool_relevancy_scorer,\n",
        "    tool_correctness_scorer,\n",
        "    parameter_correctness_scorer\n",
        ")\n",
        "\n",
        "print(\"âœ… All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load and Examine Dataset Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“Š Loaded 56 spans from dataset.json\n",
            "\n",
            "ðŸ” Sample span types:\n",
            "  - chain_start.unknown: 1\n",
            "  - llm.openai: 5\n",
            "  - tool:user_input:user_input: 4\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "with open('dataset.json', 'r') as f:\n",
        "    spans_data = json.load(f)\n",
        "\n",
        "print(f\"ðŸ“Š Loaded {len(spans_data)} spans from dataset.json\")\n",
        "print(f\"\\nðŸ” Sample span types:\")\n",
        "\n",
        "# Analyze span types\n",
        "span_types = {}\n",
        "for span in spans_data[:10]:  # Look at first 10 spans\n",
        "    span_name = span.get('name', 'unknown')\n",
        "    if span_name not in span_types:\n",
        "        span_types[span_name] = 0\n",
        "    span_types[span_name] += 1\n",
        "\n",
        "for span_type, count in span_types.items():\n",
        "    print(f\"  - {span_type}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Implement Field Mapping Logic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_tools_from_prompt(prompt: str) -> List[ToolSchema]:\n",
        "    \"\"\"\n",
        "    Parse tool definitions from LLM prompts using regex.\n",
        "    \n",
        "    Expected format: tool_name(param: type = default) -> return_type - description\n",
        "    \"\"\"\n",
        "    # Pattern to match tool signatures\n",
        "    pattern = r'(\\w+)\\(([^)]*)\\)\\s*->\\s*(\\w+)\\s*-\\s*(.+?)(?=\\n\\w+\\(|$)'\n",
        "    matches = re.findall(pattern, prompt, re.DOTALL)\n",
        "    \n",
        "    tools = []\n",
        "    for match in matches:\n",
        "        tool_name, params_str, return_type, description = match\n",
        "        \n",
        "        # Parse parameters\n",
        "        args_schema = parse_params(params_str)\n",
        "        \n",
        "        tool = ToolSchema(\n",
        "            name=tool_name,\n",
        "            description=description.strip(),\n",
        "            args_schema=args_schema,\n",
        "            return_schema={\"type\": return_type}\n",
        "        )\n",
        "        tools.append(tool)\n",
        "    \n",
        "    return tools\n",
        "\n",
        "def parse_params(params_str: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Parse parameter string into schema dictionary.\n",
        "    \n",
        "    Format: param_name: type = default_value\n",
        "    \"\"\"\n",
        "    if not params_str.strip():\n",
        "        return {}\n",
        "    \n",
        "    # Split parameters by comma\n",
        "    params = [p.strip() for p in params_str.split(',') if p.strip()]\n",
        "    schema = {}\n",
        "    \n",
        "    for param in params:\n",
        "        if ':' in param:\n",
        "            parts = param.split(':', 1)\n",
        "            param_name = parts[0].strip()\n",
        "            type_and_default = parts[1].strip()\n",
        "            \n",
        "            # Extract type and default value\n",
        "            if '=' in type_and_default:\n",
        "                type_part, default_part = type_and_default.split('=', 1)\n",
        "                param_type = type_part.strip()\n",
        "                default_val = default_part.strip().strip('\"\\'')\n",
        "                schema[param_name] = {'type': param_type, 'default': default_val}\n",
        "            else:\n",
        "                param_type = type_and_default.strip()\n",
        "                schema[param_name] = {'type': param_type}\n",
        "    \n",
        "    return schema\n",
        "\n",
        "def identify_span_type(span: Dict[str, Any]) -> str:\n",
        "    \"\"\"\n",
        "    Identify span type based on attributes.\n",
        "    \"\"\"\n",
        "    attributes = span.get('attributes', {})\n",
        "    \n",
        "    # Check for agent attributes\n",
        "    if any(key.startswith('agent.') for key in attributes.keys()):\n",
        "        return 'agent'\n",
        "    \n",
        "    # Check for LLM attributes\n",
        "    if any(key.startswith('llm.') for key in attributes.keys()):\n",
        "        return 'llm'\n",
        "    \n",
        "    # Check for tool attributes\n",
        "    if any(key.startswith('tool.') for key in attributes.keys()):\n",
        "        return 'tool'\n",
        "    \n",
        "    return 'unknown'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Field mapping functions defined!\n"
          ]
        }
      ],
      "source": [
        "def map_span_to_agent_data(span: Dict[str, Any]) -> AgentData:\n",
        "    \"\"\"\n",
        "    Map a single span from dataset.json to AgentData format.\n",
        "    \"\"\"\n",
        "    attributes = span.get('attributes', {})\n",
        "    events = span.get('events', [])\n",
        "    span_type = identify_span_type(span)\n",
        "    \n",
        "    # Base mappings\n",
        "    data = {\n",
        "        'user_id': span.get('metadata', {}).get('user_id', None),\n",
        "        'task_id': span.get('trace_id'),\n",
        "        'turn_id': span.get('span_id'),\n",
        "        'ground_truth': None,\n",
        "        'expected_tool_call': None,\n",
        "        'agent_name': span_type,\n",
        "        'agent_role': span_type,\n",
        "        'system_prompt': None,\n",
        "        'metadata': None,\n",
        "        'exit_status': span.get('status'),\n",
        "        'tools_available': [],\n",
        "        'tool_calls': [],\n",
        "        'parameters_passed': {},\n",
        "        'tool_call_results': [],\n",
        "        'retrieval_query': None,\n",
        "        'retrieved_context': None,\n",
        "        'agent_exit': False,\n",
        "        'trace': None\n",
        "    }\n",
        "    \n",
        "    # Span-specific mappings\n",
        "    if span_type == 'agent':\n",
        "        # Agent task\n",
        "        chain_inputs = attributes.get('chain.inputs', {})\n",
        "        if isinstance(chain_inputs, dict) and 'input' in chain_inputs:\n",
        "            data['agent_task'] = chain_inputs['input']\n",
        "        \n",
        "        # Agent response\n",
        "        finish_values = attributes.get('agent.output.finish.return_values', {})\n",
        "        if isinstance(finish_values, dict) and 'output' in finish_values:\n",
        "            data['agent_response'] = finish_values['output']\n",
        "        \n",
        "        # Tool calls from agent actions\n",
        "        tool_name = attributes.get('agent.output.action.tool')\n",
        "        tool_input = attributes.get('agent.output.action.tool_input')\n",
        "        \n",
        "        if tool_name:\n",
        "            tool_call = ToolCall(\n",
        "                tool_name=tool_name,\n",
        "                parameters={'input': tool_input} if tool_input else {},\n",
        "                call_id=span['span_id']\n",
        "            )\n",
        "            data['tool_calls'] = [tool_call]\n",
        "            data['parameters_passed'] = {'input': tool_input} if tool_input else {}\n",
        "            \n",
        "            # Handle retrieval query for langchain_retriever\n",
        "            if tool_name == 'langchain_retriever' and tool_input:\n",
        "                data['retrieval_query'] = [tool_input]\n",
        "        \n",
        "        # Agent exit status\n",
        "        data['agent_exit'] = any(event.get('name') == 'agent_finish' for event in events)\n",
        "        \n",
        "        # Trace (dump events as JSON)\n",
        "        if events:\n",
        "            data['trace'] = json.dumps(events)\n",
        "    \n",
        "    elif span_type == 'llm':\n",
        "        # Agent response from LLM output\n",
        "        llm_responses = attributes.get('llm.output.response', [])\n",
        "        if llm_responses:\n",
        "            data['agent_response'] = llm_responses[0]\n",
        "        \n",
        "        # Parse tools from prompt\n",
        "        prompts = attributes.get('llm.input.prompts', [])\n",
        "        if prompts:\n",
        "            try:\n",
        "                tools = parse_tools_from_prompt(prompts[0])\n",
        "                data['tools_available'] = tools\n",
        "            except Exception:\n",
        "                # Fallback to empty list if parsing fails\n",
        "                data['tools_available'] = []\n",
        "        \n",
        "        data['parameters_passed'] = {}\n",
        "    \n",
        "    elif span_type == 'tool':\n",
        "        # Agent response from tool output\n",
        "        tool_output = attributes.get('tool.output.output')\n",
        "        if tool_output:\n",
        "            data['agent_response'] = tool_output\n",
        "        \n",
        "        # Tool call results\n",
        "        tool_name = attributes.get('tool.name')\n",
        "        if tool_name and tool_output is not None:\n",
        "            tool_result = ToolResult(\n",
        "                call_id=span['span_id'],\n",
        "                result=tool_output,\n",
        "                success=span.get('status') == 'ok',\n",
        "                error_message=None if span.get('status') == 'ok' else 'Tool execution failed'\n",
        "            )\n",
        "            data['tool_call_results'] = [tool_result]\n",
        "            \n",
        "            # Handle retrieved context for langchain_retriever\n",
        "            if tool_name == 'langchain_retriever':\n",
        "                data['retrieved_context'] = [[tool_output]]\n",
        "        \n",
        "        # Parameters from tool input\n",
        "        tool_input_keys = [key for key in attributes.keys() if key.startswith('tool.input.')]\n",
        "        tool_params = {}\n",
        "        for key in tool_input_keys:\n",
        "            param_name = key.replace('tool.input.', '')\n",
        "            tool_params[param_name] = attributes[key]\n",
        "        data['parameters_passed'] = tool_params\n",
        "    \n",
        "    return AgentData(**data)\n",
        "\n",
        "print(\"âœ… Field mapping functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create AgentDataset from Spans\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ Converting spans to AgentData objects...\n",
            "\n",
            "âœ… Successfully converted 56 spans to AgentData\n",
            "ðŸ“Š AgentDataset created with 56 records\n"
          ]
        }
      ],
      "source": [
        "# Convert spans to AgentData objects\n",
        "print(\"ðŸ”„ Converting spans to AgentData objects...\")\n",
        "\n",
        "agent_data_list = []\n",
        "errors = []\n",
        "\n",
        "for i, span in enumerate(spans_data):\n",
        "    try:\n",
        "        agent_data = map_span_to_agent_data(span)\n",
        "        agent_data_list.append(agent_data)\n",
        "    except Exception as e:\n",
        "        errors.append(f\"Span {i}: {str(e)}\")\n",
        "        if len(errors) <= 5:  # Show first 5 errors only\n",
        "            print(f\"âš ï¸  Error processing span {i}: {e}\")\n",
        "\n",
        "print(f\"\\nâœ… Successfully converted {len(agent_data_list)} spans to AgentData\")\n",
        "if errors:\n",
        "    print(f\"âŒ {len(errors)} spans had errors\")\n",
        "\n",
        "# Create AgentDataset\n",
        "dataset = AgentDataset()\n",
        "dataset.data = agent_data_list\n",
        "\n",
        "print(f\"ðŸ“Š AgentDataset created with {len(dataset.data)} records\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Examine Sample Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“ˆ Dataset Statistics:\n",
            "\n",
            "Agent Types: {'agent': 21, 'llm': 28, 'tool': 7}\n",
            "Records with responses: 43\n",
            "Records with tool calls: 20\n",
            "Records with retrieval: 4\n",
            "Tool usage: {'user_input': 12, 'escalate_to_human': 4, 'langchain_retriever': 4}\n",
            "\n",
            "ðŸ” Sample AgentData records:\n",
            "\n",
            "--- Record 1 (agent) ---\n",
            "Task: Keep using user_input tool, until i tell you to exit...\n",
            "Response: The user's query has been escalated to human customer support for further assistance....\n",
            "Tool calls: 1\n",
            "Exit status: ok\n",
            "\n",
            "--- Record 2 (llm) ---\n",
            "Task: None...\n",
            "Response: I need to prompt the user for more information until instructed otherwise.\n",
            "Action: user_input\n",
            "Action...\n",
            "Tool calls: 0\n",
            "Exit status: ok\n",
            "\n",
            "--- Record 3 (agent) ---\n",
            "Task: None...\n",
            "Response: None...\n",
            "Tool calls: 1\n",
            "Exit status: ok\n"
          ]
        }
      ],
      "source": [
        "# Show statistics about the dataset\n",
        "print(\"ðŸ“ˆ Dataset Statistics:\")\n",
        "\n",
        "agent_types = {}\n",
        "tool_usage = {}\n",
        "with_responses = 0\n",
        "with_tool_calls = 0\n",
        "with_retrieval = 0\n",
        "\n",
        "for data in dataset.data:\n",
        "    # Agent types\n",
        "    if data.agent_name:\n",
        "        agent_types[data.agent_name] = agent_types.get(data.agent_name, 0) + 1\n",
        "    \n",
        "    # Responses\n",
        "    if data.agent_response:\n",
        "        with_responses += 1\n",
        "    \n",
        "    # Tool calls\n",
        "    if data.tool_calls:\n",
        "        with_tool_calls += 1\n",
        "        for tool_call in data.tool_calls:\n",
        "            if hasattr(tool_call, 'tool_name'):\n",
        "                tool_usage[tool_call.tool_name] = tool_usage.get(tool_call.tool_name, 0) + 1\n",
        "    \n",
        "    # Retrieval\n",
        "    if data.retrieval_query:\n",
        "        with_retrieval += 1\n",
        "\n",
        "print(f\"\\nAgent Types: {dict(agent_types)}\")\n",
        "print(f\"Records with responses: {with_responses}\")\n",
        "print(f\"Records with tool calls: {with_tool_calls}\")\n",
        "print(f\"Records with retrieval: {with_retrieval}\")\n",
        "print(f\"Tool usage: {dict(tool_usage)}\")\n",
        "\n",
        "# Show sample records\n",
        "print(\"\\nðŸ” Sample AgentData records:\")\n",
        "for i, data in enumerate(dataset.data[:3]):\n",
        "    print(f\"\\n--- Record {i+1} ({data.agent_name}) ---\")\n",
        "    print(f\"Task: {data.agent_task[:100] if data.agent_task else 'None'}...\")\n",
        "    print(f\"Response: {data.agent_response[:100] if data.agent_response else 'None'}...\")\n",
        "    print(f\"Tool calls: {len(data.tool_calls) if data.tool_calls else 0}\")\n",
        "    print(f\"Exit status: {data.exit_status}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Setup Gemini Model and Evaluator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… GEMINI_API_KEY found in environment\n",
            "2025-09-12 03:55:19 - INFO - novaeval.models.base - Noveum tracing initialized successfully\n",
            "âœ… Gemini model initialized\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Check for API key\n",
        "if 'GEMINI_API_KEY' not in os.environ:\n",
        "    print(\"âš ï¸  GEMINI_API_KEY environment variable not set!\")\n",
        "    print(\"Please set it before running evaluation:\")\n",
        "    print(\"export GEMINI_API_KEY='your-api-key-here'\")\n",
        "else:\n",
        "    print(\"âœ… GEMINI_API_KEY found in environment\")\n",
        "\n",
        "# Initialize Gemini model\n",
        "try:\n",
        "    gemini_model = GeminiModel(\n",
        "        model_name=\"gemini-1.5-flash\",  # Using flash model for cost efficiency\n",
        "        temperature=0.1,  # Low temperature for consistent evaluation\n",
        "        max_tokens=1024\n",
        "    )\n",
        "    print(\"âœ… Gemini model initialized\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error initializing Gemini model: {e}\")\n",
        "    gemini_model = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Initialized 6 scoring functions:\n",
            "  - task_progression_scorer\n",
            "  - context_relevancy_scorer\n",
            "  - role_adherence_scorer\n",
            "  - tool_relevancy_scorer\n",
            "  - tool_correctness_scorer\n",
            "  - parameter_correctness_scorer\n",
            "\n",
            "âœ… AgentEvaluator created with Gemini model and scoring functions\n"
          ]
        }
      ],
      "source": [
        "# Initialize scoring functions for evaluation\n",
        "scoring_functions = [\n",
        "    task_progression_scorer,\n",
        "    context_relevancy_scorer,\n",
        "    role_adherence_scorer,\n",
        "    tool_relevancy_scorer,\n",
        "    tool_correctness_scorer,\n",
        "    parameter_correctness_scorer\n",
        "]\n",
        "\n",
        "print(f\"âœ… Initialized {len(scoring_functions)} scoring functions:\")\n",
        "for func in scoring_functions:\n",
        "    print(f\"  - {func.__name__}\")\n",
        "\n",
        "# Create AgentEvaluator\n",
        "if gemini_model:\n",
        "    evaluator = AgentEvaluator(\n",
        "        agent_dataset=dataset,\n",
        "        models=[gemini_model],\n",
        "        scoring_functions=scoring_functions,\n",
        "        output_dir=\"./demo_results\",\n",
        "        stream=False,\n",
        "        include_reasoning=True\n",
        "    )\n",
        "    print(\"\\nâœ… AgentEvaluator created with Gemini model and scoring functions\")\n",
        "else:\n",
        "    print(\"\\nâŒ Cannot create evaluator - Gemini model not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Run Evaluation (Sample)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Running evaluation on sample data...\n",
            "\n",
            "ðŸ“Š Evaluating 10 sample records...\n",
            "2025-09-12 03:57:37 - INFO - novaeval.evaluators.agent_evaluator - Starting agent evaluation process\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating samples: 0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-12 03:57:37 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-09-12 03:57:39 - INFO - google_genai.models - AFC remote call 1 is done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-12 03:57:39 - noveum_trace.transport.http_transport - INFO - ðŸ“¤ EXPORTING TRACE: auto_trace_generate (ID: 874be6a5-be94-4610-a19f-bfb30da97e00) - 1 spans\n",
            "2025-09-12 03:57:39 - noveum_trace.transport.batch_processor - INFO - ðŸ“¥ ADDING TRACE TO QUEUE: auto_trace_generate (ID: 874be6a5-be94-4610-a19f-bfb30da97e00) - 1 spans\n",
            "2025-09-12 03:57:39 - noveum_trace.transport.batch_processor - INFO - âœ… Successfully queued trace 874be6a5-be94-4610-a19f-bfb30da97e00\n",
            "2025-09-12 03:57:39 - noveum_trace.transport.http_transport - INFO - âœ… Trace 874be6a5-be94-4610-a19f-bfb30da97e00 successfully queued for export\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-12 03:57:39 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-09-12 03:57:41 - INFO - google_genai.models - AFC remote call 1 is done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-12 03:57:41 - noveum_trace.transport.http_transport - INFO - ðŸ“¤ EXPORTING TRACE: auto_trace_generate (ID: 0fafae84-5bb8-441b-8b75-45525249011b) - 1 spans\n",
            "2025-09-12 03:57:41 - noveum_trace.transport.batch_processor - INFO - ðŸ“¥ ADDING TRACE TO QUEUE: auto_trace_generate (ID: 0fafae84-5bb8-441b-8b75-45525249011b) - 1 spans\n",
            "2025-09-12 03:57:41 - noveum_trace.transport.batch_processor - INFO - âœ… Successfully queued trace 0fafae84-5bb8-441b-8b75-45525249011b\n",
            "2025-09-12 03:57:41 - noveum_trace.transport.http_transport - INFO - âœ… Trace 0fafae84-5bb8-441b-8b75-45525249011b successfully queued for export\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-12 03:57:41 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-09-12 03:57:42 - INFO - google_genai.models - AFC remote call 1 is done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-12 03:57:42 - noveum_trace.transport.http_transport - INFO - ðŸ“¤ EXPORTING TRACE: auto_trace_generate (ID: 2c23140a-365e-4197-93fc-91e24e585062) - 1 spans\n",
            "2025-09-12 03:57:42 - noveum_trace.transport.batch_processor - INFO - ðŸ“¥ ADDING TRACE TO QUEUE: auto_trace_generate (ID: 2c23140a-365e-4197-93fc-91e24e585062) - 1 spans\n",
            "2025-09-12 03:57:42 - noveum_trace.transport.batch_processor - INFO - âœ… Successfully queued trace 2c23140a-365e-4197-93fc-91e24e585062\n",
            "2025-09-12 03:57:42 - noveum_trace.transport.http_transport - INFO - âœ… Trace 2c23140a-365e-4197-93fc-91e24e585062 successfully queued for export\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-12 03:57:42 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-09-12 03:57:43 - INFO - google_genai.models - AFC remote call 1 is done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-12 03:57:43 - noveum_trace.transport.http_transport - INFO - ðŸ“¤ EXPORTING TRACE: auto_trace_generate (ID: 69c93676-ced1-42e7-9da8-2886c6035fcc) - 1 spans\n",
            "2025-09-12 03:57:43 - noveum_trace.transport.batch_processor - INFO - ðŸ“¥ ADDING TRACE TO QUEUE: auto_trace_generate (ID: 69c93676-ced1-42e7-9da8-2886c6035fcc) - 1 spans\n",
            "2025-09-12 03:57:43 - noveum_trace.transport.batch_processor - INFO - âœ… Successfully queued trace 69c93676-ced1-42e7-9da8-2886c6035fcc\n",
            "2025-09-12 03:57:43 - noveum_trace.transport.http_transport - INFO - âœ… Trace 69c93676-ced1-42e7-9da8-2886c6035fcc successfully queued for export\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-12 03:57:44 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 1 samples\n",
            "2025-09-12 03:57:44 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/sample_evaluation/agent_evaluation_results.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating samples: 1it [00:06,  6.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-12 03:57:44 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 2 samples\n",
            "2025-09-12 03:57:44 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/sample_evaluation/agent_evaluation_results.csv\n",
            "2025-09-12 03:57:44 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 3 samples\n",
            "2025-09-12 03:57:44 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/sample_evaluation/agent_evaluation_results.csv\n",
            "2025-09-12 03:57:44 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 4 samples\n",
            "2025-09-12 03:57:44 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/sample_evaluation/agent_evaluation_results.csv\n",
            "2025-09-12 03:57:44 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 5 samples\n",
            "2025-09-12 03:57:44 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/sample_evaluation/agent_evaluation_results.csv\n",
            "2025-09-12 03:57:44 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 6 samples\n",
            "2025-09-12 03:57:44 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/sample_evaluation/agent_evaluation_results.csv\n",
            "2025-09-12 03:57:44 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 7 samples\n",
            "2025-09-12 03:57:44 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/sample_evaluation/agent_evaluation_results.csv\n",
            "2025-09-12 03:57:44 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 8 samples\n",
            "2025-09-12 03:57:44 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/sample_evaluation/agent_evaluation_results.csv\n",
            "2025-09-12 03:57:44 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-09-12 03:57:45 - INFO - google_genai.models - AFC remote call 1 is done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-12 03:57:45 - noveum_trace.transport.http_transport - INFO - ðŸ“¤ EXPORTING TRACE: auto_trace_generate (ID: aa836924-71ce-42c4-b0fd-0d6e5dec735e) - 1 spans\n",
            "2025-09-12 03:57:45 - noveum_trace.transport.batch_processor - INFO - ðŸ“¥ ADDING TRACE TO QUEUE: auto_trace_generate (ID: aa836924-71ce-42c4-b0fd-0d6e5dec735e) - 1 spans\n",
            "2025-09-12 03:57:45 - noveum_trace.transport.batch_processor - INFO - âœ… Successfully queued trace aa836924-71ce-42c4-b0fd-0d6e5dec735e\n",
            "2025-09-12 03:57:45 - noveum_trace.transport.http_transport - INFO - âœ… Trace aa836924-71ce-42c4-b0fd-0d6e5dec735e successfully queued for export\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-12 03:57:45 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-09-12 03:57:46 - INFO - google_genai.models - AFC remote call 1 is done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-12 03:57:46 - noveum_trace.transport.http_transport - INFO - ðŸ“¤ EXPORTING TRACE: auto_trace_generate (ID: cd151e32-5147-467b-9c6a-e92e2becfe69) - 1 spans\n",
            "2025-09-12 03:57:46 - noveum_trace.transport.batch_processor - INFO - ðŸ“¥ ADDING TRACE TO QUEUE: auto_trace_generate (ID: cd151e32-5147-467b-9c6a-e92e2becfe69) - 1 spans\n",
            "2025-09-12 03:57:46 - noveum_trace.transport.batch_processor - INFO - âœ… Successfully queued trace cd151e32-5147-467b-9c6a-e92e2becfe69\n",
            "2025-09-12 03:57:46 - noveum_trace.transport.http_transport - INFO - âœ… Trace cd151e32-5147-467b-9c6a-e92e2becfe69 successfully queued for export\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-12 03:57:46 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-09-12 03:57:48 - INFO - google_genai.models - AFC remote call 1 is done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-12 03:57:48 - noveum_trace.transport.http_transport - INFO - ðŸ“¤ EXPORTING TRACE: auto_trace_generate (ID: 20fef7a0-c44e-49d3-9a83-93fb7bb8075c) - 1 spans\n",
            "2025-09-12 03:57:48 - noveum_trace.transport.batch_processor - INFO - ðŸ“¥ ADDING TRACE TO QUEUE: auto_trace_generate (ID: 20fef7a0-c44e-49d3-9a83-93fb7bb8075c) - 1 spans\n",
            "2025-09-12 03:57:48 - noveum_trace.transport.batch_processor - INFO - âœ… Successfully queued trace 20fef7a0-c44e-49d3-9a83-93fb7bb8075c\n",
            "2025-09-12 03:57:48 - noveum_trace.transport.http_transport - INFO - âœ… Trace 20fef7a0-c44e-49d3-9a83-93fb7bb8075c successfully queued for export\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-12 03:57:48 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-09-12 03:57:50 - INFO - google_genai.models - AFC remote call 1 is done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-12 03:57:50 - noveum_trace.transport.http_transport - INFO - ðŸ“¤ EXPORTING TRACE: auto_trace_generate (ID: c8f743bc-2c02-4c2a-93cb-3314c2998e68) - 1 spans\n",
            "2025-09-12 03:57:50 - noveum_trace.transport.batch_processor - INFO - ðŸ“¥ ADDING TRACE TO QUEUE: auto_trace_generate (ID: c8f743bc-2c02-4c2a-93cb-3314c2998e68) - 1 spans\n",
            "2025-09-12 03:57:50 - noveum_trace.transport.batch_processor - INFO - âœ… Successfully queued trace c8f743bc-2c02-4c2a-93cb-3314c2998e68\n",
            "2025-09-12 03:57:50 - noveum_trace.transport.http_transport - INFO - âœ… Trace c8f743bc-2c02-4c2a-93cb-3314c2998e68 successfully queued for export\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-12 03:57:50 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 9 samples\n",
            "2025-09-12 03:57:50 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/sample_evaluation/agent_evaluation_results.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating samples: 9it [00:12,  1.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-12 03:57:50 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 10 samples\n",
            "2025-09-12 03:57:50 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/sample_evaluation/agent_evaluation_results.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating samples: 10it [00:12,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-12 03:57:50 - INFO - novaeval.evaluators.agent_evaluator - Saving final results\n",
            "2025-09-12 03:57:50 - INFO - novaeval.evaluators.agent_evaluator - Reloaded 10 results from CSV\n",
            "2025-09-12 03:57:50 - INFO - novaeval.evaluators.agent_evaluator - Agent evaluation completed\n",
            "\n",
            "âœ… Evaluation completed!\n",
            "\n",
            "ðŸ“Š Results Summary:\n",
            "  - task_progression: 0.00\n",
            "  - context_relevancy: 1.10\n",
            "  - role_adherence: 1.23\n",
            "  - tool_relevancy: 1.30\n",
            "  - tool_correctness: 0.00\n",
            "  - parameter_correctness: 1.40\n",
            "\n",
            "ðŸ” Individual Scores:\n",
            "\n",
            "  Record 1 (Task: 27a8252b-43e5-48d1-8707-12c8537feff8):\n",
            "    - task_progression: 0.0\n",
            "    - context_relevancy: 4.2\n",
            "    - role_adherence: 7.8\n",
            "    - tool_relevancy: 6.5\n",
            "    - tool_correctness: 0.0\n",
            "    - parameter_correctness: 6.5\n",
            "\n",
            "  Record 2 (Task: 27a8252b-43e5-48d1-8707-12c8537feff8):\n",
            "    - task_progression: 0.0\n",
            "    - context_relevancy: 0.0\n",
            "    - role_adherence: 0.0\n",
            "    - tool_relevancy: 0.0\n",
            "    - tool_correctness: 0.0\n",
            "    - parameter_correctness: 0.0\n",
            "\n",
            "  Record 3 (Task: 27a8252b-43e5-48d1-8707-12c8537feff8):\n",
            "    - task_progression: 0.0\n",
            "    - context_relevancy: 0.0\n",
            "    - role_adherence: 0.0\n",
            "    - tool_relevancy: 0.0\n",
            "    - tool_correctness: 0.0\n",
            "    - parameter_correctness: 0.0\n",
            "\n",
            "  Record 4 (Task: 27a8252b-43e5-48d1-8707-12c8537feff8):\n",
            "    - task_progression: 0.0\n",
            "    - context_relevancy: 0.0\n",
            "    - role_adherence: 0.0\n",
            "    - tool_relevancy: 0.0\n",
            "    - tool_correctness: 0.0\n",
            "    - parameter_correctness: 0.0\n",
            "\n",
            "  Record 5 (Task: 27a8252b-43e5-48d1-8707-12c8537feff8):\n",
            "    - task_progression: 0.0\n",
            "    - context_relevancy: 0.0\n",
            "    - role_adherence: 0.0\n",
            "    - tool_relevancy: 0.0\n",
            "    - tool_correctness: 0.0\n",
            "    - parameter_correctness: 0.0\n",
            "\n",
            "  Record 6 (Task: 27a8252b-43e5-48d1-8707-12c8537feff8):\n",
            "    - task_progression: 0.0\n",
            "    - context_relevancy: 0.0\n",
            "    - role_adherence: 0.0\n",
            "    - tool_relevancy: 0.0\n",
            "    - tool_correctness: 0.0\n",
            "    - parameter_correctness: 0.0\n",
            "\n",
            "  Record 7 (Task: 27a8252b-43e5-48d1-8707-12c8537feff8):\n",
            "    - task_progression: 0.0\n",
            "    - context_relevancy: 0.0\n",
            "    - role_adherence: 0.0\n",
            "    - tool_relevancy: 0.0\n",
            "    - tool_correctness: 0.0\n",
            "    - parameter_correctness: 0.0\n",
            "\n",
            "  Record 8 (Task: 27a8252b-43e5-48d1-8707-12c8537feff8):\n",
            "    - task_progression: 0.0\n",
            "    - context_relevancy: 0.0\n",
            "    - role_adherence: 0.0\n",
            "    - tool_relevancy: 0.0\n",
            "    - tool_correctness: 0.0\n",
            "    - parameter_correctness: 0.0\n",
            "\n",
            "  Record 9 (Task: b6370077-daa0-43ae-937e-907a0e0987ac):\n",
            "    - task_progression: 0.0\n",
            "    - context_relevancy: 6.8\n",
            "    - role_adherence: 4.5\n",
            "    - tool_relevancy: 6.5\n",
            "    - tool_correctness: 0.0\n",
            "    - parameter_correctness: 7.5\n",
            "\n",
            "  Record 10 (Task: b6370077-daa0-43ae-937e-907a0e0987ac):\n",
            "    - task_progression: 0.0\n",
            "    - context_relevancy: 0.0\n",
            "    - role_adherence: 0.0\n",
            "    - tool_relevancy: 0.0\n",
            "    - tool_correctness: 0.0\n",
            "    - parameter_correctness: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Run evaluation using the AgentEvaluator's run_all method\n",
        "print(\"ðŸš€ Running evaluation on sample data...\")\n",
        "\n",
        "if gemini_model and evaluator:\n",
        "    try:\n",
        "        # Create a smaller dataset for demo purposes\n",
        "        sample_data = [data for data in dataset.data if data.agent_response][:10]\n",
        "        print(f\"\\nðŸ“Š Evaluating {len(sample_data)} sample records...\")\n",
        "        \n",
        "        # Create a temporary dataset with just the sample data\n",
        "        sample_dataset = AgentDataset()\n",
        "        sample_dataset.data = sample_data\n",
        "        \n",
        "        # Create a new evaluator with the sample dataset\n",
        "        sample_evaluator = AgentEvaluator(\n",
        "            agent_dataset=sample_dataset,\n",
        "            models=[gemini_model],\n",
        "            scoring_functions=scoring_functions,\n",
        "            output_dir=\"./demo_results/sample_evaluation\",\n",
        "            stream=False,\n",
        "            include_reasoning=True\n",
        "        )\n",
        "        \n",
        "        # Run the evaluation\n",
        "        sample_evaluator.run_all(save_every=1, file_type=\"csv\")\n",
        "        \n",
        "        print(\"\\nâœ… Evaluation completed!\")\n",
        "        \n",
        "        # Read and display results\n",
        "        import pandas as pd\n",
        "        results_file = \"./demo_results/sample_evaluation/agent_evaluation_results.csv\"\n",
        "        \n",
        "        if pd.io.common.file_exists(results_file):\n",
        "            results_df = pd.read_csv(results_file)\n",
        "            print(f\"\\nðŸ“Š Results Summary:\")\n",
        "            \n",
        "            # Calculate averages for each scorer\n",
        "            scorer_columns = [col for col in results_df.columns if col not in ['user_id', 'task_id', 'turn_id', 'agent_name'] and not col.endswith('_reasoning')]\n",
        "            \n",
        "            for col in scorer_columns:\n",
        "                if results_df[col].dtype in ['float64', 'int64']:\n",
        "                    avg_score = results_df[col].mean()\n",
        "                    print(f\"  - {col}: {avg_score:.2f}\")\n",
        "            \n",
        "            # Show individual scores\n",
        "            print(f\"\\nðŸ” Individual Scores:\")\n",
        "            for i, row in results_df.iterrows():\n",
        "                print(f\"\\n  Record {i+1} (Task: {row.get('task_id', 'N/A')}):\")\n",
        "                for col in scorer_columns:\n",
        "                    if pd.notna(row[col]):\n",
        "                        print(f\"    - {col}: {row[col]}\")\n",
        "        else:\n",
        "            print(\"âŒ Results file not found\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during evaluation: {e}\")\n",
        "        print(f\"Error type: {type(e).__name__}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        \n",
        "else:\n",
        "    print(\"âš ï¸  Skipping evaluation - missing model or evaluator\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Analysis and Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ” Dataset Analysis:\n",
            "\n",
            "=== Agent Behavior Patterns ===\n",
            "\n",
            "ðŸ“ˆ Tool Usage:\n",
            "  - user_input: 12 uses\n",
            "  - escalate_to_human: 4 uses\n",
            "  - langchain_retriever: 4 uses\n",
            "\n",
            "ðŸ“‹ Task Types:\n",
            "  - user_input: 1\n",
            "  - other: 4\n",
            "  - exit_command: 3\n",
            "\n",
            "ðŸ“ Response Statistics:\n",
            "  - Average response length: 120.8 characters\n",
            "  - Min response length: 3\n",
            "  - Max response length: 239\n"
          ]
        }
      ],
      "source": [
        "# Analyze the dataset characteristics\n",
        "print(\"ðŸ” Dataset Analysis:\")\n",
        "print(\"\\n=== Agent Behavior Patterns ===\")\n",
        "\n",
        "# Analyze tool usage patterns\n",
        "tool_patterns = {}\n",
        "task_types = {}\n",
        "response_lengths = []\n",
        "\n",
        "for data in dataset.data:\n",
        "    # Tool usage\n",
        "    if data.tool_calls:\n",
        "        for tool_call in data.tool_calls:\n",
        "            if hasattr(tool_call, 'tool_name'):\n",
        "                tool_name = tool_call.tool_name\n",
        "                if tool_name not in tool_patterns:\n",
        "                    tool_patterns[tool_name] = {'count': 0, 'success_rate': 0}\n",
        "                tool_patterns[tool_name]['count'] += 1\n",
        "    \n",
        "    # Task analysis\n",
        "    if data.agent_task:\n",
        "        # Simple categorization\n",
        "        task_lower = data.agent_task.lower()\n",
        "        if 'user_input' in task_lower:\n",
        "            task_types['user_input'] = task_types.get('user_input', 0) + 1\n",
        "        elif 'exit' in task_lower:\n",
        "            task_types['exit_command'] = task_types.get('exit_command', 0) + 1\n",
        "        else:\n",
        "            task_types['other'] = task_types.get('other', 0) + 1\n",
        "    \n",
        "    # Response analysis\n",
        "    if data.agent_response:\n",
        "        response_lengths.append(len(data.agent_response))\n",
        "\n",
        "print(f\"\\nðŸ“ˆ Tool Usage:\")\n",
        "for tool, stats in tool_patterns.items():\n",
        "    print(f\"  - {tool}: {stats['count']} uses\")\n",
        "\n",
        "print(f\"\\nðŸ“‹ Task Types:\")\n",
        "for task_type, count in task_types.items():\n",
        "    print(f\"  - {task_type}: {count}\")\n",
        "\n",
        "if response_lengths:\n",
        "    avg_response_length = sum(response_lengths) / len(response_lengths)\n",
        "    print(f\"\\nðŸ“ Response Statistics:\")\n",
        "    print(f\"  - Average response length: {avg_response_length:.1f} characters\")\n",
        "    print(f\"  - Min response length: {min(response_lengths)}\")\n",
        "    print(f\"  - Max response length: {max(response_lengths)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Export Results (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¾ Exporting processed dataset...\n",
            "âœ… Exported to processed_agent_dataset.json\n",
            "âœ… Exported to processed_agent_dataset.csv\n",
            "\n",
            "ðŸŽ‰ Demo completed successfully!\n",
            "\n",
            "ðŸ“‹ Summary:\n",
            "  - Processed 56 spans from dataset.json\n",
            "  - Created 56 AgentData records\n",
            "  - Configured evaluation with Gemini model and 3 scorers\n",
            "  - Exported processed dataset for future use\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Export the processed dataset for future use\n",
        "print(\"ðŸ’¾ Exporting processed dataset...\")\n",
        "\n",
        "try:\n",
        "    # Export to JSON\n",
        "    dataset.export_to_json('processed_agent_dataset.json')\n",
        "    print(\"âœ… Exported to processed_agent_dataset.json\")\n",
        "    \n",
        "    # Export to CSV (optional)\n",
        "    dataset.export_to_csv('processed_agent_dataset.csv')\n",
        "    print(\"âœ… Exported to processed_agent_dataset.csv\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Export error: {e}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ Demo completed successfully!\")\n",
        "print(\"\\nðŸ“‹ Summary:\")\n",
        "print(f\"  - Processed {len(spans_data)} spans from dataset.json\")\n",
        "print(f\"  - Created {len(dataset.data)} AgentData records\")\n",
        "print(f\"  - Configured evaluation with Gemini model and 3 scorers\")\n",
        "if 'results' in locals():\n",
        "    print(f\"  - Successfully evaluated sample data\")\n",
        "print(f\"  - Exported processed dataset for future use\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
