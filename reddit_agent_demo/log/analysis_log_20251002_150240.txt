
==================================================
TIMESTAMP: 2025-10-02 15:03:08
DESCRIPTION: email_gen_send_dataset - task_progression Analysis
==================================================
Analysis for scorer: task_progression
Scorer Name: Task Progression
Reasoning:
Based on the provided data, the agent generally performs well, but the lower scores (8.5) indicate specific areas for improvement. The key reasons highlighted by the scorer are:

*   **Incomplete Confirmation:** The agent gets very close to finishing the task, but in some instances, the final outcome is not yet confirmed.
*   **Lack of Context and Error Handling:** The scorer noted that there is room for improvement by providing additional context or incorporating error handling.
*   **Output Formatting:** There were minor issues with the formatting of the output that could be improved.
*   **Unspecified Shortcomings:** In one case, the reasoning mentioned "very few shortcomings" without specifying them, preventing the score from being higher.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:03:21
DESCRIPTION: email_gen_send_dataset - context_relevancy Analysis
==================================================
Analysis for scorer: context_relevancy
Scorer Name: context_relevancy
Reasoning:
Based on the scorer's feedback, while the agent successfully executes its tool calls, it consistently fails to achieve a higher score due to a lack of depth and surrounding information in its responses. The key reasons for the lower scores are:

*   **Lack of Additional Context:** The responses are too direct and minimal, often just presenting the tool's output without any surrounding context or justification for the actions performed.
*   **Insufficient Detail:** The agent's output could be more informative and provide more clarity on the specifics of the actions taken (e.g., more details about a generated report).
*   **Missing Status Indicators:** The agent does not explicitly provide context on the success or failure of the task completion.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:03:28
DESCRIPTION: email_gen_send_dataset - role_adherence Analysis
==================================================
Analysis for scorer: role_adherence
Scorer Name: role_adherence
Reasoning: 
All provided samples for this scorer have high scores (ranging from 9.0 to 9.5). The reasoning consistently indicates that the agent is performing its role perfectly by flawlessly executing tool calls as programmed and adhering to its designated task without any deviations. Therefore, based on the data provided, there are no low scores to analyze for failure reasons.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:03:38
DESCRIPTION: email_gen_send_dataset - tool_relevancy Analysis
==================================================
Analysis for scorer: tool_relevancy
Scorer Name: tool_relevancy
Reasoning: All the provided samples with low scores indicate a "Missing required fields" error. As per the instructions, these are code-level issues and have been excluded from the analysis of the agent's performance. There are no other reasons for low scores in the data provided.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:03:48
DESCRIPTION: email_gen_send_dataset - parameter_correctness Analysis
==================================================
Analysis for scorer: parameter_correctness
Scorer Name: parameter_correctness
Reasoning: 
All the provided low-scoring samples for this scorer indicated a "Missing required fields" error. As per the instructions, this is a developer-side code issue and has been excluded from the analysis of the agent's performance. Therefore, no specific agent-related failure reasons could be extracted from this data.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:03:58
DESCRIPTION: email_gen_send_dataset - Dataset Summary
==================================================
Agent Name: email_gen_send_dataset
Reasoning: 
Based on the analysis, the agent's primary issue is the lack of depth and context in its responses. While it executes its tool calls correctly, its outputs are too direct and minimal. The agent fails to provide surrounding information, justification for its actions, or status indicators for task success or failure. Additionally, it sometimes gets very close to completing the task but does not provide a final confirmation of the outcome. Minor output formatting issues were also noted.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:04:14
DESCRIPTION: agent_comment_gen_dataset - task_progression Analysis
==================================================
Analysis for scorer: task_progression
Scorer Name: Task Progression
Reasoning:
Based on the scorer's feedback, the agent is failing to progress in the task due to the following key reasons:

*   **Fundamental Misunderstanding of the Task:** In some instances, the agent completely fails to understand its role and the assigned task, leading to responses that are irrelevant, off-topic, and unhelpful.
*   **Irrelevant API/Link Suggestions:** A common failure pattern is the agent suggesting an API or providing a link that is completely unrelated to the user's problem, even if it correctly identifies the general topic.
*   **Incorrect API Functionality:** The agent sometimes suggests a relevant-sounding API but provides a link to a tool that does not actually perform the required function, indicating a knowledge gap.
*   **Providing Useless Links:** The agent provides links that are non-functional or useless, leaving the core goal of the task unaddressed.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:04:37
DESCRIPTION: agent_comment_gen_dataset - context_relevancy Analysis
==================================================
Analysis for scorer: context_relevancy
Scorer Name: context_relevancy
Reasoning:
Based on the scorer's feedback, the agent is failing or underperforming due to the following key reasons:

*   **Providing Irrelevant Links and APIs:** The most significant and recurring issue is the agent suggesting APIs and providing links that are completely irrelevant to the user's task. For example, suggesting a Ghibli-style image generator for a real estate data export task or a random link for a discussion about coffee shops.
*   **Complete Misunderstanding of the Task:** In severe cases, the agent demonstrates a clear misunderstanding of the user's core request, causing it to offer solutions that fail to address the fundamental problem.
*   **Making Unjustified Assumptions:** The agent sometimes assumes the specifics of a user's problem without sufficient context. For instance, it assumed an "unknown API error" was related to upscaling GIFs without any evidence.
*   **Lack of Contextual Tailoring:** Responses are often too generic and not specifically adapted to the context, such as failing to directly reference the content of a guide being discussed or tailoring the suggestion to a specific community like a subreddit.
*   **Insufficient Detail:** Even when the API suggestion is relevant, the agent sometimes fails to provide a detailed explanation of the API, which would improve the helpfulness of the response.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:04:57
DESCRIPTION: agent_comment_gen_dataset - role_adherence Analysis
==================================================
Analysis for scorer: role_adherence
Scorer Name: role_adherence
Reasoning:
*   **Irrelevant and Off-Topic Responses:** The agent frequently provides responses that are completely unrelated to the user's prompt and the given context. This often manifests as suggesting random, irrelevant API links instead of performing the requested task.
*   **Misunderstanding of Assigned Role:** The agent consistently fails to adhere to its core role of "comment generation." It deviates significantly, indicating a fundamental misunderstanding or misinterpretation of its primary function.
*   **Ignoring Context:** The agent often ignores crucial contextual information, such as the specific topic of a post or the subreddit it's supposed to be interacting with. This results in generic or misplaced responses.
*   **Inappropriate Tone and Content:** The agent's tone is sometimes too conversational for its assigned role. In some instances, it provides personal anecdotes, which contradicts the expected behavior.
*   **Failure to Use Tools:** The agent was noted for not calling required tools to complete the assigned task.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:05:08
DESCRIPTION: agent_comment_gen_dataset - tool_relevancy Analysis
==================================================
Analysis for scorer: tool_relevancy
Scorer Name: tool_relevancy
Reasoning: All provided low-scoring samples indicate a "Missing required fields" error. As per the instructions, this is a developer-side code issue and is excluded from the analysis of the agent's performance. Therefore, there is no actionable reasoning regarding the agent's failures from this data set.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:05:19
DESCRIPTION: agent_comment_gen_dataset - parameter_correctness Analysis
==================================================
Analysis for scorer: parameter_correctness
Scorer Name: parameter_correctness
Reasoning: 
All the provided low-scoring samples for this scorer cited "Error: Missing required fields" as the reason. As per the instructions, this is considered a developer-side code issue and has been excluded from this analysis of the agent's performance. There are no other reasons for low scores provided in the data.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:05:35
DESCRIPTION: agent_comment_gen_dataset - Dataset Summary
==================================================
Agent Name: agent_comment_gen_dataset
Reasoning:
Based on the analyses, the agent's primary failures stem from a fundamental misunderstanding of its core task and role. This leads to several recurring issues:

*   **Irrelevant and Off-Topic Responses:** The most significant problem is the agent consistently providing responses, particularly API and link suggestions, that are completely irrelevant to the user's prompt and the given context.
*   **Ignoring Context:** The agent fails to use crucial contextual information, such as the specific topic of a post or the community (e.g., a subreddit) it's meant to be interacting with. This results in generic, misplaced comments that are not tailored to the situation.
*   **Misinterpretation of Role:** The agent deviates from its primary function of "comment generation," often acting as a tool recommender instead. It sometimes adopts an inappropriate conversational or personal tone, which contradicts its expected behavior.
*   **Unjustified Assumptions:** The agent makes assumptions about the user's problem without sufficient evidence or context.
*   **Failure to Use Tools:** The agent was noted for not calling the required tools to complete its assigned task.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:05:49
DESCRIPTION: post_validation_dataset - task_progression Analysis
==================================================
Analysis for scorer: task_progression
Scorer Name: Task Progression
Reasoning:
Based on the scorer's feedback for lower-scoring runs, the agent is facing issues in the following areas:

*   **Baseline Performance on Simple Tasks:** The agent successfully completes simple tasks, but this is considered a baseline level of achievement and not indicative of robust performance.
*   **Ineffective Tool Execution:** In some instances, while the agent correctly executes the tool, the resulting output shows '0' for most metrics, suggesting the action performed was not meaningful or effective.
*   **Lack of Data Interpretation:** The agent provides data from the tool but fails to offer any nuanced interpretation of that data.
*   **Formatting and Detail Imperfections:** The responses contain minor imperfections in formatting and details.
*   **Missing Context:** The agent's response sometimes lacks sufficient context regarding the tool's function.
*   **Empty Output:** On occasion, the agent's output is empty. While noted as potentially correct behavior for the tool, it is still flagged as an area for improvement.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:06:03
DESCRIPTION: post_validation_dataset - context_relevancy Analysis
==================================================
Analysis for scorer: context_relevancy
Scorer Name: context_relevancy
Reasoning:
Based on the scorer's feedback, the agent's responses, while relevant and functional, are scored lower for the following key reasons:
*   **Lack of Context and Explanation:** The tool's output is often presented without any context. It fails to explain what was being validated, why certain actions were taken, or why results appear as they do (e.g., why zero items were processed).
*   **Missing Summaries and Interpretation:** The agent does not provide a summary of the actions taken, an interpretation of the results, or a clear statement confirming task completion.
*   **Insufficient Detail on Tool Actions:** The response lacks insight into the specific actions the tool executed to generate the output.
*   **Need for Elaboration:** The output is considered too raw and would benefit from more elaboration to be fully helpful.
*   **Clarity of Information:** In some instances, the output could be improved with clearer labeling of the data provided.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:06:14
DESCRIPTION: post_validation_dataset - role_adherence Analysis
==================================================
Analysis for scorer: role_adherence
Based on the provided data, all 25 samples for the scorer `role_adherence` have high scores (ranging from 9.0 to 9.5). The reasoning across all samples is consistently positive, indicating the agent is performing its role perfectly. Therefore, there are no low scores or failure points to analyze from this data set.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:06:24
DESCRIPTION: post_validation_dataset - tool_relevancy Analysis
==================================================
Analysis for scorer: tool_relevancy
Scorer Name: tool_relevancy
Reasoning: 
All the provided low-scoring samples cite "Error: Missing required fields" as the reason. As per the instructions, this is a developer-side code issue and has been excluded from the analysis of the agent's performance. Therefore, no specific reasons for agent failure can be extracted from the provided data.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:06:32
DESCRIPTION: post_validation_dataset - parameter_correctness Analysis
==================================================
Analysis for scorer: parameter_correctness
Scorer Name: parameter_correctness
Reasoning: 
All the provided samples for this scorer have a reasoning of "Error: Missing required fields". As per the instructions, this is a developer-side issue and should be excluded. There are no other reasons for low scores provided in the data.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:06:44
DESCRIPTION: post_validation_dataset - Dataset Summary
==================================================
Agent Name: post_validation_dataset
Reasoning:
Based on the analysis from different scorers, the agent's primary issue lies in its failure to provide context or interpretation for the tool's output. The key problems identified are:

*   **Lack of Context and Interpretation:** The agent presents raw data from the tool without explaining what was validated, the specific actions taken, or what the results signify. It fails to provide summaries or interpretations, leaving the output unhelpful.
*   **Ineffective or Unexplained Output:** In some instances, the tool's output is empty or shows '0' for most metrics, indicating an ineffective action. The agent does not explain why these results occurred.
*   **Poor Presentation:** The responses contain minor imperfections in formatting, details, and data labeling, which reduces the overall clarity of the information provided.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:07:00
DESCRIPTION: agent_query_gen_dataset - task_progression Analysis
==================================================
Analysis for scorer: task_progression
Scorer Name: Task Progression
Reasoning:
Based on the provided data for low-scoring runs, the agent is failing to progress for the following key reasons:

*   **Complete Misunderstanding of Task and Role:** The most severe failures occur when the agent completely misunderstands its assigned role or the task itself. It frequently provides responses that are entirely off-topic, acts as a user asking unrelated questions, or generates content irrelevant to the provided API (e.g., discussing content moderation or foot traffic when the task is about a faceswap API).
*   **Failure to Use Provided API Information:** In several instances, the agent does not attempt to interact with or use the provided API URL or description. Even if the response is thematically related to the task, it consists of generic examples or questions rather than concrete steps to utilize the given tool.
*   **Inefficient or Halted Progress:** The agent sometimes understands the task's objective but gets stuck. It identifies a need for more information, such as an API description, even when it has access to resources (like the API's URL/webpage) from which it could infer the necessary details. This stalls progress without taking concrete action.
*   **Encountering API/Server Errors:** The agent correctly identifies and attempts to interact with the API but is blocked by external errors (e.g., "503 UNAVAILABLE", "server error"). While this demonstrates an understanding of the task, the error prevents successful completion and further progress.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:07:15
DESCRIPTION: agent_query_gen_dataset - context_relevancy Analysis
==================================================
Analysis for scorer: context_relevancy
Scorer Name: context_relevancy
Reasoning:
Based on the low-scoring samples, the agent is failing for the following key reasons:
*   **Irrelevance:** The agent's responses are completely irrelevant to the provided API task and context.
*   **Failure to Engage:** Instead of interacting with or analyzing the provided API, the agent generates a list of generic or unrelated questions.
*   **Lack of Understanding:** The agent demonstrates no understanding of the task, the given API description, or its intended function, failing to address the provided context in any meaningful way.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:07:36
DESCRIPTION: agent_query_gen_dataset - role_adherence Analysis
==================================================
Analysis for scorer: role_adherence
Scorer Name: role_adherence
Reasoning:
Based on the low-scoring samples, the agent fails to adhere to its role for the following key reasons:

*   **Ignoring the Core Task:** The agent completely disregards its primary instruction to interact with the provided API. It shows no intent to perform its assigned function.
*   **Generating Unrelated Content:** Instead of using the API, the agent generates lists of questions or makes statements on topics completely unrelated to the given API. These topics include content moderation, data analysis, recipe planning, and PII redaction.
*   **Failure to Use Tools:** A consistent point of failure is the agent not making any tool calls, which is the expected behavior for an agent tasked with API interaction.
*   **Acting as a Generic Chatbot:** The agent frequently misunderstands its role, behaving like a general-purpose chatbot by asking for information it should be analyzing or answering unrelated user questions.
*   **Disregarding Provided Context:** The agent often fails to acknowledge or utilize the specific API information (URL, description) provided in the prompt, making its responses irrelevant.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:07:47
DESCRIPTION: agent_query_gen_dataset - tool_relevancy Analysis
==================================================
Analysis for scorer: tool_relevancy
Scorer Name: tool_relevancy
Reasoning: 
Based on the provided data, there are no specific agent-related failures to report for the low scores. All instances of a 0.0 score are attributed to a "Missing required fields" error, which has been excluded from this analysis as per the instructions.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:07:59
DESCRIPTION: agent_query_gen_dataset - parameter_correctness Analysis
==================================================
Analysis for scorer: parameter_correctness
Scorer Name: parameter_correctness
Reasoning: 
Based on the provided data, all samples received a low score. However, the only reasoning provided is "Error: Missing required fields". As per the instructions, this is a developer-side code issue and has been excluded. There are no other reasons for agent failure available in this dataset.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:08:19
DESCRIPTION: agent_query_gen_dataset - Dataset Summary
==================================================
Agent Name: query_generation
Reasoning: 
Based on the analysis from multiple scorers, the agent's primary failures stem from a fundamental misunderstanding of its role and the assigned task. The key issues are:

*   **Misunderstanding of Role and Task:** The agent frequently fails to recognize its purpose. Instead of interacting with the provided API, it behaves like a generic chatbot, asking unrelated questions or providing off-topic responses.
*   **Failure to Use Provided API and Context:** A consistent point of failure is the agent's inability to engage with the given API information (URL, description). It makes no attempt to use the provided tool or make the expected tool calls, ignoring its core function.
*   **Generation of Irrelevant Content:** The agent often generates content entirely unrelated to the API's function, such as lists of generic questions or discussions on topics like content moderation, data analysis, or recipes.
*   **Stalled Progress:** In some cases, even when the agent understands the objective, it gets stuck. It identifies a need for more information but fails to use the provided resources (like the API's URL) to obtain it, halting any progress.
*   **External API/Server Errors:** Some failures are due to external factors. The agent correctly attempts to interact with the API but is blocked by server-side issues (e.g., "503 UNAVAILABLE" errors), preventing successful task completion.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:08:36
DESCRIPTION: tavily_search_results_dataset - task_progression Analysis
==================================================
Analysis for scorer: task_progression
Scorer Name: Task Progression
Reasoning:
Based on the scorer's feedback, the agent's primary failure is its inability to successfully execute tool calls, even when it correctly identifies that a tool is needed.

Key reasons for low scores are:
*   **Failed Tool Execution:** The most consistent issue is the agent's failure to execute the tool call. The agent attempts the correct action but the execution fails.
*   **Specific API/HTTP Errors:** The failures are frequently accompanied by specific error messages, including `HTTPError`, `Client Error`, and `432 Client Error`.
*   **Communication Problems:** The errors often indicate a communication issue between the agent and the external tool or API (e.g., the Tavily search API).
*   **Lack of Concrete Progress:** Although the agent shows intent by initiating a tool call, the execution error means no meaningful or concrete progress is made toward the task's completion.
*   **Unhelpful Responses:** The final output from the agent is an error message, which is unhelpful to the user and does not provide the requested information.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:08:50
DESCRIPTION: tavily_search_results_dataset - context_relevancy Analysis
==================================================
Analysis for scorer: context_relevancy
Scorer Name: context_relevancy
Reasoning:
*   The agent's response is a raw, technical `HTTPError` message.
*   The responses lack sufficient context regarding the root cause of the error or what specifically failed during the tool's execution.
*   The error messages are not user-friendly and fail to provide explanations, potential causes, or suggestions for resolution.
*   While the error indicates a tool execution failure, it does not provide more specific or detailed information about the nature of the problem.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:09:15
DESCRIPTION: tavily_search_results_dataset - role_adherence Analysis
==================================================
Analysis for scorer: role_adherence
Scorer Name: role_adherence
Reasoning:
Based on the scorer's feedback, the agent consistently fails to adhere to its role due to the following key reasons:

*   **Failure to Execute Tool Calls:** The most significant issue is the agent's complete failure to execute its primary function, which is to make a tool call. The agent either does not initiate the call or the call is unsuccessful.
*   **Returning Technical Errors:** Instead of providing a valid output from the tool, the agent frequently returns raw, unhelpful technical error messages, such as "HTTPError" and "432 Client Error".
*   **Empty Tool Call Data:** In many instances of failure, the scorer explicitly notes that the `tool_calls` list is empty, providing clear evidence that the agent did not even attempt to perform its assigned task.
*   **Contradiction of Assigned Role:** The agent's behavior of returning errors instead of executing its function directly contradicts its fundamental role as a "tool". It is not executing as programmed and fails to perform its simple, assigned task.
*   **Providing Unusable Responses:** The error messages returned are not useful, provide no information, and do not help in progressing the task, rendering the agent's response useless.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:09:26
DESCRIPTION: tavily_search_results_dataset - tool_relevancy Analysis
==================================================
Analysis for scorer: tool_relevancy
Scorer Name: tool_relevancy
Reasoning: 
All the provided low-score samples indicate a "Missing required fields" error. As per the instructions, these are code-related issues on the developer's side and have been excluded from this analysis. There are no other reasons for low scores in the provided data.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:09:36
DESCRIPTION: tavily_search_results_dataset - parameter_correctness Analysis
==================================================
Analysis for scorer: parameter_correctness
Scorer Name: parameter_correctness
Reasoning: All the provided low-scoring samples were due to a "Missing required fields" error. As per the instructions, this developer-side issue has been excluded from the analysis. There are no other reasons for agent failure in the provided data.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:09:50
DESCRIPTION: tavily_search_results_dataset - Dataset Summary
==================================================
Agent Name: tavily_search_results_dataset
Reasoning:
The agent's primary failure is its inability to successfully execute tool calls. In some instances, the agent correctly identifies that a tool is needed, but the execution fails. In other cases, it fails to even initiate the call, resulting in an empty `tool_calls` list.

Instead of providing a useful response, the agent returns raw, technical error messages to the user. These errors, including `HTTPError` and `432 Client Error`, suggest a communication problem between the agent and the external tool's API. The final output is not user-friendly, lacks context about the cause of the failure, and is ultimately unhelpful, preventing any progress on the given task.
==================================================


==================================================
TIMESTAMP: 2025-10-02 15:10:25
DESCRIPTION: Final Comprehensive Analysis
==================================================
Based on the provided documentation and part-wise analysis, the agent is experiencing a cascading failure that originates at the very beginning of its core workflow: search query generation and execution. The issues identified in the later stages (post-validation, comment generation, email reporting) are all symptoms of these initial, critical failures.

The root cause is twofold: a fundamental logic failure in the query generation agent and a technical failure in the search tool execution.

1.  **Logic Failure (Query Generation):** The `search_agent` is not fulfilling its primary function. Instead of generating search queries based on the provided API context, it acts like a generic chatbot, misunderstanding its role and ignoring its inputs.
2.  **Technical Failure (Search Execution):** The `tavily_search` step is failing with technical errors (`HTTPError`, `432 Client Error`), indicating a broken integration. The agent cannot communicate with the search API.

Because no relevant (or any) search results are being retrieved, all subsequent steps are starved of the necessary input, leading to the observed failures down the line: post validation has nothing to validate, comment generation has no context to write about, and the final email report has no content to summarize.

Suggested Fixes:
 - **fix_1: Resolve the Search Tool's Technical Failures.** The `tavily_search_results_dataset` analysis points to `HTTPError` and `432 Client Error`. This is the most critical blocker. You must investigate the integration with the Tavily search API.
    - Check API keys, authentication headers, and endpoint URLs for correctness.
    - Implement robust error handling to catch these specific exceptions and log more informative messages, rather than passing raw error traces to the output.
    - Verify that the agent's environment has proper network access to the Tavily API endpoints.

 - **fix_2: Re-engineer the Query Generation Agent's Prompt.** The `agent_query_gen_dataset` analysis shows a complete misunderstanding of the task. The agent's prompt needs to be significantly improved to constrain its behavior.
    - Use stronger, more direct instructions. For example: "You are a search query generator. Your ONLY task is to generate 5 search queries based on the following API title and description. DO NOT ask questions. DO NOT chat. Only output the queries."
    - Provide few-shot examples within the prompt to show the agent the exact input-output format you expect.
    - Add a system-level instruction that defines its role strictly, preventing it from defaulting to a conversational chatbot persona.

 - **fix_3: Implement a "Fail-Fast" Mechanism for the Workflow.** The agent continues its run even after the critical search step fails, causing downstream agents to produce nonsensical or empty outputs. The workflow should be made more robust.
    - After the "Reddit Search Phase," check if a valid list of URLs was returned.
    - If the search fails or returns zero results, the agent should immediately terminate the current attempt for that specific API, log the failure reason (e.g., "No Reddit posts found"), and move on to the next attempt or API. This will prevent the `post_validation`, `comment_generation`, and `email_gen_send` agents from running without the necessary data, which is the direct cause of their poor performance.
==================================================

