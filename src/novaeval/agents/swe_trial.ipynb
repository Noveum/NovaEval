{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# SWE Agent Trajectories Testing\n",
    "\n",
    "This notebook tests the methods from `swe_agent_trajectories.py` with the specified parquet file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de0a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Add the src directory to the path to import novaeval modules\n",
    "sys.path.append('../../../')\n",
    "\n",
    "from novaeval.agents.swe_agent_trajectories import swe_agent_trajectories_preprocessing, create_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c1b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parquet file path\n",
    "parquet_file_path = \"/mnt/drive2/train-00000-of-00012.parquet\"\n",
    "\n",
    "# Check if the file exists\n",
    "print(f\"Checking if file exists: {parquet_file_path}\")\n",
    "print(f\"File exists: {os.path.exists(parquet_file_path)}\")\n",
    "\n",
    "# If file doesn't exist, let's check what's in the directory\n",
    "if not os.path.exists(parquet_file_path):\n",
    "    print(\"\\nFile not found. Checking directory contents:\")\n",
    "    try:\n",
    "        dir_path = \"/mnt/drive2/\"\n",
    "        if os.path.exists(dir_path):\n",
    "            files = os.listdir(dir_path)\n",
    "            parquet_files = [f for f in files if f.endswith('.parquet')]\n",
    "            print(f\"Parquet files in {dir_path}: {parquet_files[:10]}\")  # Show first 10\n",
    "        else:\n",
    "            print(f\"Directory {dir_path} does not exist\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking directory: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Try to read the parquet file directly with pandas first\n",
    "try:\n",
    "    print(\"Attempting to read parquet file directly with pandas...\")\n",
    "    df = pd.read_parquet(parquet_file_path)\n",
    "    print(f\"Successfully read parquet file!\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    required_cols = ['instance_id', 'model_name', 'target', 'trajectory', 'exit_status', 'generated_patch', 'eval_logs']\n",
    "    missing = [col for col in required_cols if col not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"\\nMissing required columns: {missing}\")\n",
    "    else:\n",
    "        print(f\"\\nAll required columns present!\")\n",
    "        \n",
    "    # Check trajectory column structure\n",
    "    if 'trajectory' in df.columns:\n",
    "        print(f\"\\nTrajectory column sample:\")\n",
    "        print(df['trajectory'].iloc[0])\n",
    "        print(f\"Type: {type(df['trajectory'].iloc[0])}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error reading parquet file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0037e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Test the preprocessing function\n",
    "try:\n",
    "    print(\"Testing swe_agent_trajectories_preprocessing function...\")\n",
    "    output_csv = \"/mnt/drive2/test_output.csv\"\n",
    "    \n",
    "    # Call the preprocessing function\n",
    "    swe_agent_trajectories_preprocessing(\n",
    "        parquet_files=[parquet_file_path],\n",
    "        output_csv=output_csv\n",
    "    )\n",
    "    \n",
    "    print(f\"Preprocessing completed! Output saved to: {output_csv}\")\n",
    "    \n",
    "    # Read and display the output\n",
    "    output_df = pd.read_csv(output_csv)\n",
    "    print(f\"\\nOutput shape: {output_df.shape}\")\n",
    "    print(f\"Output columns: {list(output_df.columns)}\")\n",
    "    print(f\"\\nFirst few rows of output:\")\n",
    "    print(output_df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in preprocessing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb52c21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a133ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Test the create_dataset function\n",
    "try:\n",
    "    print(\"Testing create_dataset function...\")\n",
    "    \n",
    "    # Create dataset from the preprocessed CSV\n",
    "    dataset = create_dataset(output_csv)\n",
    "    \n",
    "    print(f\"Dataset created successfully!\")\n",
    "    print(f\"Dataset type: {type(dataset)}\")\n",
    "    \n",
    "    # Try to access some dataset properties/methods\n",
    "    if hasattr(dataset, '__len__'):\n",
    "        print(f\"Dataset length: {len(dataset)}\")\n",
    "    \n",
    "    if hasattr(dataset, 'data'):\n",
    "        print(f\"Dataset data keys: {list(dataset.data.keys()) if isinstance(dataset.data, dict) else 'Not a dict'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in create_dataset: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f1ed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World\")\n",
    "print(type(dataset))\n",
    "print(dir(dataset))\n",
    "gen = dataset.get_datapoint()\n",
    "obj = next(gen)\n",
    "print(next(gen))    \n",
    "print(obj.model_dump())\n",
    "print(type(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0880732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Add the src directory to the path to import novaeval modules\n",
    "sys.path.append('../../../')\n",
    "\n",
    "from novaeval.agents.swe_agent_trajectories import swe_agent_trajectories_preprocessing, create_dataset\n",
    "total_processed = 0\n",
    "for chunk in dataset.stream_from_csv(\n",
    "    file_path=output_csv,\n",
    "    chunk_size=500,  # Process 500 rows at a time\n",
    "    turn_id='instance_id',\n",
    "    agent_name='model_name',\n",
    "    agent_task='target',\n",
    "    tool_call_results='generated_patch',\n",
    "    metadata='eval_logs'\n",
    "):\n",
    "    # chunk is list[AgentData] with max 500 items\n",
    "    chunk_size = len(chunk)\n",
    "    total_processed += chunk_size\n",
    "    \n",
    "    # Example processing: Print first item in chunk\n",
    "    if total_processed <= 500:  # Only print from first chunk\n",
    "        print(f\"\\nSample data from first chunk:\")\n",
    "        print(f\"Agent Name: {chunk[0].agent_name}\")\n",
    "        print(f\"Task: {chunk[0].agent_task}\")\n",
    "        print(f\"Turn ID: {chunk[0].turn_id}\")\n",
    "    \n",
    "    print(f\"Processed chunk of {chunk_size} items. Total processed: {total_processed}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0934ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Test with directory approach (if we have multiple files)\n",
    "try:\n",
    "    print(\"Testing with directory approach...\")\n",
    "    \n",
    "    # Get the directory of the parquet file\n",
    "    parquet_dir = os.path.dirname(parquet_file_path)\n",
    "    print(f\"Parquet directory: {parquet_dir}\")\n",
    "    \n",
    "    # Check if directory exists and contains parquet files\n",
    "    if os.path.exists(parquet_dir):\n",
    "        files = os.listdir(parquet_dir)\n",
    "        parquet_files = [f for f in files if f.endswith('.parquet')]\n",
    "        print(f\"Found {len(parquet_files)} parquet files in directory\")\n",
    "        \n",
    "        if len(parquet_files) > 1:\n",
    "            print(\"Testing with multiple files using directory approach...\")\n",
    "            output_csv_multi = \"test_output_multi.csv\"\n",
    "            \n",
    "            swe_agent_trajectories_preprocessing(\n",
    "                parquet_dir=parquet_dir,\n",
    "                output_csv=output_csv_multi\n",
    "            )\n",
    "            \n",
    "            print(f\"Multi-file preprocessing completed! Output saved to: {output_csv_multi}\")\n",
    "            \n",
    "            # Read and display the output\n",
    "            output_df_multi = pd.read_csv(output_csv_multi)\n",
    "            print(f\"\\nMulti-file output shape: {output_df_multi.shape}\")\n",
    "            print(f\"Multi-file output columns: {list(output_df_multi.columns)}\")\n",
    "        else:\n",
    "            print(\"Only one parquet file found, skipping multi-file test\")\n",
    "    else:\n",
    "        print(f\"Directory {parquet_dir} does not exist\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in directory approach test: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954e9354",
   "metadata": {},
   "outputs": [],
   "source": [
    ".."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Remove temporary files\n",
    "import os\n",
    "\n",
    "files_to_cleanup = [\"test_output.csv\", \"test_output_multi.csv\"]\n",
    "for file in files_to_cleanup:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        print(f\"Cleaned up: {file}\")\n",
    "\n",
    "print(\"\\nTesting completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b23dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic usage comparison of create_dataset and stream_dataset\n",
    "\n",
    "print(\"1. Using create_dataset (loads everything at once):\")\n",
    "try:\n",
    "    # Create full dataset\n",
    "    dataset = create_dataset(output_csv)\n",
    "    print(f\"Total records loaded: {len(dataset.data)}\")\n",
    "    \n",
    "    if dataset.data:\n",
    "        first_record = dataset.data[0]\n",
    "        print(\"\\nSample from create_dataset:\")\n",
    "        print(f\"Agent Name: {first_record.agent_name}\")\n",
    "        print(f\"Task: {first_record.agent_task}\")\n",
    "        print(f\"Tool Call Results type: {type(first_record.tool_call_results)}\")\n",
    "        print(f\"Tool Call Results: {first_record.tool_call_results}\")\n",
    "        print(f\"Metadata: {first_record.metadata}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in create_dataset: {e}\")\n",
    "\n",
    "print(\"\\n2. Using stream_dataset (processes in chunks):\")\n",
    "try:\n",
    "    total_processed = 0\n",
    "    first_chunk_sample = None\n",
    "    \n",
    "    # Process in chunks of 500\n",
    "    for chunk_num, chunk in enumerate(stream_dataset(output_csv, chunk_size=500), 1):\n",
    "        chunk_size = len(chunk)\n",
    "        total_processed += chunk_size\n",
    "        \n",
    "        # Save sample from first chunk\n",
    "        if first_chunk_sample is None and chunk:\n",
    "            first_chunk_sample = chunk[0]\n",
    "            print(\"\\nSample from first chunk:\")\n",
    "            print(f\"Agent Name: {first_chunk_sample.agent_name}\")\n",
    "            print(f\"Task: {first_chunk_sample.agent_task}\")\n",
    "            print(f\"Tool Call Results type: {type(first_chunk_sample.tool_call_results)}\")\n",
    "            print(f\"Tool Call Results: {first_chunk_sample.tool_call_results}\")\n",
    "            print(f\"Metadata: {first_chunk_sample.metadata}\")\n",
    "        \n",
    "        print(f\"\\nProcessed chunk {chunk_num}: {chunk_size} records. Running total: {total_processed}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in stream_dataset: {e}\")\n",
    "\n",
    "print(\"\\nProcessing completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0625032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Detailed data structure comparison\n",
    "\n",
    "def compare_agent_data(data1, data2, prefix=\"\"):\n",
    "    \"\"\"Helper function to compare two AgentData objects\"\"\"\n",
    "    print(f\"{prefix}Comparing AgentData objects:\")\n",
    "    \n",
    "    # Compare basic fields\n",
    "    print(f\"{prefix}Basic fields:\")\n",
    "    print(f\"{prefix}  Agent Name match: {data1.agent_name == data2.agent_name}\")\n",
    "    print(f\"{prefix}  Task match: {data1.agent_task == data2.agent_task}\")\n",
    "    print(f\"{prefix}  Role match: {data1.agent_role == data2.agent_role}\")\n",
    "    \n",
    "    # Compare tool_call_results\n",
    "    print(f\"\\n{prefix}Tool Call Results:\")\n",
    "    print(f\"{prefix}  Type match: {type(data1.tool_call_results) == type(data2.tool_call_results)}\")\n",
    "    if data1.tool_call_results and data2.tool_call_results:\n",
    "        print(f\"{prefix}  Structure match: {data1.tool_call_results == data2.tool_call_results}\")\n",
    "        # Show structure of first tool call result\n",
    "        print(f\"{prefix}  First tool call structure:\")\n",
    "        print(f\"{prefix}    call_id present: {'call_id' in data1.tool_call_results[0]}\")\n",
    "        print(f\"{prefix}    result present: {'result' in data1.tool_call_results[0]}\")\n",
    "        print(f\"{prefix}    success present: {'success' in data1.tool_call_results[0]}\")\n",
    "    \n",
    "    # Compare metadata\n",
    "    print(f\"\\n{prefix}Metadata:\")\n",
    "    print(f\"{prefix}  Type match: {type(data1.metadata) == type(data2.metadata)}\")\n",
    "    if data1.metadata and data2.metadata:\n",
    "        print(f\"{prefix}  Content match: {data1.metadata == data2.metadata}\")\n",
    "        # Show metadata structure\n",
    "        print(f\"{prefix}  Structure contains:\")\n",
    "        print(f\"{prefix}    exit_status present: {'exit_status' in data1.metadata}\")\n",
    "        print(f\"{prefix}    mask present: {'mask' in data1.metadata}\")\n",
    "\n",
    "print(\"Detailed comparison of create_dataset and stream_dataset outputs:\")\n",
    "\n",
    "# Get data from both methods\n",
    "dataset = create_dataset(output_csv)\n",
    "create_record = dataset.data[0] if dataset.data else None\n",
    "\n",
    "stream_record = None\n",
    "for chunk in stream_dataset(output_csv, chunk_size=500):\n",
    "    if chunk:\n",
    "        stream_record = chunk[0]\n",
    "        break\n",
    "\n",
    "if create_record and stream_record:\n",
    "    print(\"\\nComparing first record from both methods:\")\n",
    "    compare_agent_data(create_record, stream_record, \"  \")\n",
    "else:\n",
    "    print(\"No records available for comparison\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87307856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Processing chunks with a specific task\n",
    "\n",
    "print(\"Example of processing chunks with a specific task:\")\n",
    "\n",
    "def process_chunk(chunk: list[AgentData], task_name: str) -> int:\n",
    "    \"\"\"\n",
    "    Example function to process a chunk of data.\n",
    "    Returns count of records matching the task.\n",
    "    \"\"\"\n",
    "    matching_count = sum(1 for record in chunk if record.agent_task == task_name)\n",
    "    return matching_count\n",
    "\n",
    "try:\n",
    "    # Process chunks and count records with specific task\n",
    "    target_task = \"fix bug\"  # example task name\n",
    "    total_matching = 0\n",
    "    total_processed = 0\n",
    "    \n",
    "    print(f\"\\nCounting records with task: '{target_task}'\")\n",
    "    \n",
    "    for chunk_num, chunk in enumerate(stream_dataset(output_csv, chunk_size=500), 1):\n",
    "        # Process this chunk\n",
    "        matching_in_chunk = process_chunk(chunk, target_task)\n",
    "        \n",
    "        # Update totals\n",
    "        total_matching += matching_in_chunk\n",
    "        total_processed += len(chunk)\n",
    "        \n",
    "        # Progress report\n",
    "        print(f\"Chunk {chunk_num}: Found {matching_in_chunk} matching records out of {len(chunk)}\")\n",
    "        \n",
    "    # Final summary\n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"Total records processed: {total_processed}\")\n",
    "    print(f\"Total records matching '{target_task}': {total_matching}\")\n",
    "    if total_processed > 0:\n",
    "        print(f\"Percentage matching: {(total_matching/total_processed)*100:.2f}%\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error processing chunks: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f2ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using stream_dataset vs create_dataset\n",
    "\n",
    "print(\"Comparing stream_dataset with create_dataset:\")\n",
    "\n",
    "# 1. Using create_dataset (loads everything at once)\n",
    "print(\"\\n1. Using create_dataset:\")\n",
    "try:\n",
    "    dataset = create_dataset(output_csv)\n",
    "    print(f\"Total records loaded: {len(dataset.data)}\")\n",
    "    print(\"\\nFirst record sample:\")\n",
    "    first_record = dataset.data[0]\n",
    "    print(f\"Agent Name: {first_record.agent_name}\")\n",
    "    print(f\"Task: {first_record.agent_task}\")\n",
    "    print(f\"Tool Call Results: {first_record.tool_call_results[:200]}...\")  # Show first 200 chars\n",
    "except Exception as e:\n",
    "    print(f\"Error in create_dataset: {e}\")\n",
    "\n",
    "# 2. Using stream_dataset (processes in chunks)\n",
    "print(\"\\n2. Using stream_dataset:\")\n",
    "try:\n",
    "    total_processed = 0\n",
    "    records_sample = None\n",
    "    chunk_count = 0\n",
    "    \n",
    "    # Process chunks of 500 records\n",
    "    for chunk in stream_dataset(output_csv, chunk_size=500):\n",
    "        chunk_count += 1\n",
    "        chunk_size = len(chunk)\n",
    "        total_processed += chunk_size\n",
    "        \n",
    "        # Save first chunk's first record for comparison\n",
    "        if records_sample is None and chunk:\n",
    "            records_sample = chunk[0]\n",
    "            \n",
    "        print(f\"Processed chunk {chunk_count}: {chunk_size} records. Total processed: {total_processed}\")\n",
    "    \n",
    "    # Show sample from streaming for comparison\n",
    "    if records_sample:\n",
    "        print(\"\\nFirst record sample from streaming:\")\n",
    "        print(f\"Agent Name: {records_sample.agent_name}\")\n",
    "        print(f\"Task: {records_sample.agent_task}\")\n",
    "        print(f\"Tool Call Results: {records_sample.tool_call_results[:200]}...\")  # Show first 200 chars\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in stream_dataset: {e}\")\n",
    "\n",
    "print(\"\\nComparison completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcbf66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed comparison of create_dataset and stream_dataset outputs\n",
    "\n",
    "def compare_agent_data(data1, data2, prefix=\"\"):\n",
    "    \"\"\"Helper function to compare two AgentData objects\"\"\"\n",
    "    print(f\"{prefix}Comparing AgentData objects:\")\n",
    "    print(f\"{prefix}  Agent Name: {data1.agent_name == data2.agent_name}\")\n",
    "    print(f\"{prefix}  Task: {data1.agent_task == data2.agent_task}\")\n",
    "    print(f\"{prefix}  Tool Call Results structure matches: {type(data1.tool_call_results) == type(data2.tool_call_results)}\")\n",
    "    if data1.tool_call_results and data2.tool_call_results:\n",
    "        print(f\"{prefix}  Tool Call Results content matches: {data1.tool_call_results == data2.tool_call_results}\")\n",
    "    print(f\"{prefix}  Metadata structure matches: {type(data1.metadata) == type(data2.metadata)}\")\n",
    "    if data1.metadata and data2.metadata:\n",
    "        print(f\"{prefix}  Metadata content matches: {data1.metadata == data2.metadata}\")\n",
    "\n",
    "print(\"Comparing outputs of create_dataset and stream_dataset:\")\n",
    "\n",
    "# Get first record from create_dataset\n",
    "dataset = create_dataset(output_csv)\n",
    "create_record = dataset.data[0] if dataset.data else None\n",
    "\n",
    "# Get first record from stream_dataset\n",
    "stream_record = None\n",
    "for chunk in stream_dataset(output_csv, chunk_size=500):\n",
    "    if chunk:\n",
    "        stream_record = chunk[0]\n",
    "        break\n",
    "\n",
    "if create_record and stream_record:\n",
    "    print(\"\\nDetailed comparison of first record:\")\n",
    "    compare_agent_data(create_record, stream_record, \"  \")\n",
    "    \n",
    "    print(\"\\nSample values from create_dataset:\")\n",
    "    print(f\"  Tool Call Results: {create_record.tool_call_results}\")\n",
    "    print(f\"  Metadata: {create_record.metadata}\")\n",
    "    \n",
    "    print(\"\\nSample values from stream_dataset:\")\n",
    "    print(f\"  Tool Call Results: {stream_record.tool_call_results}\")\n",
    "    print(f\"  Metadata: {stream_record.metadata}\")\n",
    "else:\n",
    "    print(\"No records available for comparison\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of streaming methods\n",
    "\n",
    "# 1. Process huge CSV in chunks\n",
    "# 2. Save chunk to JSON and test JSON streaming\n",
    "print(\"\\nExample 2: Testing JSON streaming\")\n",
    "# First save a chunk to JSON for testing\n",
    "if total_processed > 0:\n",
    "    # Save first 100 items to JSON\n",
    "    with open('test_chunk.json', 'w') as f:\n",
    "        json.dump(\n",
    "            [agent.model_dump() for agent in chunk[:100]], \n",
    "            f, \n",
    "            indent=2\n",
    "        )\n",
    "    \n",
    "    # Now read it back using stream_from_json\n",
    "    print(\"\\nReading back from JSON in chunks:\")\n",
    "    json_total = 0\n",
    "    for json_chunk in dataset.stream_from_json(\n",
    "        file_path='test_chunk.json',\n",
    "        chunk_size=10,  # Small chunks for demonstration\n",
    "        turn_id='turn_id',\n",
    "        agent_name='agent_name',\n",
    "        agent_task='agent_task',\n",
    "        tool_call_results='tool_call_results',\n",
    "        metadata='metadata'\n",
    "    ):\n",
    "        json_total += len(json_chunk)\n",
    "        print(f\"Read JSON chunk of {len(json_chunk)} items. Total: {json_total}\")\n",
    "\n",
    "    # Cleanup\n",
    "    os.remove('test_chunk.json')\n",
    "\n",
    "print(\"\\nStreaming examples completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a61477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data sizes at each step\n",
    "import os\n",
    "\n",
    "# Original parquet file size\n",
    "parquet_size = os.path.getsize(parquet_file_path) / (1024 * 1024)  # in MB\n",
    "print(f\"Original parquet file size: {parquet_size:.2f} MB\")\n",
    "\n",
    "# Read parquet and check DataFrame size\n",
    "df = pd.read_parquet(parquet_file_path)\n",
    "print(f\"\\nParquet DataFrame info:\")\n",
    "print(f\"Number of rows: {len(df)}\")\n",
    "print(f\"Number of columns: {len(df.columns)}\")\n",
    "print(\"\\nMemory usage per column:\")\n",
    "print(df.memory_usage(deep=True) / (1024 * 1024), \"MB\")\n",
    "\n",
    "# Check trajectory column\n",
    "print(\"\\nTrajectory column sample:\")\n",
    "if 'trajectory' in df.columns:\n",
    "    sample_traj = df['trajectory'].iloc[0]\n",
    "    print(f\"Type of trajectory: {type(sample_traj)}\")\n",
    "    if hasattr(sample_traj, '__len__'):\n",
    "        print(f\"Length of first trajectory: {len(sample_traj)}\")\n",
    "    print(\"\\nFirst trajectory content (truncated):\")\n",
    "    print(str(sample_traj)[:500], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf14c979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trajectory elements: 4898\n",
      "Processing sample.parquet\n",
      "Actual output rows: 4898\n",
      "\n",
      "Sample processing results:\n",
      "Input rows: 100\n",
      "Output rows: 4898\n",
      "Expansion factor: 48.98x\n",
      "\n",
      "Output DataFrame memory usage per column:\n",
      "Index               0.000122\n",
      "instance_id         0.389066\n",
      "model_name          0.354831\n",
      "target              0.004671\n",
      "exit_status         0.342138\n",
      "generated_patch     7.765799\n",
      "eval_logs          21.691992\n",
      "cutoff_date         0.152813\n",
      "mask                0.004671\n",
      "role                0.280552\n",
      "system_prompt       0.616585\n",
      "text                5.412631\n",
      "dtype: float64 MB\n"
     ]
    }
   ],
   "source": [
    "# Test preprocessing with a small sample first\n",
    "sample_size = 100  # adjust this number as needed\n",
    "df_sample = df.head(sample_size).copy()\n",
    "\n",
    "# Analyze trajectory lengths BEFORE preprocessing\n",
    "trajectory_lengths = []\n",
    "for idx, row in df_sample.iterrows():\n",
    "    traj = row['trajectory']\n",
    "    traj_len = len(traj) if hasattr(traj, '__len__') else 0\n",
    "    trajectory_lengths.append(traj_len)\n",
    "\n",
    "print(f\"Total trajectory elements: {sum(trajectory_lengths)}\")\n",
    "\n",
    "# Save sample to parquet\n",
    "sample_parquet = \"sample.parquet\"\n",
    "df_sample.to_parquet(sample_parquet)\n",
    "\n",
    "# Process the sample\n",
    "output_csv_sample = \"sample_output.csv\"\n",
    "swe_agent_trajectories_preprocessing(\n",
    "    parquet_files=[sample_parquet],\n",
    "    output_csv=output_csv_sample\n",
    ")\n",
    "\n",
    "# Check sizes AFTER preprocessing\n",
    "sample_output_df = pd.read_csv(output_csv_sample)\n",
    "print(f\"Actual output rows: {len(sample_output_df)}\")\n",
    "\n",
    "print(f\"\\nSample processing results:\")\n",
    "print(f\"Input rows: {len(df_sample)}\")\n",
    "print(f\"Output rows: {len(sample_output_df)}\")\n",
    "expansion_factor = len(sample_output_df) / len(df_sample)\n",
    "print(f\"Expansion factor: {expansion_factor:.2f}x\")\n",
    "\n",
    "# Check memory usage of output\n",
    "print(\"\\nOutput DataFrame memory usage per column:\")\n",
    "print(sample_output_df.memory_usage(deep=True) / (1024 * 1024), \"MB\")\n",
    "\n",
    "# Clean up sample files\n",
    "os.remove(sample_parquet)\n",
    "os.remove(output_csv_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16050f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
