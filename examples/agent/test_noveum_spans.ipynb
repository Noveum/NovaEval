{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Noveum Spans Dataset Testing\n",
    "\n",
    "This notebook tests the `noveum_spans_dataset.py` functionality with Noveum trace JSON files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your JSON file path here\n",
    "# Configuration - can be overridden with environment variables for portability\n",
    "JSON_FILE_PATH = os.getenv(\"NOVEUM_JSON_PATH\", \"/mnt/drive2/trace_details.json\")\n",
    "OUTPUT_CSV = os.getenv(\"NOVEUM_OUTPUT_CSV\", \"/mnt/drive2/noveum_spans_output.csv\")\n",
    "CHUNK_SIZE = int(os.getenv(\"NOVEUM_CHUNK_SIZE\", \"2\"))  # Set to 2 for testing purposes to verify streaming behavior\n",
    "\n",
    "# Alternative: Use relative paths if the files are in the current directory\n",
    "# JSON_FILE_PATH = os.getenv(\"NOVEUM_JSON_PATH\", \"./trace_details.json\")\n",
    "# OUTPUT_CSV = os.getenv(\"NOVEUM_OUTPUT_CSV\", \"./noveum_spans_output.csv\")\n",
    "\n",
    "# Output paths\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Import and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Add the parent directory to the path to import noveum_spans_dataset\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from novaeval.datasets.noveum_spans_dataset import (\n",
    "    create_dataset,\n",
    "    noveum_spans_preprocessing,\n",
    "    stream_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Check JSON File Structure\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Test Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing noveum_spans_preprocessing...\n",
      "✓ Preprocessing completed successfully!\n",
      "\n",
      "Output CSV created with 8 rows\n",
      "Columns: ['turn_id', 'agent_name', 'agent_task', 'agent_response', 'metadata', 'trace_id', 'span_name', 'status', 'start_time', 'end_time', 'attributes', 'exit_status', 'agent_exit']\n",
      "\n",
      "First few rows:\n",
      "                                turn_id            agent_name  \\\n",
      "0  f2d9563d-6a0b-4ff6-b46f-330c1cdf9b07  research_coordinator   \n",
      "1  d33dba4a-dc67-46ae-9836-ac49ed938c6f          search_agent   \n",
      "2  f1e4204b-9cbb-4322-9ba0-cfada1607da7       web_search_tool   \n",
      "3  8387dbbd-9923-4299-aef9-62a60b0e4c70  academic_search_tool   \n",
      "4  7826fa8a-3534-4206-a336-d15fe41ae35a        analysis_agent   \n",
      "\n",
      "                                          agent_task status  \n",
      "0              artificial intelligence in healthcare     ok  \n",
      "1                                                NaN     ok  \n",
      "2              artificial intelligence in healthcare     ok  \n",
      "3              artificial intelligence in healthcare     ok  \n",
      "4  [{'title': 'Understanding artificial intellige...     ok  \n"
     ]
    }
   ],
   "source": [
    "# Test the preprocessing function\n",
    "print(\"Testing noveum_spans_preprocessing...\")\n",
    "\n",
    "try:\n",
    "    noveum_spans_preprocessing(\n",
    "        json_files=[JSON_FILE_PATH],\n",
    "        output_csv=OUTPUT_CSV\n",
    "    )\n",
    "    print(\"✓ Preprocessing completed successfully!\")\n",
    "\n",
    "    # Check the output CSV\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        df = pd.read_csv(OUTPUT_CSV)\n",
    "        print(f\"\\nOutput CSV created with {len(df)} rows\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "        # Show first few rows\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        print(df[[\"turn_id\", \"agent_name\", \"agent_task\", \"status\"]].head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during preprocessing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ JSON file found: /mnt/drive2/trace_details.json\n",
      "\n",
      "Trace ID: 2d842385-2baa-40f3-8e12-9a7da4240d17\n",
      "Number of spans: 8\n",
      "Trace name: test_research_workflow_openai_gpt-3.5-turbo\n",
      "\n",
      "First span:\n",
      "  - Span ID: f2d9563d-6a0b-4ff6-b46f-330c1cdf9b07\n",
      "  - Name: agent:research_coordinator:research_coordinator\n",
      "  - Status: ok\n",
      "  - Duration: 1801.633 ms\n",
      "  - Function name: research_coordinator\n",
      "  - Input fields: ['agent.input.research_topic']\n"
     ]
    }
   ],
   "source": [
    "# Check if file exists and preview its structure\n",
    "if os.path.exists(JSON_FILE_PATH):\n",
    "    print(f\"✓ JSON file found: {JSON_FILE_PATH}\")\n",
    "\n",
    "    # Load and preview the JSON structure\n",
    "    with open(JSON_FILE_PATH) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(f\"\\nTrace ID: {data.get('trace_id')}\")\n",
    "    print(f\"Number of spans: {len(data.get('spans', []))}\")\n",
    "    print(f\"Trace name: {data.get('name')}\")\n",
    "\n",
    "    # Preview first span\n",
    "    if data.get(\"spans\"):\n",
    "        first_span = data[\"spans\"][0]\n",
    "        print(\"\\nFirst span:\")\n",
    "        print(f\"  - Span ID: {first_span.get('span_id')}\")\n",
    "        print(f\"  - Name: {first_span.get('name')}\")\n",
    "        print(f\"  - Status: {first_span.get('status')}\")\n",
    "        print(f\"  - Duration: {first_span.get('duration_ms')} ms\")\n",
    "\n",
    "        # Check attributes\n",
    "        attributes = first_span.get(\"attributes\", {})\n",
    "        print(f\"  - Function name: {attributes.get('function.name')}\")\n",
    "\n",
    "        # Check for input fields\n",
    "        input_fields = [k for k in attributes if k.startswith(\"agent.input.\") or k.startswith(\"tool.input.\")]\n",
    "        print(f\"  - Input fields: {input_fields}\")\n",
    "\n",
    "else:\n",
    "    print(f\"❌ JSON file not found: {JSON_FILE_PATH}\")\n",
    "    print(\"Please update the JSON_FILE_PATH variable above.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Test Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing noveum_spans_preprocessing...\n",
      "✓ Preprocessing completed successfully!\n",
      "\n",
      "Output CSV created with 8 rows\n",
      "Columns: ['turn_id', 'agent_name', 'agent_task', 'agent_response', 'metadata', 'trace_id', 'span_name', 'status', 'start_time', 'end_time', 'attributes', 'exit_status', 'agent_exit']\n",
      "\n",
      "First few rows:\n",
      "                                turn_id            agent_name  \\\n",
      "0  f2d9563d-6a0b-4ff6-b46f-330c1cdf9b07  research_coordinator   \n",
      "1  d33dba4a-dc67-46ae-9836-ac49ed938c6f          search_agent   \n",
      "2  f1e4204b-9cbb-4322-9ba0-cfada1607da7       web_search_tool   \n",
      "3  8387dbbd-9923-4299-aef9-62a60b0e4c70  academic_search_tool   \n",
      "4  7826fa8a-3534-4206-a336-d15fe41ae35a        analysis_agent   \n",
      "\n",
      "                                          agent_task status  \n",
      "0              artificial intelligence in healthcare     ok  \n",
      "1                                                NaN     ok  \n",
      "2              artificial intelligence in healthcare     ok  \n",
      "3              artificial intelligence in healthcare     ok  \n",
      "4  [{'title': 'Understanding artificial intellige...     ok  \n"
     ]
    }
   ],
   "source": [
    "# Test the preprocessing function\n",
    "print(\"Testing noveum_spans_preprocessing...\")\n",
    "\n",
    "try:\n",
    "    noveum_spans_preprocessing(\n",
    "        json_files=[JSON_FILE_PATH],\n",
    "        output_csv=OUTPUT_CSV\n",
    "    )\n",
    "    print(\"✓ Preprocessing completed successfully!\")\n",
    "\n",
    "    # Check the output CSV\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        df = pd.read_csv(OUTPUT_CSV)\n",
    "        print(f\"\\nOutput CSV created with {len(df)} rows\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "        # Show first few rows\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        print(df[[\"turn_id\", \"agent_name\", \"agent_task\", \"status\"]].head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during preprocessing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Test Dataset Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing create_dataset...\n",
      "✓ Dataset created successfully!\n",
      "\n",
      "Dataset info:\n",
      "Number of records: 8\n",
      "\n",
      "First record:\n",
      "  - Turn ID: f2d9563d-6a0b-4ff6-b46f-330c1cdf9b07\n",
      "  - Agent name: research_coordinator\n",
      "  - Agent role: coordinator\n",
      "  - Task: artificial intelligence in healthcare\n",
      "  - Response: {'topic': 'artificial intelligence in healthcare', 'search_results': [{'title': 'Understanding artif...\n",
      "  - Metadata keys: ['trace_id', 'duration_ms', 'parent_span_id', 'status_message', 'agent_type', 'exit_status', 'agent_exit', 'span_name', 'status', 'start_time', 'end_time']\n"
     ]
    }
   ],
   "source": [
    "# Test creating a dataset from the CSV\n",
    "print(\"Testing create_dataset...\")\n",
    "\n",
    "try:\n",
    "    dataset = create_dataset(OUTPUT_CSV)\n",
    "    print(\"✓ Dataset created successfully!\")\n",
    "\n",
    "    # Check dataset properties\n",
    "    print(\"\\nDataset info:\")\n",
    "    print(f\"Number of records: {len(dataset.data)}\")\n",
    "\n",
    "    if dataset.data:\n",
    "        first_record = dataset.data[0]\n",
    "        print(\"\\nFirst record:\")\n",
    "        print(f\"  - Turn ID: {first_record.turn_id}\")\n",
    "        print(f\"  - Agent name: {first_record.agent_name}\")\n",
    "        print(f\"  - Agent role: {first_record.agent_role}\")\n",
    "        if len(first_record.agent_task) > 100:\n",
    "            print(f\"  - Task: {first_record.agent_task[:100]}...\")\n",
    "        else:\n",
    "            print(f\"  - Task: {first_record.agent_task}\")\n",
    "        if len(first_record.agent_response) > 100:\n",
    "            print(f\"  - Response: {first_record.agent_response[:100]}...\")\n",
    "        else:\n",
    "            print(f\"  - Response: {first_record.agent_response}\")\n",
    "\n",
    "        # Check metadata\n",
    "        if first_record.metadata:\n",
    "            metadata = json.loads(first_record.metadata)\n",
    "            print(f\"  - Metadata keys: {list(metadata.keys())}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating dataset: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5.5. Sample Dataset Objects and Save to JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling random dataset objects...\n",
      "✓ Sampled 3 objects from dataset\n",
      "✓ Saved sample objects to: sample_dataset_object.json\n",
      "\n",
      "Preview of sampled objects:\n",
      "\n",
      "Sample 1:\n",
      "  - Turn ID: 8387dbbd-9923-4299-aef9-62a60b0e4c70\n",
      "  - Agent Name: academic_search_tool\n",
      "  - Agent Role: academic_database\n",
      "  - Task: artificial intelligence in healthcare\n",
      "  - Response: [{'title': 'Recent Advances in artificial intelligence in healthcare: A Comprehe...\n",
      "\n",
      "Sample 2:\n",
      "  - Turn ID: 7826fa8a-3534-4206-a336-d15fe41ae35a\n",
      "  - Agent Name: analysis_agent\n",
      "  - Agent Role: analyzer\n",
      "  - Task: [{'title': 'Understanding artificial intelligence in healthcare: A Comprehensive...\n",
      "  - Response: {'topic': 'artificial intelligence in healthcare', 'input_sources': 3, 'llm_anal...\n",
      "\n",
      "Sample 3:\n",
      "  - Turn ID: 4165ef25-8865-460d-8acd-e14e8fc9a47e\n",
      "  - Agent Name: summary_agent\n",
      "  - Agent Role: summarizer\n",
      "  - Task: {'topic': 'artificial intelligence in healthcare', 'input_sources': 3, 'llm_anal...\n",
      "  - Response: This research on artificial intelligence in healthcare offers a comprehensive ov...\n",
      "\n",
      "✓ Complete JSON objects saved to sample_dataset_object.json\n"
     ]
    }
   ],
   "source": [
    "# Sample a few random objects from the dataset and save them as JSON\n",
    "import random\n",
    "\n",
    "print(\"Sampling random dataset objects...\")\n",
    "\n",
    "try:\n",
    "    if 'dataset' in locals() and dataset.data:\n",
    "        # Sample 3 random objects (or fewer if dataset is smaller)\n",
    "        sample_size = min(3, len(dataset.data))\n",
    "        sampled_objects = random.sample(dataset.data, sample_size)\n",
    "        \n",
    "        print(f\"✓ Sampled {sample_size} objects from dataset\")\n",
    "        \n",
    "        # Convert AgentData objects to JSON-serializable format\n",
    "        sample_json_data = []\n",
    "        for obj in sampled_objects:\n",
    "            # Use the model_dump() method to convert Pydantic model to dict\n",
    "            sample_json_data.append(obj.model_dump())\n",
    "        \n",
    "        # Save to JSON file\n",
    "        sample_file_path = \"sample_dataset_object.json\"\n",
    "        with open(sample_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(sample_json_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"✓ Saved sample objects to: {sample_file_path}\")\n",
    "        \n",
    "        # Show preview of what was saved\n",
    "        print(\"\\nPreview of sampled objects:\")\n",
    "        for i, obj in enumerate(sampled_objects):\n",
    "            print(f\"\\nSample {i+1}:\")\n",
    "            print(f\"  - Turn ID: {obj.turn_id}\")\n",
    "            print(f\"  - Agent Name: {obj.agent_name}\")\n",
    "            print(f\"  - Agent Role: {obj.agent_role}\")\n",
    "            task_preview = obj.agent_task[:80] + \"...\" if len(obj.agent_task) > 80 else obj.agent_task\n",
    "            print(f\"  - Task: {task_preview}\")\n",
    "            response_preview = obj.agent_response[:80] + \"...\" if len(obj.agent_response) > 80 else obj.agent_response\n",
    "            print(f\"  - Response: {response_preview}\")\n",
    "            \n",
    "        print(f\"\\n✓ Complete JSON objects saved to {sample_file_path}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ No dataset available to sample from. Make sure the dataset creation step completed successfully.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error sampling dataset objects: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Test Streaming Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing stream_dataset with chunk_size=2...\n",
      "\n",
      "Chunk 1: 2 records\n",
      "  Record 1:\n",
      "    - Turn ID: f2d9563d-6a0b-4ff6-b46f-330c1cdf9b07\n",
      "    - Agent: research_coordinator\n",
      "    - Role: coordinator\n",
      "    - Task: artificial intelligence in healthcare\n",
      "  Record 2:\n",
      "    - Turn ID: d33dba4a-dc67-46ae-9836-ac49ed938c6f\n",
      "    - Agent: search_agent\n",
      "    - Role: researcher\n",
      "❌ Error during streaming: object of type 'NoneType' has no len()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_19741/1077325344.py\", line 22, in <module>\n",
      "    task_preview = record.agent_task[:50] + \"...\" if len(record.agent_task) > 50 else record.agent_task\n",
      "TypeError: object of type 'NoneType' has no len()\n"
     ]
    }
   ],
   "source": [
    "# Test streaming the dataset with chunk size 2\n",
    "print(f\"Testing stream_dataset with chunk_size={CHUNK_SIZE}...\")\n",
    "\n",
    "try:\n",
    "    chunk_count = 0\n",
    "    total_records = 0\n",
    "\n",
    "    for chunk in stream_dataset(OUTPUT_CSV, chunk_size=CHUNK_SIZE):\n",
    "        chunk_count += 1\n",
    "        chunk_size_actual = len(chunk)\n",
    "        total_records += chunk_size_actual\n",
    "\n",
    "        print(f\"\\nChunk {chunk_count}: {chunk_size_actual} records\")\n",
    "\n",
    "        # Show details for first chunk\n",
    "        if chunk_count == 1:\n",
    "            for i, record in enumerate(chunk):\n",
    "                print(f\"  Record {i+1}:\")\n",
    "                print(f\"    - Turn ID: {record.turn_id}\")\n",
    "                print(f\"    - Agent: {record.agent_name}\")\n",
    "                print(f\"    - Role: {record.agent_role}\")\n",
    "                task_preview = record.agent_task[:50] + \"...\" if len(record.agent_task) > 50 else record.agent_task\n",
    "                print(f\"    - Task: {task_preview}\")\n",
    "\n",
    "        # Limit output for large datasets\n",
    "        if chunk_count >= 5:\n",
    "            print(\"\\n... (showing first 5 chunks only)\")\n",
    "            # Continue counting without printing\n",
    "            # Continue counting remaining chunks without printing to preserve streaming behavior\n",
    "            # Skip the first 5 chunks and process the rest\n",
    "            skip_count = 0\n",
    "            for remaining_chunk in stream_dataset(OUTPUT_CSV, chunk_size=CHUNK_SIZE):\n",
    "                if skip_count < 5:\n",
    "                    skip_count += 1\n",
    "                    continue\n",
    "                chunk_count += 1\n",
    "                total_records += len(remaining_chunk)\n",
    "            break\n",
    "\n",
    "    print(\"\\n✓ Streaming completed!\")\n",
    "    print(f\"Total chunks: {chunk_count}\")\n",
    "    print(f\"Total records: {total_records}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during streaming: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. Validation and Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Analysis:\n",
      "\n",
      "Unique agent names: 8\n",
      "Agent name distribution:\n",
      "agent_name\n",
      "research_coordinator    1\n",
      "search_agent            1\n",
      "web_search_tool         1\n",
      "academic_search_tool    1\n",
      "analysis_agent          1\n",
      "llm_analysis_call       1\n",
      "summary_agent           1\n",
      "llm_summary_call        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Status distribution:\n",
      "status\n",
      "ok    8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Empty agent_task fields: 1\n",
      "Empty agent_response fields: 0\n",
      "\n",
      "Sample of different span types:\n",
      "\n",
      "research_coordinator:\n",
      "  Span name: agent:research_coordinator:research_coordinator\n",
      "  Task: artificial intelligence in healthcare\n",
      "  Response length: 1000 chars\n",
      "\n",
      "search_agent:\n",
      "  Span name: agent:search_agent:search_agent\n",
      "  Task: nan\n",
      "  Response length: 1000 chars\n",
      "\n",
      "web_search_tool:\n",
      "  Span name: tool:web_search:web_search_tool\n",
      "  Task: artificial intelligence in healthcare\n",
      "  Response length: 632 chars\n",
      "\n",
      "academic_search_tool:\n",
      "  Span name: tool:academic_search:academic_search_tool\n",
      "  Task: artificial intelligence in healthcare\n",
      "  Response length: 414 chars\n",
      "\n",
      "analysis_agent:\n",
      "  Span name: agent:analysis_agent:analysis_agent\n",
      "  Task: [{'title': 'Understanding artificial intelligence in healthcare: A Comprehensive Guide', 'url': 'htt...\n",
      "  Response length: 746 chars\n"
     ]
    }
   ],
   "source": [
    "# Analyze the processed data\n",
    "print(\"Data Analysis:\")\n",
    "\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    df = pd.read_csv(OUTPUT_CSV)\n",
    "\n",
    "    print(f\"\\nUnique agent names: {df['agent_name'].nunique()}\")\n",
    "    print(\"Agent name distribution:\")\n",
    "    print(df[\"agent_name\"].value_counts())\n",
    "\n",
    "    print(\"\\nStatus distribution:\")\n",
    "    print(df[\"status\"].value_counts())\n",
    "\n",
    "    # Check for empty fields\n",
    "    print(f\"\\nEmpty agent_task fields: {df['agent_task'].isna().sum() + (df['agent_task'] == '').sum()}\")\n",
    "    print(f\"Empty agent_response fields: {df['agent_response'].isna().sum() + (df['agent_response'] == '').sum()}\")\n",
    "\n",
    "    # Show a sample of different span types\n",
    "    print(\"\\nSample of different span types:\")\n",
    "    unique_names = df[\"agent_name\"].unique()[:5]\n",
    "    for name in unique_names:\n",
    "        sample = df[df[\"agent_name\"] == name].iloc[0]\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Span name: {sample['span_name']}\")\n",
    "        if len(str(sample[\"agent_task\"])) > 100:\n",
    "            print(f\"  Task: {sample['agent_task'][:100]}...\")\n",
    "        else:\n",
    "            print(f\"  Task: {sample['agent_task']}\")\n",
    "        print(f\"  Response length: {len(str(sample['agent_response']))} chars\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Output CSV not found\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 8. Cleanup (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing completed!\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the line below to clean up the output CSV file\n",
    "# os.remove(OUTPUT_CSV)\n",
    "# print(f\"Cleaned up {OUTPUT_CSV}\")\n",
    "\n",
    "print(\"Testing completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NovaEval (.venv)",
   "language": "python",
   "name": "novaeval_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
