{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The agent as shown on Noveum.ai platform\n",
        "\n",
        "![Alt text](support_agent.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Agent Evaluation Demo with NovaEval\n",
        "\n",
        "This notebook demonstrates a streamlined approach to agent evaluation using modular utility functions:\n",
        "\n",
        "1. **Load agent trace data** from JSON datasets\n",
        "2. **Map trace spans** to AgentData format using utility functions\n",
        "3. **Create and analyze** AgentDataset\n",
        "4. **Evaluate agent performance** using AgentEvaluator with Gemini model\n",
        "5. **Analyze results** and export data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scorers Used\n",
        "\n",
        "**context_relevancy_scorer** - Evaluates whether the agent response is appropriate and relevant given the agent's task and role.\n",
        "\n",
        "**role_adherence_scorer** - Scores whether the agent's tool calls and response adhere to its assigned role and task.\n",
        "\n",
        "**task_progression_scorer** - Measures whether the agent has made meaningful progress on the assigned task.\n",
        "\n",
        "**tool_relevancy_scorer** - Assesses how relevant and appropriate the tool call is given the available tools and the agent's context.\n",
        "\n",
        "**tool_correctness_scorer** - Compares actual tool calls against expected tool calls to evaluate correctness of tool usage and parameters.\n",
        "\n",
        "**parameter_correctness_scorer** - Validates whether correct parameters were passed to tool calls by analyzing the tool results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Dependencies and Utility Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All utility functions imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import our custom utility functions\n",
        "from demo_utils import (\n",
        "    list_dataset_files,\n",
        "    load_and_analyze_dataset,\n",
        "    convert_spans_to_agent_dataset,\n",
        "    analyze_dataset_statistics,\n",
        "    setup_gemini_model,\n",
        "    setup_agent_evaluator,\n",
        "    run_evaluation,\n",
        "    analyze_agent_behavior_patterns,\n",
        "    export_processed_dataset,\n",
        "    setup_logging,\n",
        "    validate_environment,\n",
        "    print_demo_summary\n",
        ")\n",
        "\n",
        "print(\"✅ All utility functions imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"GEMINI_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input file: /Users/mramanindia/Documents/Work/NovaEval/noveum_customer_support_bt/traces/traces/dataset_filtered_mapped.json\n",
            "Output directory: split_datasets\n",
            "\n",
            "Loading dataset from /Users/mramanindia/Documents/Work/NovaEval/noveum_customer_support_bt/traces/traces/dataset_filtered_mapped.json...\n",
            "Loaded 120 objects\n",
            "Found 8 unique span names\n",
            "Using sanitized name: agent.query_routing -> agent.query_routing_dataset.json\n",
            "  Wrote 20 objects to split_datasets/agent.query_routing_dataset.json\n",
            "Using sanitized name: agent.web_search_generation -> agent.web_search_generation_dataset.json\n",
            "  Wrote 8 objects to split_datasets/agent.web_search_generation_dataset.json\n",
            "Using sanitized name: agent.routing_evaluation_metrics -> agent.routing_evaluation_metrics_dataset.json\n",
            "  Wrote 20 objects to split_datasets/agent.routing_evaluation_metrics_dataset.json\n",
            "Using sanitized name: tool-orchestator -> tool-orchestator_dataset.json\n",
            "  Wrote 20 objects to split_datasets/tool-orchestator_dataset.json\n",
            "Using sanitized name: agent.llm_model_execution -> agent.llm_model_execution_dataset.json\n",
            "  Wrote 20 objects to split_datasets/agent.llm_model_execution_dataset.json\n",
            "Using sanitized name: agent.web_search_evaluation_metrics -> agent.web_search_evaluation_metrics_dataset.json\n",
            "  Wrote 8 objects to split_datasets/agent.web_search_evaluation_metrics_dataset.json\n",
            "Using sanitized name: agent.llm-rag -> agent.llm-rag_dataset.json\n",
            "  Wrote 12 objects to split_datasets/agent.llm-rag_dataset.json\n",
            "Using sanitized name: agent.rag_evaluation_metrics -> agent.rag_evaluation_metrics_dataset.json\n",
            "  Wrote 12 objects to split_datasets/agent.rag_evaluation_metrics_dataset.json\n",
            "\n",
            "Split complete! Created 8 files in split_datasets\n"
          ]
        }
      ],
      "source": [
        "!python preprocess_filter.py /Users/mramanindia/Documents/Work/NovaEval/noveum_customer_support_bt/traces/traces/dataset.json\n",
        "!python preprocess_map.py /Users/mramanindia/Documents/Work/NovaEval/noveum_customer_support_bt/traces/traces/dataset_filtered.json\n",
        "!python preprocess_split_data.py /Users/mramanindia/Documents/Work/NovaEval/noveum_customer_support_bt/traces/traces/dataset_filtered_mapped.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Force reload the demo_utils module to get the latest changes\n",
        "import importlib\n",
        "import sys\n",
        "\n",
        "# Remove the module from cache if it exists\n",
        "if 'demo_utils' in sys.modules:\n",
        "    del sys.modules['demo_utils']\n",
        "\n",
        "# Import the updated module\n",
        "from demo_utils import run_complete_agent_evaluation\n",
        "\n",
        "print(\"✅ Module reloaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Starting Complete Agent Evaluation Pipeline\n",
            "📁 Processing file: /Users/mramanindia/Documents/Work/NovaEval/noveum_customer_support_bt/split_datasets/agent.rag_evaluation_metrics_dataset.json\n",
            "============================================================\n",
            "\n",
            "📋 Step 1: Environment Setup\n",
            "✅ Logging configured at INFO level\n",
            "🔍 Environment validation:\n",
            "  ✅ gemini_api_key: True\n",
            "  ✅ pandas_available: True\n",
            "  ✅ novaeval_available: True\n",
            "✅ Environment ready for evaluation!\n",
            "\n",
            "📋 Step 2: Loading Dataset\n",
            "📊 Loaded 12 spans from /Users/mramanindia/Documents/Work/NovaEval/noveum_customer_support_bt/split_datasets/agent.rag_evaluation_metrics_dataset.json\n",
            "\n",
            "🔍 Available span types:\n",
            "  - agent.rag_evaluation_metrics: 12\n",
            "✅ Dataset loaded: 12 spans\n",
            "\n",
            "📋 Step 3: Converting to AgentDataset Format\n",
            "🔄 Converting spans to AgentData objects...\n",
            "\n",
            "✅ Successfully converted 12 spans to AgentData\n",
            "📊 AgentDataset created with 12 records\n",
            "✅ AgentDataset created: 12 records\n",
            "\n",
            "📋 Step 4: Dataset Analysis\n",
            "📈 Dataset Statistics:\n",
            "\n",
            "Agent Types: {'agent': 12}\n",
            "Records with responses: 12\n",
            "Records with tool calls: 0\n",
            "Records with retrieval: 12\n",
            "Tool usage: {}\n",
            "🔍 Dataset Analysis:\n",
            "\n",
            "=== Agent Behavior Patterns ===\n",
            "\n",
            "📈 Tool Usage:\n",
            "\n",
            "📋 Task Types:\n",
            "  - other: 12\n",
            "\n",
            "📝 Response Statistics:\n",
            "  - Average response length: 215.0 characters\n",
            "  - Min response length: 215\n",
            "  - Max response length: 215\n",
            "\n",
            "📋 Step 5: Setting up Evaluation\n",
            "✅ GEMINI_API_KEY found in environment\n",
            "2025-10-09 21:13:31 - WARNING - google_genai._api_client - Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n",
            "✅ Gemini model initialized\n",
            "✅ Initialized 5 scoring functions:\n",
            "  - task_progression_scorer\n",
            "  - context_relevancy_scorer\n",
            "  - role_adherence_scorer\n",
            "  - tool_relevancy_scorer\n",
            "  - parameter_correctness_scorer\n",
            "\n",
            "✅ AgentEvaluator created with Gemini model and scoring functions\n",
            "✅ Evaluation components ready!\n",
            "\n",
            "📋 Step 6: Running Evaluation\n",
            "🎯 Evaluating 25 samples...\n",
            "🚀 Running evaluation on sample data...\n",
            "\n",
            "📊 Evaluating 12 sample records...\n",
            "2025-10-09 21:13:31 - INFO - novaeval.evaluators.agent_evaluator - Starting agent evaluation process\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating samples: 0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-09 21:13:31 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:32 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:33 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:34 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 1 samples\n",
            "2025-10-09 21:13:34 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/agent.rag_evaluation_metrics_dataset/agent_evaluation_results.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating samples: 1it [00:03,  3.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-09 21:13:34 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:36 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:37 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:39 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 2 samples\n",
            "2025-10-09 21:13:39 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/agent.rag_evaluation_metrics_dataset/agent_evaluation_results.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating samples: 2it [00:08,  4.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-09 21:13:39 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:40 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:41 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:42 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 3 samples\n",
            "2025-10-09 21:13:42 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/agent.rag_evaluation_metrics_dataset/agent_evaluation_results.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating samples: 3it [00:11,  3.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-09 21:13:42 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:43 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:44 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:45 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 4 samples\n",
            "2025-10-09 21:13:45 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/agent.rag_evaluation_metrics_dataset/agent_evaluation_results.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating samples: 4it [00:14,  3.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-09 21:13:45 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:46 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:47 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:48 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 5 samples\n",
            "2025-10-09 21:13:48 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/agent.rag_evaluation_metrics_dataset/agent_evaluation_results.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating samples: 5it [00:17,  3.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-09 21:13:48 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:49 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:52 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:53 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 6 samples\n",
            "2025-10-09 21:13:53 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/agent.rag_evaluation_metrics_dataset/agent_evaluation_results.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating samples: 6it [00:21,  3.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-09 21:13:53 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:54 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:55 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:56 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 7 samples\n",
            "2025-10-09 21:13:56 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/agent.rag_evaluation_metrics_dataset/agent_evaluation_results.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating samples: 7it [00:24,  3.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-09 21:13:56 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:57 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:58 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:13:59 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 8 samples\n",
            "2025-10-09 21:13:59 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/agent.rag_evaluation_metrics_dataset/agent_evaluation_results.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating samples: 8it [00:28,  3.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-09 21:13:59 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:14:00 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:14:01 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:14:02 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 9 samples\n",
            "2025-10-09 21:14:02 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/agent.rag_evaluation_metrics_dataset/agent_evaluation_results.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating samples: 9it [00:31,  3.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-09 21:14:02 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:14:03 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:14:04 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:14:05 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 10 samples\n",
            "2025-10-09 21:14:05 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/agent.rag_evaluation_metrics_dataset/agent_evaluation_results.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating samples: 10it [00:34,  3.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-09 21:14:05 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:14:06 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:14:07 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:14:08 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 11 samples\n",
            "2025-10-09 21:14:08 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/agent.rag_evaluation_metrics_dataset/agent_evaluation_results.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating samples: 11it [00:37,  3.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-09 21:14:08 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:14:09 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:14:10 - INFO - google_genai.models - AFC is enabled with max remote calls: 10.\n",
            "2025-10-09 21:14:11 - INFO - novaeval.evaluators.agent_evaluator - Saving intermediate results after 12 samples\n",
            "2025-10-09 21:14:11 - INFO - novaeval.evaluators.agent_evaluator - Intermediate results saved to demo_results/agent.rag_evaluation_metrics_dataset/agent_evaluation_results.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating samples: 12it [00:40,  3.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-09 21:14:11 - INFO - novaeval.evaluators.agent_evaluator - Saving final results\n",
            "2025-10-09 21:14:11 - INFO - novaeval.evaluators.agent_evaluator - Reloaded 12 results from CSV\n",
            "2025-10-09 21:14:11 - INFO - novaeval.evaluators.agent_evaluator - Agent evaluation completed\n",
            "\n",
            "✅ Evaluation completed!\n",
            "\n",
            "📊 Results Summary:\n",
            "  - task_progression: 4.14\n",
            "  - context_relevancy: 7.93\n",
            "  - role_adherence: 9.00\n",
            "  - tool_relevancy: 0.00\n",
            "  - parameter_correctness: 0.00\n",
            "\n",
            "🔍 Individual Scores:\n",
            "\n",
            "  Record 1 (Task: f1f37bd7-0851-4659-b493-b80d3800d920):\n",
            "    - task_progression: 3.8\n",
            "    - context_relevancy: 7.8\n",
            "    - role_adherence: 9.0\n",
            "    - tool_relevancy: 0.0\n",
            "    - parameter_correctness: 0.0\n",
            "\n",
            "  Record 2 (Task: 52aacb67-c361-4445-9b72-c157f79f47d6):\n",
            "    - task_progression: 2.8\n",
            "    - context_relevancy: 7.8\n",
            "    - role_adherence: 9.0\n",
            "    - tool_relevancy: 0.0\n",
            "    - parameter_correctness: 0.0\n",
            "\n",
            "  Record 3 (Task: 2218f641-604c-491a-9710-b51a9941b982):\n",
            "    - task_progression: 4.3\n",
            "    - context_relevancy: 7.8\n",
            "    - role_adherence: 9.0\n",
            "    - tool_relevancy: 0.0\n",
            "    - parameter_correctness: 0.0\n",
            "\n",
            "  Record 4 (Task: 255fd49c-84b4-4b18-887e-6308a412d535):\n",
            "    - task_progression: 4.3\n",
            "    - context_relevancy: 7.8\n",
            "    - role_adherence: 9.0\n",
            "    - tool_relevancy: 0.0\n",
            "    - parameter_correctness: 0.0\n",
            "\n",
            "  Record 5 (Task: dc511122-c0b6-415c-9a49-c7b45132dd87):\n",
            "    - task_progression: 4.5\n",
            "    - context_relevancy: 8.1\n",
            "    - role_adherence: 9.0\n",
            "    - tool_relevancy: 0.0\n",
            "    - parameter_correctness: 0.0\n",
            "\n",
            "  Record 6 (Task: 04bebf38-a343-4563-80db-0154bef8d927):\n",
            "    - task_progression: 4.5\n",
            "    - context_relevancy: 7.8\n",
            "    - role_adherence: 9.0\n",
            "    - tool_relevancy: 0.0\n",
            "    - parameter_correctness: 0.0\n",
            "\n",
            "  Record 7 (Task: 5e043630-6493-42b5-beb8-79faa19bfa37):\n",
            "    - task_progression: 4.2\n",
            "    - context_relevancy: 8.1\n",
            "    - role_adherence: 9.0\n",
            "    - tool_relevancy: 0.0\n",
            "    - parameter_correctness: 0.0\n",
            "\n",
            "  Record 8 (Task: 7da9814d-a2e8-4c4e-b750-68b26bd5fd22):\n",
            "    - task_progression: 4.5\n",
            "    - context_relevancy: 7.8\n",
            "    - role_adherence: 9.0\n",
            "    - tool_relevancy: 0.0\n",
            "    - parameter_correctness: 0.0\n",
            "\n",
            "  Record 9 (Task: 16143f74-2831-4753-b33d-ce4b645093c5):\n",
            "    - task_progression: 4.2\n",
            "    - context_relevancy: 8.1\n",
            "    - role_adherence: 9.0\n",
            "    - tool_relevancy: 0.0\n",
            "    - parameter_correctness: 0.0\n",
            "\n",
            "  Record 10 (Task: fc64e6cc-6739-4256-ac4a-7b80c3028233):\n",
            "    - task_progression: 3.8\n",
            "    - context_relevancy: 7.8\n",
            "    - role_adherence: 9.0\n",
            "    - tool_relevancy: 0.0\n",
            "    - parameter_correctness: 0.0\n",
            "\n",
            "  Record 11 (Task: b7945c49-f584-4c70-972d-536a805d8a31):\n",
            "    - task_progression: 4.3\n",
            "    - context_relevancy: 8.1\n",
            "    - role_adherence: 9.0\n",
            "    - tool_relevancy: 0.0\n",
            "    - parameter_correctness: 0.0\n",
            "\n",
            "  Record 12 (Task: f5c40ecf-36c0-45ba-9cc9-dc0329b0324b):\n",
            "    - task_progression: 4.5\n",
            "    - context_relevancy: 8.2\n",
            "    - role_adherence: 9.0\n",
            "    - tool_relevancy: 0.0\n",
            "    - parameter_correctness: 0.0\n",
            "✅ Evaluation completed successfully!\n",
            "\n",
            "📋 Step 7: Exporting Dataset\n",
            "💾 Exporting processed dataset...\n",
            "✅ Exported to ./processed_datasets/agent.rag_evaluation_metrics_dataset_processed_dataset.json\n",
            "✅ Exported to ./processed_datasets/agent.rag_evaluation_metrics_dataset_processed_dataset.csv\n",
            "✅ Export completed successfully!\n",
            "\n",
            "============================================================\n",
            "🎉 EVALUATION PIPELINE COMPLETED!\n",
            "📊 Final Results:\n",
            "  - File processed: /Users/mramanindia/Documents/Work/NovaEval/noveum_customer_support_bt/split_datasets/agent.rag_evaluation_metrics_dataset.json\n",
            "  - Spans loaded: 12\n",
            "  - Dataset size: 12\n",
            "  - Evaluation completed: True\n",
            "  - Export successful: True\n",
            "  - Results saved to: ./demo_results/agent.rag_evaluation_metrics_dataset/\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'success': True,\n",
              " 'file_processed': '/Users/mramanindia/Documents/Work/NovaEval/noveum_customer_support_bt/split_datasets/agent.rag_evaluation_metrics_dataset.json',\n",
              " 'spans_loaded': 12,\n",
              " 'dataset_created': True,\n",
              " 'dataset_size': 12,\n",
              " 'evaluation_completed': True,\n",
              " 'results_df':     user_id                               task_id  \\\n",
              " 0       NaN  f1f37bd7-0851-4659-b493-b80d3800d920   \n",
              " 1       NaN  52aacb67-c361-4445-9b72-c157f79f47d6   \n",
              " 2       NaN  2218f641-604c-491a-9710-b51a9941b982   \n",
              " 3       NaN  255fd49c-84b4-4b18-887e-6308a412d535   \n",
              " 4       NaN  dc511122-c0b6-415c-9a49-c7b45132dd87   \n",
              " 5       NaN  04bebf38-a343-4563-80db-0154bef8d927   \n",
              " 6       NaN  5e043630-6493-42b5-beb8-79faa19bfa37   \n",
              " 7       NaN  7da9814d-a2e8-4c4e-b750-68b26bd5fd22   \n",
              " 8       NaN  16143f74-2831-4753-b33d-ce4b645093c5   \n",
              " 9       NaN  fc64e6cc-6739-4256-ac4a-7b80c3028233   \n",
              " 10      NaN  b7945c49-f584-4c70-972d-536a805d8a31   \n",
              " 11      NaN  f5c40ecf-36c0-45ba-9cc9-dc0329b0324b   \n",
              " \n",
              "                                  turn_id agent_name  task_progression  \\\n",
              " 0   ac166548-ffa5-47b9-8cf7-4787f270cb5a      agent               3.8   \n",
              " 1   e193cd4f-cca3-4b94-95d3-a1f2446b9fc6      agent               2.8   \n",
              " 2   432e74f3-3291-411b-8d7e-85493868f8cf      agent               4.3   \n",
              " 3   a0475dd5-ef9f-4eb2-bca4-9be72bf6bfd6      agent               4.3   \n",
              " 4   ed8ad539-f008-4f1a-beb1-12ad5793950e      agent               4.5   \n",
              " 5   c7358391-9ac1-4312-a4b3-79edfa8bc0d3      agent               4.5   \n",
              " 6   6b21de5d-456f-4501-953f-0ce5bc4fa5a5      agent               4.2   \n",
              " 7   2ad4e155-7b76-4040-923d-d6b65a97a45b      agent               4.5   \n",
              " 8   6b822089-be40-457d-9fa6-0979bafdfb85      agent               4.2   \n",
              " 9   08db68f6-3623-44b5-840b-41400fac7333      agent               3.8   \n",
              " 10  6a57c92e-e473-425a-978b-19dbe976a46e      agent               4.3   \n",
              " 11  efb19490-bc9e-48dc-a365-2f839100e41a      agent               4.5   \n",
              " \n",
              "     context_relevancy  role_adherence  tool_relevancy  parameter_correctness  \\\n",
              " 0                 7.8             9.0             0.0                    0.0   \n",
              " 1                 7.8             9.0             0.0                    0.0   \n",
              " 2                 7.8             9.0             0.0                    0.0   \n",
              " 3                 7.8             9.0             0.0                    0.0   \n",
              " 4                 8.1             9.0             0.0                    0.0   \n",
              " 5                 7.8             9.0             0.0                    0.0   \n",
              " 6                 8.1             9.0             0.0                    0.0   \n",
              " 7                 7.8             9.0             0.0                    0.0   \n",
              " 8                 8.1             9.0             0.0                    0.0   \n",
              " 9                 7.8             9.0             0.0                    0.0   \n",
              " 10                8.1             9.0             0.0                    0.0   \n",
              " 11                8.2             9.0             0.0                    0.0   \n",
              " \n",
              "                            task_progression_reasoning  \\\n",
              " 0   The agent understands its role and the task, a...   \n",
              " 1   The agent acknowledges the task and attempts t...   \n",
              " 2   The agent correctly identifies benefits of Nov...   \n",
              " 3   The agent correctly identifies the need for th...   \n",
              " 4   The agent correctly understands its role and t...   \n",
              " 5   The agent correctly understood its role and th...   \n",
              " 6   The agent correctly identifies the limitations...   \n",
              " 7   The agent successfully addresses the query by ...   \n",
              " 8   The agent successfully answered the question b...   \n",
              " 9   The agent correctly identifies that the provid...   \n",
              " 10  The agent correctly identifies the task and pr...   \n",
              " 11  The agent has provided a relevant and helpful ...   \n",
              " \n",
              "                           context_relevancy_reasoning  \\\n",
              " 0   The agent's response is relevant, acknowledgin...   \n",
              " 1   The agent acknowledges the lack of direct info...   \n",
              " 2   The response directly answers the question abo...   \n",
              " 3   The response directly addresses the query by s...   \n",
              " 4   The response is highly relevant and directly a...   \n",
              " 5   The response directly addresses the query abou...   \n",
              " 6   The agent's response directly addresses the qu...   \n",
              " 7   The agent response is directly relevant to the...   \n",
              " 8   The response is directly relevant, providing a...   \n",
              " 9   The response directly addresses the query abou...   \n",
              " 10  The response is highly relevant, providing a c...   \n",
              " 11  The response is highly relevant, directly addr...   \n",
              " \n",
              "                              role_adherence_reasoning  \\\n",
              " 0   The agent effectively fulfills its role by pro...   \n",
              " 1   The agent's response is directly relevant to t...   \n",
              " 2   The agent perfectly adheres to its role by pro...   \n",
              " 3   The agent perfectly adheres to its role by pro...   \n",
              " 4   The agent successfully fulfills its role by pr...   \n",
              " 5   The agent adheres perfectly to its role, provi...   \n",
              " 6   The agent perfectly adheres to its role by pro...   \n",
              " 7   The agent perfectly adheres to its role as a d...   \n",
              " 8   The agent perfectly adheres to its role by pro...   \n",
              " 9   The agent successfully fulfills its role by an...   \n",
              " 10  The agent perfectly adheres to its role. The r...   \n",
              " 11  The agent effectively fulfills its role by pro...   \n",
              " \n",
              "                              tool_relevancy_reasoning  \\\n",
              " 0   Error: Missing required fields: ['tool_calls']...   \n",
              " 1   Error: Missing required fields: ['tool_calls']...   \n",
              " 2   Error: Missing required fields: ['tool_calls']...   \n",
              " 3   Error: Missing required fields: ['tool_calls']...   \n",
              " 4   Error: Missing required fields: ['tool_calls']...   \n",
              " 5   Error: Missing required fields: ['tool_calls']...   \n",
              " 6   Error: Missing required fields: ['tool_calls']...   \n",
              " 7   Error: Missing required fields: ['tool_calls']...   \n",
              " 8   Error: Missing required fields: ['tool_calls']...   \n",
              " 9   Error: Missing required fields: ['tool_calls']...   \n",
              " 10  Error: Missing required fields: ['tool_calls']...   \n",
              " 11  Error: Missing required fields: ['tool_calls']...   \n",
              " \n",
              "                       parameter_correctness_reasoning  \n",
              " 0   Error: Missing required fields: ['tool_calls']...  \n",
              " 1   Error: Missing required fields: ['tool_calls']...  \n",
              " 2   Error: Missing required fields: ['tool_calls']...  \n",
              " 3   Error: Missing required fields: ['tool_calls']...  \n",
              " 4   Error: Missing required fields: ['tool_calls']...  \n",
              " 5   Error: Missing required fields: ['tool_calls']...  \n",
              " 6   Error: Missing required fields: ['tool_calls']...  \n",
              " 7   Error: Missing required fields: ['tool_calls']...  \n",
              " 8   Error: Missing required fields: ['tool_calls']...  \n",
              " 9   Error: Missing required fields: ['tool_calls']...  \n",
              " 10  Error: Missing required fields: ['tool_calls']...  \n",
              " 11  Error: Missing required fields: ['tool_calls']...  ,\n",
              " 'export_success': True,\n",
              " 'errors': []}"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from demo_utils import run_complete_agent_evaluation\n",
        "run_complete_agent_evaluation('/Users/mramanindia/Documents/Work/NovaEval/noveum_customer_support_bt/split_datasets/agent.rag_evaluation_metrics_dataset.json',\n",
        "evaluation_name = \"agent.rag_evaluation_metrics_dataset\", output_dir = \"./demo_results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analysis of poor scores in comment generation agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'demo_results/agent_comment_gen_dataset/agent_evaluation_results.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m comment_gen = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdemo_results/agent_comment_gen_dataset/agent_evaluation_results.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m split_size = \u001b[32m3\u001b[39m\n\u001b[32m      6\u001b[39m task_progression = comment_gen.sort_values(by = \u001b[33m'\u001b[39m\u001b[33mtask_progression\u001b[39m\u001b[33m'\u001b[39m, ascending= \u001b[38;5;28;01mTrue\u001b[39;00m).iloc[:split_size][[\u001b[33m'\u001b[39m\u001b[33mtask_progression\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtask_progression_reasoning\u001b[39m\u001b[33m'\u001b[39m]]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/NovaEval/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/NovaEval/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/NovaEval/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/NovaEval/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/NovaEval/venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'demo_results/agent_comment_gen_dataset/agent_evaluation_results.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "comment_gen = pd.read_csv(\"demo_results/agent_comment_gen_dataset/agent_evaluation_results.csv\")\n",
        "\n",
        "split_size = 3\n",
        "\n",
        "task_progression = comment_gen.sort_values(by = 'task_progression', ascending= True).iloc[:split_size][['task_progression', 'task_progression_reasoning']]\n",
        "\n",
        "print(\"Task Progression:\")\n",
        "print()\n",
        "for idx, row in task_progression.iterrows():\n",
        "    print(f\"Score = {row['task_progression']}\")\n",
        "    print(f\"Reasoning = {row['task_progression_reasoning']}\")\n",
        "    print()  # blank line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'comment_gen' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Context Relevancy Analysis\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m context_relevancy = \u001b[43mcomment_gen\u001b[49m.sort_values(by=\u001b[33m'\u001b[39m\u001b[33mcontext_relevancy\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mTrue\u001b[39;00m).iloc[:\u001b[32m3\u001b[39m][[\u001b[33m'\u001b[39m\u001b[33mcontext_relevancy\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontext_relevancy_reasoning\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mContext Relevancy Analysis:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'comment_gen' is not defined"
          ]
        }
      ],
      "source": [
        "# Context Relevancy Analysis\n",
        "context_relevancy = comment_gen.sort_values(by='context_relevancy', ascending=True).iloc[:3][['context_relevancy', 'context_relevancy_reasoning']]\n",
        "\n",
        "print(\"Context Relevancy Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "for idx, row in context_relevancy.iterrows():\n",
        "    print(f\"Score = {row['context_relevancy']}\")\n",
        "    print(f\"Reasoning = {row['context_relevancy_reasoning']}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'comment_gen' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Role Adherence Analysis\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m role_adherence = \u001b[43mcomment_gen\u001b[49m.sort_values(by=\u001b[33m'\u001b[39m\u001b[33mrole_adherence\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mTrue\u001b[39;00m).iloc[:\u001b[32m3\u001b[39m][[\u001b[33m'\u001b[39m\u001b[33mrole_adherence\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrole_adherence_reasoning\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRole Adherence Analysis:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'comment_gen' is not defined"
          ]
        }
      ],
      "source": [
        "# Role Adherence Analysis\n",
        "role_adherence = comment_gen.sort_values(by='role_adherence', ascending=True).iloc[:3][['role_adherence', 'role_adherence_reasoning']]\n",
        "\n",
        "print(\"Role Adherence Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "for idx, row in role_adherence.iterrows():\n",
        "    print(f\"Score = {row['role_adherence']}\")\n",
        "    print(f\"Reasoning = {row['role_adherence_reasoning']}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "NOVAPILOT AGENT ANALYSIS - RECOMMEND IMPROVEMENTS\n",
            "============================================================\n",
            "This function runs the complete analysis pipeline equivalent to\n",
            "running the entire complete_analysis_demo.ipynb notebook.\n",
            "============================================================\n",
            "Setup complete! Log file: log/analysis_log_20251009_191149.txt\n",
            "Warning: Agent document not found at reddit_agent.md\n",
            "Found 5 dataset directories to process:\n",
            "  - agent.rag_evaluation_metrics_dataset\n",
            "  - agent_comment_gen_dataset\n",
            "  - agent.query_routing_dataset\n",
            "  - agent.llm-rag_dataset\n",
            "  - agent.web_search_generation_dataset\n",
            "\n",
            "Processing agent.rag_evaluation_metrics_dataset...\n",
            "  No CSV files found in agent.rag_evaluation_metrics_dataset, skipping...\n",
            "\n",
            "Processing agent_comment_gen_dataset...\n",
            "  No CSV files found in agent_comment_gen_dataset, skipping...\n",
            "\n",
            "Processing agent.query_routing_dataset...\n",
            "  No CSV files found in agent.query_routing_dataset, skipping...\n",
            "\n",
            "Processing agent.llm-rag_dataset...\n",
            "  No CSV files found in agent.llm-rag_dataset, skipping...\n",
            "\n",
            "Processing agent.web_search_generation_dataset...\n",
            "  No CSV files found in agent.web_search_generation_dataset, skipping...\n",
            "\n",
            "Completed processing 5 datasets.\n",
            "\n",
            "Making final comprehensive analysis call...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "No agent documentation provided. Load it first or pass as parameter.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnovapilot_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m recommend_improvements\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Advanced usage with custom parameters\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m final_analysis, summaries, log_file = \u001b[43mrecommend_improvements\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdemo_results_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdemo_results/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent_doc_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreddit_agent.md\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlog\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/NovaEval/noveum_customer_support_bt/novapilot_utils.py:548\u001b[39m, in \u001b[36mrecommend_improvements\u001b[39m\u001b[34m(demo_results_dir, agent_doc_path, log_dir, api_key, model_name, verbose)\u001b[39m\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m    546\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMaking final comprehensive analysis call...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m final_analysis = \u001b[43manalyzer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_final_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_summaries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_doc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m    551\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFinal analysis completed and logged!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Work/NovaEval/noveum_customer_support_bt/novapilot_utils.py:365\u001b[39m, in \u001b[36mNovaPilotAnalyzer.create_final_analysis\u001b[39m\u001b[34m(self, dataset_summaries, agent_doc)\u001b[39m\n\u001b[32m    362\u001b[39m     agent_doc = \u001b[38;5;28mself\u001b[39m.reddit_agent_doc\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m agent_doc:\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo agent documentation provided. Load it first or pass as parameter.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    367\u001b[39m \u001b[38;5;66;03m# Combine all dataset summaries\u001b[39;00m\n\u001b[32m    368\u001b[39m combined_summaries = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(dataset_summaries)\n",
            "\u001b[31mValueError\u001b[39m: No agent documentation provided. Load it first or pass as parameter."
          ]
        }
      ],
      "source": [
        "from novapilot_utils import recommend_improvements\n",
        "\n",
        "# Advanced usage with custom parameters\n",
        "final_analysis, summaries, log_file = recommend_improvements(\n",
        "    demo_results_dir=\"demo_results/\",\n",
        "    agent_doc_path=\"reddit_agent.md\",\n",
        "    log_dir=\"log\",\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(final_analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
