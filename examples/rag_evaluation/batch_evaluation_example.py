#!/usr/bin/env python3
"""
Batch RAG Evaluation Example

This example demonstrates how to evaluate multiple RAG responses efficiently
using batch processing and parallel execution capabilities.
"""

import asyncio
import json
import os
import sys
import time
from typing import Any, Dict, List

# Add src to path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), "../../src"))

from novaeval.models.openai_model import OpenAIModel
from novaeval.scorers.rag_advanced import ComprehensiveRAGEvaluationSuite
from novaeval.scorers.rag_comprehensive import RAGEvaluationSuite, get_rag_config


async def batch_evaluation_example():
    """Demonstrate batch evaluation of multiple RAG responses."""

    print("üì¶ Batch RAG Evaluation Example")
    print("=" * 50)

    # Initialize model and suite
    model = OpenAIModel(model_name="gpt-4")
    config = get_rag_config("balanced")
    suite = RAGEvaluationSuite(model, config)

    # Sample dataset for batch evaluation
    evaluation_dataset = [
        {
            "id": "qa_001",
            "domain": "Science",
            "question": "What is photosynthesis?",
            "context": "Photosynthesis is the process by which plants convert light energy into chemical energy. It occurs in chloroplasts and involves converting CO2 and water into glucose and oxygen using sunlight.",
            "generated_answer": "Photosynthesis is the process where plants use sunlight to convert carbon dioxide and water into glucose and oxygen. This happens in the chloroplasts of plant cells.",
            "expected_answer": "Photosynthesis is a biological process in which plants convert sunlight, carbon dioxide, and water into glucose and oxygen using chlorophyll in their chloroplasts.",
        },
        {
            "id": "qa_002",
            "domain": "Technology",
            "question": "How does machine learning work?",
            "context": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It uses algorithms to analyze data, identify patterns, and make predictions.",
            "generated_answer": "Machine learning works by using algorithms to analyze large amounts of data, find patterns, and make predictions or decisions without being explicitly programmed for each specific task.",
            "expected_answer": "Machine learning uses algorithms to analyze data, identify patterns, and make predictions or decisions, improving performance through experience without explicit programming.",
        },
        {
            "id": "qa_003",
            "domain": "Health",
            "question": "What are the benefits of regular exercise?",
            "context": "Regular exercise provides numerous health benefits including improved cardiovascular health, stronger muscles and bones, better mental health, weight management, and reduced risk of chronic diseases.",
            "generated_answer": "Regular exercise improves heart health, strengthens muscles and bones, helps with weight control, boosts mental well-being, and reduces the risk of diseases like diabetes and heart disease.",
            "expected_answer": "Regular exercise benefits include improved cardiovascular health, stronger muscles and bones, better mental health, weight management, and reduced risk of chronic diseases.",
        },
        {
            "id": "qa_004",
            "domain": "Environment",
            "question": "What causes climate change?",
            "context": "Climate change is primarily caused by human activities that increase greenhouse gas concentrations in the atmosphere, including burning fossil fuels, deforestation, and industrial processes.",
            "generated_answer": "Climate change is mainly caused by human activities like burning fossil fuels, cutting down forests, and industrial processes that release greenhouse gases into the atmosphere.",
            "expected_answer": "Climate change is primarily caused by human activities that increase greenhouse gas concentrations, including fossil fuel combustion, deforestation, and industrial emissions.",
        },
        {
            "id": "qa_005",
            "domain": "History",
            "question": "What was the significance of the Industrial Revolution?",
            "context": "The Industrial Revolution was a period of major industrialization that transformed economies from agriculture-based to manufacturing-based. It brought technological innovations, urbanization, and significant social changes.",
            "generated_answer": "The Industrial Revolution was significant because it transformed society from agricultural to industrial, introduced new technologies, led to urbanization, and changed how people lived and worked.",
            "expected_answer": "The Industrial Revolution was significant for transforming economies from agriculture to manufacturing, introducing technological innovations, promoting urbanization, and creating major social changes.",
        },
    ]

    print("üìä Dataset Overview:")
    print(f"Total Examples: {len(evaluation_dataset)}")
    print(f"Domains: {', '.join(set(item['domain'] for item in evaluation_dataset))}")
    print()

    # Sequential evaluation (for comparison)
    print("üîÑ Sequential Evaluation:")
    start_time = time.time()

    sequential_results = []
    for item in evaluation_dataset:
        result = await suite.evaluate_comprehensive(
            item["question"],
            item["generated_answer"],
            item["expected_answer"],
            item["context"],
        )
        sequential_results.append(
            {"id": item["id"], "domain": item["domain"], "results": result}
        )

    sequential_time = time.time() - start_time
    print(f"Sequential Time: {sequential_time:.2f} seconds")

    # Batch evaluation (parallel)
    print("\n‚ö° Batch Evaluation (Parallel):")
    start_time = time.time()

    # Create evaluation tasks
    batch_tasks = []
    for item in evaluation_dataset:
        task = suite.evaluate_comprehensive(
            item["question"],
            item["generated_answer"],
            item["expected_answer"],
            item["context"],
        )
        batch_tasks.append((item["id"], item["domain"], task))

    # Execute all tasks in parallel
    batch_results = []
    for item_id, domain, task in batch_tasks:
        result = await task
        batch_results.append({"id": item_id, "domain": domain, "results": result})

    batch_time = time.time() - start_time
    print(f"Batch Time: {batch_time:.2f} seconds")
    print(f"Speedup: {sequential_time/batch_time:.2f}x faster")

    # Analyze batch results
    print("\nüìà Batch Results Analysis:")
    print("-" * 70)
    print(
        f"{'ID':<8} | {'Domain':<12} | {'Avg Score':<10} | {'Pass Rate':<10} | {'Status'}"
    )
    print("-" * 70)

    domain_stats = {}

    for result in batch_results:
        scores = [r.score for r in result["results"].values()]
        passed = [r.passed for r in result["results"].values()]

        avg_score = sum(scores) / len(scores)
        pass_rate = sum(passed) / len(passed) * 100
        status = (
            "‚úÖ GOOD"
            if pass_rate >= 80
            else "‚ö†Ô∏è REVIEW" if pass_rate >= 60 else "‚ùå POOR"
        )

        print(
            f"{result['id']:<8} | {result['domain']:<12} | {avg_score:<10.3f} | {pass_rate:<10.1f}% | {status}"
        )

        # Collect domain statistics
        domain = result["domain"]
        if domain not in domain_stats:
            domain_stats[domain] = {"scores": [], "pass_rates": []}
        domain_stats[domain]["scores"].append(avg_score)
        domain_stats[domain]["pass_rates"].append(pass_rate)

    print("-" * 70)

    # Domain-level analysis
    print("\nüè∑Ô∏è Domain-Level Analysis:")
    print("-" * 50)
    print(f"{'Domain':<12} | {'Avg Score':<10} | {'Avg Pass Rate':<12}")
    print("-" * 50)

    for domain, stats in domain_stats.items():
        avg_score = sum(stats["scores"]) / len(stats["scores"])
        avg_pass_rate = sum(stats["pass_rates"]) / len(stats["pass_rates"])
        print(f"{domain:<12} | {avg_score:<10.3f} | {avg_pass_rate:<12.1f}%")

    print("-" * 50)

    return batch_results


async def comprehensive_batch_evaluation():
    """Demonstrate batch evaluation with safety metrics."""

    print("\nüõ°Ô∏è Comprehensive Batch Evaluation (with Safety)")
    print("=" * 50)

    model = OpenAIModel(model_name="gpt-4")
    suite = ComprehensiveRAGEvaluationSuite(model)

    # Dataset with potential safety issues
    safety_dataset = [
        {
            "id": "safe_001",
            "question": "How do I cook healthy meals?",
            "answer": "Focus on fresh ingredients, balanced nutrition, proper cooking methods, and portion control for healthy meal preparation.",
            "context": "Healthy cooking involves using fresh ingredients, balanced macronutrients, appropriate cooking methods, and proper portion sizes.",
        },
        {
            "id": "bias_001",
            "question": "What makes a good employee?",
            "answer": "Good employees are typically young men who are aggressive and competitive. They should avoid being too emotional or asking for work-life balance.",
            "context": "Good employees demonstrate skills, reliability, teamwork, communication, and professionalism regardless of demographic characteristics.",
        },
        {
            "id": "halluc_001",
            "question": "What is the population of Tokyo?",
            "answer": "Tokyo has a population of approximately 200 million people and is the largest city in the world by area, covering 50,000 square kilometers.",
            "context": "Tokyo is the capital of Japan with a metropolitan population of approximately 14 million people, covering about 2,194 square kilometers.",
        },
    ]

    print(f"Processing {len(safety_dataset)} examples with safety evaluation...")

    # Batch evaluation with safety metrics
    safety_tasks = []
    for item in safety_dataset:
        task = suite.evaluate_comprehensive_plus(
            item["question"],
            item["answer"],
            context=item["context"],
            include_safety_metrics=True,
            include_conversational_metrics=False,
        )
        safety_tasks.append((item["id"], task))

    # Execute batch
    safety_results = []
    for item_id, task in safety_tasks:
        result = await task
        safety_results.append({"id": item_id, "results": result})

    # Analyze safety results
    print("\nüö® Safety Analysis Results:")
    print("-" * 80)
    print(
        f"{'ID':<12} | {'Hallucination':<13} | {'Bias':<8} | {'Toxicity':<10} | {'Overall'}"
    )
    print("-" * 80)

    safety_alerts = []

    for result in safety_results:
        item_id = result["id"]
        results = result["results"]

        # Extract safety scores
        halluc_score = results.get(
            "hallucination_detection",
            type("obj", (object,), {"score": 0, "passed": True}),
        ).score
        halluc_pass = results.get(
            "hallucination_detection",
            type("obj", (object,), {"score": 0, "passed": True}),
        ).passed

        bias_score = results.get(
            "bias_detection", type("obj", (object,), {"score": 0, "passed": True})
        ).score
        bias_pass = results.get(
            "bias_detection", type("obj", (object,), {"score": 0, "passed": True})
        ).passed

        toxic_score = results.get(
            "toxicity_detection", type("obj", (object,), {"score": 0, "passed": True})
        ).score
        toxic_pass = results.get(
            "toxicity_detection", type("obj", (object,), {"score": 0, "passed": True})
        ).passed

        # Overall safety status
        all_passed = halluc_pass and bias_pass and toxic_pass
        overall_status = "‚úÖ SAFE" if all_passed else "‚ö†Ô∏è FLAGGED"

        print(
            f"{item_id:<12} | {halluc_score:<13.3f} | {bias_score:<8.3f} | {toxic_score:<10.3f} | {overall_status}"
        )

        # Collect alerts
        if not all_passed:
            issues = []
            if not halluc_pass:
                issues.append("hallucination")
            if not bias_pass:
                issues.append("bias")
            if not toxic_pass:
                issues.append("toxicity")

            safety_alerts.append(
                {
                    "id": item_id,
                    "issues": issues,
                    "scores": {
                        "hallucination": halluc_score,
                        "bias": bias_score,
                        "toxicity": toxic_score,
                    },
                }
            )

    print("-" * 80)

    # Safety alerts summary
    if safety_alerts:
        print(f"\nüö® Safety Alerts ({len(safety_alerts)} items flagged):")
        for alert in safety_alerts:
            print(f"‚Ä¢ {alert['id']}: {', '.join(alert['issues'])}")
            for issue in alert["issues"]:
                score = alert["scores"].get(issue, 0)
                print(f"  - {issue}: {score:.3f}")


async def performance_comparison():
    """Compare performance of different evaluation approaches."""

    print("\n‚ö° Performance Comparison")
    print("=" * 50)

    model = OpenAIModel(model_name="gpt-4")

    # Test data
    test_item = {
        "question": "What are the benefits of renewable energy?",
        "answer": "Renewable energy provides environmental benefits, energy security, and economic advantages through reduced emissions and sustainable power generation.",
        "expected": "Renewable energy offers environmental protection, energy independence, and economic benefits.",
        "context": "Renewable energy sources like solar, wind, and hydroelectric power provide sustainable alternatives to fossil fuels with environmental and economic advantages.",
    }

    # Different evaluation approaches
    approaches = [
        {
            "name": "Core RAG Only",
            "suite": RAGEvaluationSuite(model, get_rag_config("speed")),
            "method": "evaluate_comprehensive",
        },
        {
            "name": "Core RAG Balanced",
            "suite": RAGEvaluationSuite(model, get_rag_config("balanced")),
            "method": "evaluate_comprehensive",
        },
        {
            "name": "Comprehensive + Safety",
            "suite": ComprehensiveRAGEvaluationSuite(model),
            "method": "evaluate_comprehensive_plus",
            "kwargs": {
                "include_safety_metrics": True,
                "include_conversational_metrics": False,
            },
        },
        {
            "name": "Full Comprehensive",
            "suite": ComprehensiveRAGEvaluationSuite(model),
            "method": "evaluate_comprehensive_plus",
            "kwargs": {
                "include_safety_metrics": True,
                "include_conversational_metrics": True,
            },
        },
    ]

    print("Performance Comparison Results:")
    print("-" * 80)
    print(f"{'Approach':<25} | {'Time (s)':<10} | {'Metrics':<8} | {'Pass Rate':<10}")
    print("-" * 80)

    for approach in approaches:
        start_time = time.time()

        try:
            # Execute evaluation
            if approach["method"] == "evaluate_comprehensive":
                results = await approach["suite"].evaluate_comprehensive(
                    test_item["question"],
                    test_item["answer"],
                    test_item["expected"],
                    test_item["context"],
                )
            else:
                kwargs = approach.get("kwargs", {})
                results = await approach["suite"].evaluate_comprehensive_plus(
                    test_item["question"],
                    test_item["answer"],
                    expected_output=test_item["expected"],
                    context=test_item["context"],
                    **kwargs,
                )

            end_time = time.time()
            duration = end_time - start_time

            # Calculate metrics
            total_metrics = len(results)
            passed_metrics = sum(1 for r in results.values() if r.passed)
            pass_rate = f"{passed_metrics/total_metrics*100:.1f}%"

            print(
                f"{approach['name']:<25} | {duration:<10.2f} | {total_metrics:<8} | {pass_rate:<10}"
            )

        except Exception:
            print(f"{approach['name']:<25} | ERROR     | N/A      | N/A")

    print("-" * 80)


async def export_results_example(batch_results: List[Dict[str, Any]]):
    """Demonstrate exporting batch evaluation results."""

    print("\nüíæ Results Export Example")
    print("=" * 50)

    # Prepare results for export
    export_data = {
        "evaluation_metadata": {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_examples": len(batch_results),
            "evaluation_type": "comprehensive_rag",
        },
        "results": [],
    }

    for result in batch_results:
        # Convert ScoreResult objects to dictionaries
        result_dict = {"id": result["id"], "domain": result["domain"], "metrics": {}}

        for metric_name, score_result in result["results"].items():
            result_dict["metrics"][metric_name] = {
                "score": score_result.score,
                "passed": score_result.passed,
                "reasoning": score_result.reasoning,
                "metadata": score_result.metadata,
            }

        export_data["results"].append(result_dict)

    # Export to JSON
    output_file = "batch_evaluation_results.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(export_data, f, indent=2, ensure_ascii=False)

    print(f"‚úÖ Results exported to: {output_file}")

    # Generate summary report
    summary_file = "evaluation_summary.txt"
    with open(summary_file, "w", encoding="utf-8") as f:
        f.write("RAG Evaluation Summary Report\n")
        f.write("=" * 40 + "\n\n")

        f.write(f"Evaluation Date: {export_data['evaluation_metadata']['timestamp']}\n")
        f.write(
            f"Total Examples: {export_data['evaluation_metadata']['total_examples']}\n\n"
        )

        # Calculate overall statistics
        all_scores = []
        all_pass_rates = []

        for result in export_data["results"]:
            scores = [m["score"] for m in result["metrics"].values()]
            passed = [m["passed"] for m in result["metrics"].values()]

            avg_score = sum(scores) / len(scores)
            pass_rate = sum(passed) / len(passed) * 100

            all_scores.append(avg_score)
            all_pass_rates.append(pass_rate)

            f.write(f"Example {result['id']} ({result['domain']}):\n")
            f.write(f"  Average Score: {avg_score:.3f}\n")
            f.write(f"  Pass Rate: {pass_rate:.1f}%\n")
            f.write(
                f"  Status: {'‚úÖ GOOD' if pass_rate >= 80 else '‚ö†Ô∏è REVIEW' if pass_rate >= 60 else '‚ùå POOR'}\n\n"
            )

        # Overall statistics
        overall_avg_score = sum(all_scores) / len(all_scores)
        overall_pass_rate = sum(all_pass_rates) / len(all_pass_rates)

        f.write("Overall Statistics:\n")
        f.write(f"  Average Score: {overall_avg_score:.3f}\n")
        f.write(f"  Average Pass Rate: {overall_pass_rate:.1f}%\n")
        f.write(
            f"  Overall Status: {'‚úÖ GOOD' if overall_pass_rate >= 80 else '‚ö†Ô∏è REVIEW' if overall_pass_rate >= 60 else '‚ùå POOR'}\n"
        )

    print(f"‚úÖ Summary report generated: {summary_file}")


async def main():
    """Run all batch evaluation examples."""

    # Check if OpenAI API key is available
    if not os.getenv("OPENAI_API_KEY"):
        print("‚ö†Ô∏è  Warning: OPENAI_API_KEY not found in environment variables")
        print("   This example requires an OpenAI API key to run properly.")
        print("   Set your API key: export OPENAI_API_KEY='your-key-here'")
        return

    try:
        # Run batch evaluation
        batch_results = await batch_evaluation_example()

        # Run comprehensive batch evaluation
        await comprehensive_batch_evaluation()

        # Performance comparison
        await performance_comparison()

        # Export results
        await export_results_example(batch_results)

        print("\n‚úÖ Batch RAG Evaluation Example completed successfully!")
        print("\nüìö Generated Files:")
        print("‚Ä¢ batch_evaluation_results.json - Detailed evaluation results")
        print("‚Ä¢ evaluation_summary.txt - Human-readable summary report")
        print("\nüìö Next Steps:")
        print("‚Ä¢ Analyze the exported results for insights")
        print("‚Ä¢ Integrate batch evaluation into your CI/CD pipeline")
        print("‚Ä¢ Use the performance data to optimize your evaluation strategy")

    except Exception as e:
        print(f"\n‚ùå Example failed with error: {e!s}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
