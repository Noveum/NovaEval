{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agent Evaluation Demo with NovaEval\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "1. Load agent trace data from dataset.json or dataset_tool_calls_removed.json\n",
        "2. Map trace spans to AgentData format\n",
        "3. Create an AgentDataset \n",
        "4. Evaluate agent performance using AgentEvaluator with Gemini model\n",
        "5. Analyze results with multiple scorers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "# NovaEval imports\n",
        "from novaeval.agents.agent_data import AgentData, ToolSchema, ToolCall, ToolResult\n",
        "from novaeval.datasets.agent_dataset import AgentDataset\n",
        "from novaeval.evaluators.agent_evaluator import AgentEvaluator\n",
        "from novaeval.models.gemini import GeminiModel\n",
        "from novaeval.scorers.agent_scorers import (\n",
        "    context_relevancy_scorer,\n",
        "    role_adherence_scorer,\n",
        "    task_progression_scorer,\n",
        "    tool_relevancy_scorer,\n",
        "    tool_correctness_scorer,\n",
        "    parameter_correctness_scorer\n",
        ")\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "print(\"✅ All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pwd\n",
        "!python langchain_agent/traces/fetch_traces_api.py\n",
        "!python langchain_agent/traces/combine_spans_api_compat.py\n",
        "\n",
        "!cp langchain_agent/traces/traces/dataset.json .\n",
        "\n",
        "!python clean_tool_call_spans.py "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load and Examine Dataset Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "file_name = 'dataset_tool_calls_removed.json'\n",
        "with open(file_name, 'r') as f:\n",
        "    spans_data = json.load(f)\n",
        "\n",
        "print(f\"📊 Loaded {len(spans_data)} spans from {file_name}\")\n",
        "print(f\"\\n🔍 Sample span types:\")\n",
        "\n",
        "# Analyze span types\n",
        "span_types = {}\n",
        "for span in spans_data[:10]:  # Look at first 10 spans\n",
        "    span_name = span.get('name', 'unknown')\n",
        "    if span_name not in span_types:\n",
        "        span_types[span_name] = 0\n",
        "    span_types[span_name] += 1\n",
        "\n",
        "for span_type, count in span_types.items():\n",
        "    print(f\"  - {span_type}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Implement Field Mapping Logic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_tools_from_prompt(prompt: str) -> List[ToolSchema]:\n",
        "    \"\"\"\n",
        "    Parse tool definitions from LLM prompts using regex.\n",
        "    \n",
        "    Expected format: tool_name(param: type = default) -> return_type - description\n",
        "    \"\"\"\n",
        "    # Pattern to match tool signatures\n",
        "    pattern = r'(\\w+)\\(([^)]*)\\)\\s*->\\s*(\\w+)\\s*-\\s*(.+?)(?=\\n\\w+\\(|$)'\n",
        "    matches = re.findall(pattern, prompt, re.DOTALL)\n",
        "    \n",
        "    tools = []\n",
        "    for match in matches:\n",
        "        tool_name, params_str, return_type, description = match\n",
        "        \n",
        "        # Parse parameters\n",
        "        args_schema = parse_params(params_str)\n",
        "        \n",
        "        tool = ToolSchema(\n",
        "            name=tool_name,\n",
        "            description=description.strip(),\n",
        "            args_schema=args_schema,\n",
        "            return_schema={\"type\": return_type}\n",
        "        )\n",
        "        tools.append(tool)\n",
        "    \n",
        "    return tools\n",
        "\n",
        "def parse_params(params_str: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Parse parameter string into schema dictionary.\n",
        "    \n",
        "    Format: param_name: type = default_value\n",
        "    \"\"\"\n",
        "    if not params_str.strip():\n",
        "        return {}\n",
        "    \n",
        "    # Split parameters by comma\n",
        "    params = [p.strip() for p in params_str.split(',') if p.strip()]\n",
        "    schema = {}\n",
        "    \n",
        "    for param in params:\n",
        "        if ':' in param:\n",
        "            parts = param.split(':', 1)\n",
        "            param_name = parts[0].strip()\n",
        "            type_and_default = parts[1].strip()\n",
        "            \n",
        "            # Extract type and default value\n",
        "            if '=' in type_and_default:\n",
        "                type_part, default_part = type_and_default.split('=', 1)\n",
        "                param_type = type_part.strip()\n",
        "                default_val = default_part.strip().strip('\"\\'')\n",
        "                schema[param_name] = {'type': param_type, 'default': default_val}\n",
        "            else:\n",
        "                param_type = type_and_default.strip()\n",
        "                schema[param_name] = {'type': param_type}\n",
        "    \n",
        "    return schema\n",
        "\n",
        "def identify_span_type(span: Dict[str, Any]) -> str:\n",
        "    \"\"\"\n",
        "    Identify span type based on attributes.\n",
        "    \"\"\"\n",
        "    attributes = span.get('attributes', {})\n",
        "    \n",
        "    # Check for agent attributes\n",
        "    if any('chain.name' == key for key in attributes.keys()):\n",
        "        return 'agent'\n",
        "    \n",
        "    # Check for LLM attributes\n",
        "    if any('llm.model' == key for key in attributes.keys()):\n",
        "        return 'llm'\n",
        "    \n",
        "    # Check for tool attributes\n",
        "    if any('tool.name' == key for key in attributes.keys()):\n",
        "        return 'tool'\n",
        "    print('returning unknown type for span')\n",
        "    return 'unknown'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_span_to_agent_data(span: Dict[str, Any]) -> AgentData:\n",
        "    \"\"\"\n",
        "    Map a single span from file_name to AgentData format.\n",
        "    \"\"\"\n",
        "    attributes = span.get('attributes', {})\n",
        "    events = span.get('events', [])\n",
        "    span_type = identify_span_type(span)\n",
        "\n",
        "    # Base mappings\n",
        "    data = {\n",
        "        'user_id': span.get('metadata', {}).get('user_id', None),\n",
        "        'task_id': span.get('trace_id'),\n",
        "        'turn_id': span.get('span_id'),\n",
        "        'ground_truth': None,\n",
        "        'expected_tool_call': None,\n",
        "        'agent_name': span_type,\n",
        "        'agent_role': span_type,\n",
        "        'system_prompt': \"You are a helpful customer support agent\",\n",
        "        'metadata': None,\n",
        "        'exit_status': span.get('status'),\n",
        "        'tools_available': [],\n",
        "        'tool_calls': [],\n",
        "        'parameters_passed': {},\n",
        "        'tool_call_results': [],\n",
        "        'retrieval_query': None,\n",
        "        'retrieved_context': None,\n",
        "        'agent_exit': False,\n",
        "        'trace': None\n",
        "    }\n",
        "    \n",
        "    # Span-specific mappings\n",
        "    if span_type == 'agent':\n",
        "        # Agent task\n",
        "        chain_inputs = attributes.get('chain.inputs', {})\n",
        "        if isinstance(chain_inputs, dict) and 'input' in chain_inputs:\n",
        "            data['agent_task'] = chain_inputs['input']\n",
        "        else:\n",
        "            print('agent_task not found')\n",
        "        # Agent response\n",
        "        finish_values = attributes.get('agent.output.finish.return_values', {})\n",
        "        if isinstance(finish_values, dict) and 'output' in finish_values:\n",
        "            data['agent_response'] = finish_values['output']\n",
        "        else:\n",
        "            print(\"agent_response is not available\")\n",
        "        # Tool calls from agent actions\n",
        "        tool_name = attributes.get('agent.output.action.tool')\n",
        "        tool_input = attributes.get('agent.output.action.tool_input')\n",
        "        \n",
        "        if tool_name:\n",
        "            tool_call = ToolCall(\n",
        "                tool_name=tool_name,\n",
        "                parameters={'input': tool_input} if tool_input else {},\n",
        "                call_id=span['span_id']\n",
        "            )\n",
        "            data['tool_calls'] = [tool_call]\n",
        "            data['parameters_passed'] = {'input': tool_input} if tool_input else {}\n",
        "            \n",
        "            # Handle retrieval query for langchain_retriever\n",
        "            if tool_name == 'langchain_retriever' and tool_input:\n",
        "                data['retrieval_query'] = [tool_input]\n",
        "        \n",
        "        # Agent exit status\n",
        "        data['agent_exit'] = any(event.get('name') == 'agent_finish' for event in events)\n",
        "        \n",
        "        # Trace (dump events as JSON)\n",
        "        if events:\n",
        "            data['trace'] = json.dumps(events)\n",
        "    \n",
        "    elif span_type == 'llm':\n",
        "        # Agent response from LLM output\n",
        "        llm_input = attributes.get('llm.input.prompts', ['input is not available'])\n",
        "        data['agent_task'] = llm_input[0]\n",
        "\n",
        "        llm_responses = attributes.get('llm.output.response', [])\n",
        "        if llm_responses:\n",
        "            data['agent_response'] = llm_responses[0]\n",
        "        else:\n",
        "            print(\"llm_response is not available\")\n",
        "        # Parse tools from prompt\n",
        "        prompts = attributes.get('llm.input.prompts', [])\n",
        "        if prompts:\n",
        "            try:\n",
        "                tools = parse_tools_from_prompt(prompts[0])\n",
        "                data['tools_available'] = tools\n",
        "            except Exception:\n",
        "                # Fallback to empty list if parsing fails\n",
        "                data['tools_available'] = []\n",
        "        \n",
        "        data['parameters_passed'] = {}\n",
        "    \n",
        "    elif span_type == 'tool':\n",
        "        # Agent response from tool output\n",
        "        tool_output = attributes.get('tool.output.output')\n",
        "        data['agent_task'] = f\"This is a simple tool call, and the tool will execute as programmed. Its name is - {attributes.get('tool.name')}\"\n",
        "        if tool_output:\n",
        "            data['agent_response'] = tool_output\n",
        "        else:\n",
        "            print(\"tool_output is not available\")\n",
        "        # Tool call results\n",
        "        tool_name = attributes.get('tool.name')\n",
        "        if tool_name and tool_output is not None:\n",
        "            tool_result = ToolResult(\n",
        "                call_id=span['span_id'],\n",
        "                result=tool_output,\n",
        "                success=span.get('status') == 'ok',\n",
        "                error_message=None if span.get('status') == 'ok' else 'Tool execution failed'\n",
        "            )\n",
        "            data['tool_call_results'] = [tool_result]\n",
        "            \n",
        "            # Handle retrieved context for langchain_retriever\n",
        "            if tool_name == 'langchain_retriever':\n",
        "                data['retrieved_context'] = [[tool_output]]\n",
        "        \n",
        "        # Parameters from tool input\n",
        "        tool_input_keys = [key for key in attributes.keys() if key.startswith('tool.input.')]\n",
        "        tool_params = {}\n",
        "        for key in tool_input_keys:\n",
        "            param_name = key.replace('tool.input.', '')\n",
        "            tool_params[param_name] = attributes[key]\n",
        "        data['parameters_passed'] = tool_params\n",
        "    \n",
        "    return AgentData(**data)\n",
        "\n",
        "print(\"✅ Field mapping functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create AgentDataset from Spans\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert spans to AgentData objects\n",
        "print(\"🔄 Converting spans to AgentData objects...\")\n",
        "\n",
        "agent_data_list = []\n",
        "errors = []\n",
        "\n",
        "for i, span in enumerate(spans_data):\n",
        "    try:\n",
        "        agent_data = map_span_to_agent_data(span)\n",
        "        agent_data_list.append(agent_data)\n",
        "    except Exception as e:\n",
        "        errors.append(f\"Span {i}: {str(e)}\")\n",
        "        if len(errors) <= 5:  # Show first 5 errors only\n",
        "            print(f\"⚠️  Error processing span {i}: {e}\")\n",
        "\n",
        "print(f\"\\n✅ Successfully converted {len(agent_data_list)} spans to AgentData\")\n",
        "if errors:\n",
        "    print(f\"❌ {len(errors)} spans had errors\")\n",
        "\n",
        "# Create AgentDataset\n",
        "dataset = AgentDataset()\n",
        "dataset.data = agent_data_list\n",
        "\n",
        "print(f\"📊 AgentDataset created with {len(dataset.data)} records\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Examine Sample Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show statistics about the dataset\n",
        "print(\"📈 Dataset Statistics:\")\n",
        "\n",
        "agent_types = {}\n",
        "tool_usage = {}\n",
        "with_responses = 0\n",
        "with_tool_calls = 0\n",
        "with_retrieval = 0\n",
        "\n",
        "for data in dataset.data:\n",
        "    # Agent types\n",
        "    if data.agent_name:\n",
        "        agent_types[data.agent_name] = agent_types.get(data.agent_name, 0) + 1\n",
        "    \n",
        "    # Responses\n",
        "    if data.agent_response:\n",
        "        with_responses += 1\n",
        "    \n",
        "    # Tool calls\n",
        "    if data.tool_calls:\n",
        "        with_tool_calls += 1\n",
        "        for tool_call in data.tool_calls:\n",
        "            if hasattr(tool_call, 'tool_name'):\n",
        "                tool_usage[tool_call.tool_name] = tool_usage.get(tool_call.tool_name, 0) + 1\n",
        "    \n",
        "    # Retrieval\n",
        "    if data.retrieval_query:\n",
        "        with_retrieval += 1\n",
        "\n",
        "print(f\"\\nAgent Types: {dict(agent_types)}\")\n",
        "print(f\"Records with responses: {with_responses}\")\n",
        "print(f\"Records with tool calls: {with_tool_calls}\")\n",
        "print(f\"Records with retrieval: {with_retrieval}\")\n",
        "print(f\"Tool usage: {dict(tool_usage)}\")\n",
        "\n",
        "# Show sample records\n",
        "print(\"\\n🔍 Sample AgentData records:\")\n",
        "for i, data in enumerate(dataset.data[:3]):\n",
        "    print(f\"\\n--- Record {i+1} ({data.agent_name}) ---\")\n",
        "    print(f\"Task: {data.agent_task[:100] if data.agent_task else 'None'}...\")\n",
        "    print(f\"Response: {data.agent_response[:100] if data.agent_response else 'None'}...\")\n",
        "    print(f\"Tool calls: {len(data.tool_calls) if data.tool_calls else 0}\")\n",
        "    print(f\"Exit status: {data.exit_status}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Setup Gemini Model and Evaluator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check for API key\n",
        "if 'GEMINI_API_KEY' not in os.environ:\n",
        "    print(\"⚠️  GEMINI_API_KEY environment variable not set!\")\n",
        "    print(\"Please set it before running evaluation:\")\n",
        "    print(\"export GEMINI_API_KEY='your-api-key-here'\")\n",
        "else:\n",
        "    print(\"✅ GEMINI_API_KEY found in environment\")\n",
        "\n",
        "# Initialize Gemini model\n",
        "try:\n",
        "    gemini_model = GeminiModel(\n",
        "        model_name=\"gemini-1.5-flash\",  # Using flash model for cost efficiency\n",
        "        temperature=0.1,  # Low temperature for consistent evaluation\n",
        "        max_tokens=1024\n",
        "    )\n",
        "    print(\"✅ Gemini model initialized\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error initializing Gemini model: {e}\")\n",
        "    gemini_model = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize scoring functions for evaluation\n",
        "scoring_functions = [\n",
        "    task_progression_scorer,\n",
        "    context_relevancy_scorer,\n",
        "    role_adherence_scorer,\n",
        "    tool_relevancy_scorer,\n",
        "    parameter_correctness_scorer\n",
        "]\n",
        "\n",
        "print(f\"✅ Initialized {len(scoring_functions)} scoring functions:\")\n",
        "for func in scoring_functions:\n",
        "    print(f\"  - {func.__name__}\")\n",
        "\n",
        "# Create AgentEvaluator\n",
        "if gemini_model:\n",
        "    evaluator = AgentEvaluator(\n",
        "        agent_dataset=dataset,\n",
        "        models=[gemini_model],\n",
        "        scoring_functions=scoring_functions,\n",
        "        output_dir=\"./demo_results\",\n",
        "        stream=False,\n",
        "        include_reasoning=True\n",
        "    )\n",
        "    print(\"\\n✅ AgentEvaluator created with Gemini model and scoring functions\")\n",
        "else:\n",
        "    print(\"\\n❌ Cannot create evaluator - Gemini model not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Run Evaluation (Sample)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation using the AgentEvaluator's run_all method\n",
        "print(\"🚀 Running evaluation on sample data...\")\n",
        "\n",
        "if gemini_model and evaluator:\n",
        "    try:\n",
        "        # Create a smaller dataset for demo purposes\n",
        "        sample_data = [data for data in dataset.data if data.agent_response][:]\n",
        "        print(f\"\\n📊 Evaluating {len(sample_data)} sample records...\")\n",
        "        \n",
        "        # Create a temporary dataset with just the sample data\n",
        "        sample_dataset = AgentDataset()\n",
        "        sample_dataset.data = sample_data\n",
        "        \n",
        "        # Create a new evaluator with the sample dataset\n",
        "        sample_evaluator = AgentEvaluator(\n",
        "            agent_dataset=sample_dataset,\n",
        "            models=[gemini_model],\n",
        "            scoring_functions=scoring_functions,\n",
        "            output_dir=\"./demo_results/sample_evaluation\",\n",
        "            stream=False,\n",
        "            include_reasoning=True\n",
        "        )\n",
        "        \n",
        "        # Run the evaluation\n",
        "        sample_evaluator.run_all(save_every=1, file_type=\"csv\")\n",
        "        \n",
        "        print(\"\\n✅ Evaluation completed!\")\n",
        "        \n",
        "        # Read and display results\n",
        "        import pandas as pd\n",
        "        results_file = \"./demo_results/sample_evaluation/agent_evaluation_results.csv\"\n",
        "        \n",
        "        if pd.io.common.file_exists(results_file):\n",
        "            results_df = pd.read_csv(results_file)\n",
        "            print(f\"\\n📊 Results Summary:\")\n",
        "            \n",
        "            # Calculate averages for each scorer\n",
        "            scorer_columns = [col for col in results_df.columns if col not in ['user_id', 'task_id', 'turn_id', 'agent_name'] and not col.endswith('_reasoning')]\n",
        "            \n",
        "            for col in scorer_columns:\n",
        "                if results_df[col].dtype in ['float64', 'int64']:\n",
        "                    avg_score = results_df[col].mean()\n",
        "                    print(f\"  - {col}: {avg_score:.2f}\")\n",
        "            \n",
        "            # Show individual scores\n",
        "            print(f\"\\n🔍 Individual Scores:\")\n",
        "            for i, row in results_df.iterrows():\n",
        "                print(f\"\\n  Record {i+1} (Task: {row.get('task_id', 'N/A')}):\")\n",
        "                for col in scorer_columns:\n",
        "                    if pd.notna(row[col]):\n",
        "                        print(f\"    - {col}: {row[col]}\")\n",
        "        else:\n",
        "            print(\"❌ Results file not found\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during evaluation: {e}\")\n",
        "        print(f\"Error type: {type(e).__name__}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        \n",
        "else:\n",
        "    print(\"⚠️  Skipping evaluation - missing model or evaluator\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Analysis and Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze the dataset characteristics\n",
        "print(\"🔍 Dataset Analysis:\")\n",
        "print(\"\\n=== Agent Behavior Patterns ===\")\n",
        "\n",
        "# Analyze tool usage patterns\n",
        "tool_patterns = {}\n",
        "task_types = {}\n",
        "response_lengths = []\n",
        "\n",
        "for data in dataset.data:\n",
        "    # Tool usage\n",
        "    if data.tool_calls:\n",
        "        for tool_call in data.tool_calls:\n",
        "            if hasattr(tool_call, 'tool_name'):\n",
        "                tool_name = tool_call.tool_name\n",
        "                if tool_name not in tool_patterns:\n",
        "                    tool_patterns[tool_name] = {'count': 0, 'success_rate': 0}\n",
        "                tool_patterns[tool_name]['count'] += 1\n",
        "    \n",
        "    # Task analysis\n",
        "    if data.agent_task:\n",
        "        # Simple categorization\n",
        "        task_lower = data.agent_task.lower()\n",
        "        if 'user_input' in task_lower:\n",
        "            task_types['user_input'] = task_types.get('user_input', 0) + 1\n",
        "        elif 'exit' in task_lower:\n",
        "            task_types['exit_command'] = task_types.get('exit_command', 0) + 1\n",
        "        else:\n",
        "            task_types['other'] = task_types.get('other', 0) + 1\n",
        "    \n",
        "    # Response analysis\n",
        "    if data.agent_response:\n",
        "        response_lengths.append(len(data.agent_response))\n",
        "\n",
        "print(f\"\\n📈 Tool Usage:\")\n",
        "for tool, stats in tool_patterns.items():\n",
        "    print(f\"  - {tool}: {stats['count']} uses\")\n",
        "\n",
        "print(f\"\\n📋 Task Types:\")\n",
        "for task_type, count in task_types.items():\n",
        "    print(f\"  - {task_type}: {count}\")\n",
        "\n",
        "if response_lengths:\n",
        "    avg_response_length = sum(response_lengths) / len(response_lengths)\n",
        "    print(f\"\\n📝 Response Statistics:\")\n",
        "    print(f\"  - Average response length: {avg_response_length:.1f} characters\")\n",
        "    print(f\"  - Min response length: {min(response_lengths)}\")\n",
        "    print(f\"  - Max response length: {max(response_lengths)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Export Results (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Export the processed dataset for future use\n",
        "print(\"💾 Exporting processed dataset...\")\n",
        "\n",
        "try:\n",
        "    # Export to JSON\n",
        "    dataset.export_to_json(f'processed_agent_{file_name}')\n",
        "    print(f\"✅ Exported to processed_agent_{file_name}\")\n",
        "    \n",
        "    # Export to CSV (optional)\n",
        "    dataset.export_to_csv('processed_agent_dataset.csv')\n",
        "    print(\"✅ Exported to processed_agent_dataset.csv\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Export error: {e}\")\n",
        "\n",
        "print(\"\\n🎉 Demo completed successfully!\")\n",
        "print(\"\\n📋 Summary:\")\n",
        "print(f\"  - Processed {len(spans_data)} spans from {file_name}\")\n",
        "print(f\"  - Created {len(dataset.data)} AgentData records\")\n",
        "print(f\"  - Configured evaluation with Gemini model and 6 scorers\")\n",
        "if 'results' in locals():\n",
        "    print(f\"  - Successfully evaluated sample data\")\n",
        "print(f\"  - Exported processed dataset for future use\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
