{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agent Evaluation Demo with NovaEval\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "1. Load agent trace data from dataset.json\n",
        "2. Map trace spans to AgentData format\n",
        "3. Create an AgentDataset \n",
        "4. Evaluate agent performance using AgentEvaluator with Gemini model\n",
        "5. Analyze results with multiple scorers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'AgentTaskCompletionScorer' from 'novaeval.scorers.agent_scorers' (/home/shivam/Desktop/noveum/NovaEval/src/novaeval/scorers/agent_scorers.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnovaeval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_evaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AgentEvaluator\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnovaeval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgemini\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GeminiModel\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnovaeval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_scorers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AgentTaskCompletionScorer, AgentToolUsageScorer, AgentReasoningScorer\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… All imports successful!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AgentTaskCompletionScorer' from 'novaeval.scorers.agent_scorers' (/home/shivam/Desktop/noveum/NovaEval/src/novaeval/scorers/agent_scorers.py)"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "# NovaEval imports\n",
        "from novaeval.agents.agent_data import AgentData, ToolSchema, ToolCall, ToolResult\n",
        "from novaeval.datasets.agent_dataset import AgentDataset\n",
        "from novaeval.evaluators.agent_evaluator import AgentEvaluator\n",
        "from novaeval.models.gemini import GeminiModel\n",
        "from novaeval.scorers.agent_scorers import AgentTaskCompletionScorer, AgentToolUsageScorer, AgentReasoningScorer\n",
        "\n",
        "print(\"âœ… All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load and Examine Dataset Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "with open('dataset.json', 'r') as f:\n",
        "    spans_data = json.load(f)\n",
        "\n",
        "print(f\"ðŸ“Š Loaded {len(spans_data)} spans from dataset.json\")\n",
        "print(f\"\\nðŸ” Sample span types:\")\n",
        "\n",
        "# Analyze span types\n",
        "span_types = {}\n",
        "for span in spans_data[:10]:  # Look at first 10 spans\n",
        "    span_name = span.get('name', 'unknown')\n",
        "    if span_name not in span_types:\n",
        "        span_types[span_name] = 0\n",
        "    span_types[span_name] += 1\n",
        "\n",
        "for span_type, count in span_types.items():\n",
        "    print(f\"  - {span_type}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Implement Field Mapping Logic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_tools_from_prompt(prompt: str) -> List[ToolSchema]:\n",
        "    \"\"\"\n",
        "    Parse tool definitions from LLM prompts using regex.\n",
        "    \n",
        "    Expected format: tool_name(param: type = default) -> return_type - description\n",
        "    \"\"\"\n",
        "    # Pattern to match tool signatures\n",
        "    pattern = r'(\\w+)\\(([^)]*)\\)\\s*->\\s*(\\w+)\\s*-\\s*(.+?)(?=\\n\\w+\\(|$)'\n",
        "    matches = re.findall(pattern, prompt, re.DOTALL)\n",
        "    \n",
        "    tools = []\n",
        "    for match in matches:\n",
        "        tool_name, params_str, return_type, description = match\n",
        "        \n",
        "        # Parse parameters\n",
        "        args_schema = parse_params(params_str)\n",
        "        \n",
        "        tool = ToolSchema(\n",
        "            name=tool_name,\n",
        "            description=description.strip(),\n",
        "            args_schema=args_schema,\n",
        "            return_schema={\"type\": return_type}\n",
        "        )\n",
        "        tools.append(tool)\n",
        "    \n",
        "    return tools\n",
        "\n",
        "def parse_params(params_str: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Parse parameter string into schema dictionary.\n",
        "    \n",
        "    Format: param_name: type = default_value\n",
        "    \"\"\"\n",
        "    if not params_str.strip():\n",
        "        return {}\n",
        "    \n",
        "    # Split parameters by comma\n",
        "    params = [p.strip() for p in params_str.split(',') if p.strip()]\n",
        "    schema = {}\n",
        "    \n",
        "    for param in params:\n",
        "        if ':' in param:\n",
        "            parts = param.split(':', 1)\n",
        "            param_name = parts[0].strip()\n",
        "            type_and_default = parts[1].strip()\n",
        "            \n",
        "            # Extract type and default value\n",
        "            if '=' in type_and_default:\n",
        "                type_part, default_part = type_and_default.split('=', 1)\n",
        "                param_type = type_part.strip()\n",
        "                default_val = default_part.strip().strip('\"\\'')\n",
        "                schema[param_name] = {'type': param_type, 'default': default_val}\n",
        "            else:\n",
        "                param_type = type_and_default.strip()\n",
        "                schema[param_name] = {'type': param_type}\n",
        "    \n",
        "    return schema\n",
        "\n",
        "def identify_span_type(span: Dict[str, Any]) -> str:\n",
        "    \"\"\"\n",
        "    Identify span type based on attributes.\n",
        "    \"\"\"\n",
        "    attributes = span.get('attributes', {})\n",
        "    \n",
        "    # Check for agent attributes\n",
        "    if any(key.startswith('agent.') for key in attributes.keys()):\n",
        "        return 'agent'\n",
        "    \n",
        "    # Check for LLM attributes\n",
        "    if any(key.startswith('llm.') for key in attributes.keys()):\n",
        "        return 'llm'\n",
        "    \n",
        "    # Check for tool attributes\n",
        "    if any(key.startswith('tool.') for key in attributes.keys()):\n",
        "        return 'tool'\n",
        "    \n",
        "    return 'unknown'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_span_to_agent_data(span: Dict[str, Any]) -> AgentData:\n",
        "    \"\"\"\n",
        "    Map a single span from dataset.json to AgentData format.\n",
        "    \"\"\"\n",
        "    attributes = span.get('attributes', {})\n",
        "    events = span.get('events', [])\n",
        "    span_type = identify_span_type(span)\n",
        "    \n",
        "    # Base mappings\n",
        "    data = {\n",
        "        'user_id': span.get('metadata', {}).get('user_id', None),\n",
        "        'task_id': span.get('trace_id'),\n",
        "        'turn_id': span.get('span_id'),\n",
        "        'ground_truth': None,\n",
        "        'expected_tool_call': None,\n",
        "        'agent_name': span_type,\n",
        "        'agent_role': span_type,\n",
        "        'system_prompt': None,\n",
        "        'metadata': None,\n",
        "        'exit_status': span.get('status'),\n",
        "        'tools_available': [],\n",
        "        'tool_calls': [],\n",
        "        'parameters_passed': {},\n",
        "        'tool_call_results': [],\n",
        "        'retrieval_query': None,\n",
        "        'retrieved_context': None,\n",
        "        'agent_exit': False,\n",
        "        'trace': None\n",
        "    }\n",
        "    \n",
        "    # Span-specific mappings\n",
        "    if span_type == 'agent':\n",
        "        # Agent task\n",
        "        chain_inputs = attributes.get('chain.inputs', {})\n",
        "        if isinstance(chain_inputs, dict) and 'input' in chain_inputs:\n",
        "            data['agent_task'] = chain_inputs['input']\n",
        "        \n",
        "        # Agent response\n",
        "        finish_values = attributes.get('agent.output.finish.return_values', {})\n",
        "        if isinstance(finish_values, dict) and 'output' in finish_values:\n",
        "            data['agent_response'] = finish_values['output']\n",
        "        \n",
        "        # Tool calls from agent actions\n",
        "        tool_name = attributes.get('agent.output.action.tool')\n",
        "        tool_input = attributes.get('agent.output.action.tool_input')\n",
        "        \n",
        "        if tool_name:\n",
        "            tool_call = ToolCall(\n",
        "                tool_name=tool_name,\n",
        "                parameters={'input': tool_input} if tool_input else {},\n",
        "                call_id=span['span_id']\n",
        "            )\n",
        "            data['tool_calls'] = [tool_call]\n",
        "            data['parameters_passed'] = {'input': tool_input} if tool_input else {}\n",
        "            \n",
        "            # Handle retrieval query for langchain_retriever\n",
        "            if tool_name == 'langchain_retriever' and tool_input:\n",
        "                data['retrieval_query'] = [tool_input]\n",
        "        \n",
        "        # Agent exit status\n",
        "        data['agent_exit'] = any(event.get('name') == 'agent_finish' for event in events)\n",
        "        \n",
        "        # Trace (dump events as JSON)\n",
        "        if events:\n",
        "            data['trace'] = json.dumps(events)\n",
        "    \n",
        "    elif span_type == 'llm':\n",
        "        # Agent response from LLM output\n",
        "        llm_responses = attributes.get('llm.output.response', [])\n",
        "        if llm_responses:\n",
        "            data['agent_response'] = llm_responses[0]\n",
        "        \n",
        "        # Parse tools from prompt\n",
        "        prompts = attributes.get('llm.input.prompts', [])\n",
        "        if prompts:\n",
        "            try:\n",
        "                tools = parse_tools_from_prompt(prompts[0])\n",
        "                data['tools_available'] = tools\n",
        "            except Exception:\n",
        "                # Fallback to empty list if parsing fails\n",
        "                data['tools_available'] = []\n",
        "        \n",
        "        data['parameters_passed'] = {}\n",
        "    \n",
        "    elif span_type == 'tool':\n",
        "        # Agent response from tool output\n",
        "        tool_output = attributes.get('tool.output.output')\n",
        "        if tool_output:\n",
        "            data['agent_response'] = tool_output\n",
        "        \n",
        "        # Tool call results\n",
        "        tool_name = attributes.get('tool.name')\n",
        "        if tool_name and tool_output is not None:\n",
        "            tool_result = ToolResult(\n",
        "                call_id=span['span_id'],\n",
        "                result=tool_output,\n",
        "                success=span.get('status') == 'ok',\n",
        "                error_message=None if span.get('status') == 'ok' else 'Tool execution failed'\n",
        "            )\n",
        "            data['tool_call_results'] = [tool_result]\n",
        "            \n",
        "            # Handle retrieved context for langchain_retriever\n",
        "            if tool_name == 'langchain_retriever':\n",
        "                data['retrieved_context'] = [[tool_output]]\n",
        "        \n",
        "        # Parameters from tool input\n",
        "        tool_input_keys = [key for key in attributes.keys() if key.startswith('tool.input.')]\n",
        "        tool_params = {}\n",
        "        for key in tool_input_keys:\n",
        "            param_name = key.replace('tool.input.', '')\n",
        "            tool_params[param_name] = attributes[key]\n",
        "        data['parameters_passed'] = tool_params\n",
        "    \n",
        "    return AgentData(**data)\n",
        "\n",
        "print(\"âœ… Field mapping functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create AgentDataset from Spans\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert spans to AgentData objects\n",
        "print(\"ðŸ”„ Converting spans to AgentData objects...\")\n",
        "\n",
        "agent_data_list = []\n",
        "errors = []\n",
        "\n",
        "for i, span in enumerate(spans_data):\n",
        "    try:\n",
        "        agent_data = map_span_to_agent_data(span)\n",
        "        agent_data_list.append(agent_data)\n",
        "    except Exception as e:\n",
        "        errors.append(f\"Span {i}: {str(e)}\")\n",
        "        if len(errors) <= 5:  # Show first 5 errors only\n",
        "            print(f\"âš ï¸  Error processing span {i}: {e}\")\n",
        "\n",
        "print(f\"\\nâœ… Successfully converted {len(agent_data_list)} spans to AgentData\")\n",
        "if errors:\n",
        "    print(f\"âŒ {len(errors)} spans had errors\")\n",
        "\n",
        "# Create AgentDataset\n",
        "dataset = AgentDataset()\n",
        "dataset.data = agent_data_list\n",
        "\n",
        "print(f\"ðŸ“Š AgentDataset created with {len(dataset.data)} records\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Examine Sample Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show statistics about the dataset\n",
        "print(\"ðŸ“ˆ Dataset Statistics:\")\n",
        "\n",
        "agent_types = {}\n",
        "tool_usage = {}\n",
        "with_responses = 0\n",
        "with_tool_calls = 0\n",
        "with_retrieval = 0\n",
        "\n",
        "for data in dataset.data:\n",
        "    # Agent types\n",
        "    if data.agent_name:\n",
        "        agent_types[data.agent_name] = agent_types.get(data.agent_name, 0) + 1\n",
        "    \n",
        "    # Responses\n",
        "    if data.agent_response:\n",
        "        with_responses += 1\n",
        "    \n",
        "    # Tool calls\n",
        "    if data.tool_calls:\n",
        "        with_tool_calls += 1\n",
        "        for tool_call in data.tool_calls:\n",
        "            if hasattr(tool_call, 'tool_name'):\n",
        "                tool_usage[tool_call.tool_name] = tool_usage.get(tool_call.tool_name, 0) + 1\n",
        "    \n",
        "    # Retrieval\n",
        "    if data.retrieval_query:\n",
        "        with_retrieval += 1\n",
        "\n",
        "print(f\"\\nAgent Types: {dict(agent_types)}\")\n",
        "print(f\"Records with responses: {with_responses}\")\n",
        "print(f\"Records with tool calls: {with_tool_calls}\")\n",
        "print(f\"Records with retrieval: {with_retrieval}\")\n",
        "print(f\"Tool usage: {dict(tool_usage)}\")\n",
        "\n",
        "# Show sample records\n",
        "print(\"\\nðŸ” Sample AgentData records:\")\n",
        "for i, data in enumerate(dataset.data[:3]):\n",
        "    print(f\"\\n--- Record {i+1} ({data.agent_name}) ---\")\n",
        "    print(f\"Task: {data.agent_task[:100] if data.agent_task else 'None'}...\")\n",
        "    print(f\"Response: {data.agent_response[:100] if data.agent_response else 'None'}...\")\n",
        "    print(f\"Tool calls: {len(data.tool_calls) if data.tool_calls else 0}\")\n",
        "    print(f\"Exit status: {data.exit_status}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Setup Gemini Model and Evaluator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check for API key\n",
        "if 'GEMINI_API_KEY' not in os.environ:\n",
        "    print(\"âš ï¸  GEMINI_API_KEY environment variable not set!\")\n",
        "    print(\"Please set it before running evaluation:\")\n",
        "    print(\"export GEMINI_API_KEY='your-api-key-here'\")\n",
        "else:\n",
        "    print(\"âœ… GEMINI_API_KEY found in environment\")\n",
        "\n",
        "# Initialize Gemini model\n",
        "try:\n",
        "    gemini_model = GeminiModel(\n",
        "        model_name=\"gemini-1.5-flash\",  # Using flash model for cost efficiency\n",
        "        temperature=0.1,  # Low temperature for consistent evaluation\n",
        "        max_tokens=1024\n",
        "    )\n",
        "    print(\"âœ… Gemini model initialized\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error initializing Gemini model: {e}\")\n",
        "    gemini_model = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize 3 scorers for evaluation\n",
        "scorers = [\n",
        "    AgentTaskCompletionScorer(\n",
        "        model=gemini_model,\n",
        "        name=\"task_completion\",\n",
        "        description=\"Evaluates how well the agent completed the assigned task\"\n",
        "    ),\n",
        "    AgentToolUsageScorer(\n",
        "        model=gemini_model, \n",
        "        name=\"tool_usage\",\n",
        "        description=\"Evaluates the appropriateness and effectiveness of tool usage\"\n",
        "    ),\n",
        "    AgentReasoningScorer(\n",
        "        model=gemini_model,\n",
        "        name=\"reasoning_quality\", \n",
        "        description=\"Evaluates the quality of agent's reasoning and decision making\"\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"âœ… Initialized {len(scorers)} scorers:\")\n",
        "for scorer in scorers:\n",
        "    print(f\"  - {scorer.name}: {scorer.description}\")\n",
        "\n",
        "# Create AgentEvaluator\n",
        "evaluator = AgentEvaluator(\n",
        "    model=gemini_model,\n",
        "    scorers=scorers\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… AgentEvaluator created with Gemini model and 3 scorers\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Run Evaluation (Sample)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation on a small sample first\n",
        "print(\"ðŸš€ Running evaluation on sample data...\")\n",
        "\n",
        "# Take a small sample for demo (first 5 records with responses)\n",
        "sample_data = [data for data in dataset.data if data.agent_response][:5]\n",
        "\n",
        "print(f\"\\nðŸ“Š Evaluating {len(sample_data)} sample records...\")\n",
        "\n",
        "if gemini_model and sample_data:\n",
        "    try:\n",
        "        # Run evaluation\n",
        "        results = evaluator.evaluate_dataset(\n",
        "            agent_data=sample_data,\n",
        "            show_progress=True\n",
        "        )\n",
        "        \n",
        "        print(\"\\nâœ… Evaluation completed!\")\n",
        "        print(f\"\\nðŸ“Š Results Summary:\")\n",
        "        \n",
        "        if hasattr(results, 'aggregate_scores') and results.aggregate_scores:\n",
        "            for scorer_name, score in results.aggregate_scores.items():\n",
        "                print(f\"  - {scorer_name}: {score:.2f}\")\n",
        "        \n",
        "        # Show individual scores for first few records\n",
        "        if hasattr(results, 'individual_scores') and results.individual_scores:\n",
        "            print(f\"\\nðŸ” Individual Scores (first 3):\")\n",
        "            for i, scores in enumerate(results.individual_scores[:3]):\n",
        "                print(f\"\\n  Record {i+1}:\")\n",
        "                for scorer_name, score in scores.items():\n",
        "                    print(f\"    - {scorer_name}: {score}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during evaluation: {e}\")\n",
        "        print(f\"Error type: {type(e).__name__}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        \n",
        "else:\n",
        "    print(\"âš ï¸  Skipping evaluation - missing model or data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Analysis and Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze the dataset characteristics\n",
        "print(\"ðŸ” Dataset Analysis:\")\n",
        "print(\"\\n=== Agent Behavior Patterns ===\")\n",
        "\n",
        "# Analyze tool usage patterns\n",
        "tool_patterns = {}\n",
        "task_types = {}\n",
        "response_lengths = []\n",
        "\n",
        "for data in dataset.data:\n",
        "    # Tool usage\n",
        "    if data.tool_calls:\n",
        "        for tool_call in data.tool_calls:\n",
        "            if hasattr(tool_call, 'tool_name'):\n",
        "                tool_name = tool_call.tool_name\n",
        "                if tool_name not in tool_patterns:\n",
        "                    tool_patterns[tool_name] = {'count': 0, 'success_rate': 0}\n",
        "                tool_patterns[tool_name]['count'] += 1\n",
        "    \n",
        "    # Task analysis\n",
        "    if data.agent_task:\n",
        "        # Simple categorization\n",
        "        task_lower = data.agent_task.lower()\n",
        "        if 'user_input' in task_lower:\n",
        "            task_types['user_input'] = task_types.get('user_input', 0) + 1\n",
        "        elif 'exit' in task_lower:\n",
        "            task_types['exit_command'] = task_types.get('exit_command', 0) + 1\n",
        "        else:\n",
        "            task_types['other'] = task_types.get('other', 0) + 1\n",
        "    \n",
        "    # Response analysis\n",
        "    if data.agent_response:\n",
        "        response_lengths.append(len(data.agent_response))\n",
        "\n",
        "print(f\"\\nðŸ“ˆ Tool Usage:\")\n",
        "for tool, stats in tool_patterns.items():\n",
        "    print(f\"  - {tool}: {stats['count']} uses\")\n",
        "\n",
        "print(f\"\\nðŸ“‹ Task Types:\")\n",
        "for task_type, count in task_types.items():\n",
        "    print(f\"  - {task_type}: {count}\")\n",
        "\n",
        "if response_lengths:\n",
        "    avg_response_length = sum(response_lengths) / len(response_lengths)\n",
        "    print(f\"\\nðŸ“ Response Statistics:\")\n",
        "    print(f\"  - Average response length: {avg_response_length:.1f} characters\")\n",
        "    print(f\"  - Min response length: {min(response_lengths)}\")\n",
        "    print(f\"  - Max response length: {max(response_lengths)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Export Results (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Export the processed dataset for future use\n",
        "print(\"ðŸ’¾ Exporting processed dataset...\")\n",
        "\n",
        "try:\n",
        "    # Export to JSON\n",
        "    dataset.export_to_json('processed_agent_dataset.json')\n",
        "    print(\"âœ… Exported to processed_agent_dataset.json\")\n",
        "    \n",
        "    # Export to CSV (optional)\n",
        "    dataset.export_to_csv('processed_agent_dataset.csv')\n",
        "    print(\"âœ… Exported to processed_agent_dataset.csv\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Export error: {e}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ Demo completed successfully!\")\n",
        "print(\"\\nðŸ“‹ Summary:\")\n",
        "print(f\"  - Processed {len(spans_data)} spans from dataset.json\")\n",
        "print(f\"  - Created {len(dataset.data)} AgentData records\")\n",
        "print(f\"  - Configured evaluation with Gemini model and 3 scorers\")\n",
        "if 'results' in locals():\n",
        "    print(f\"  - Successfully evaluated sample data\")\n",
        "print(f\"  - Exported processed dataset for future use\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
